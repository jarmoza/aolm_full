1. Information criteria for unsupervised models make sense (bayesian criteria for topic models)

2. Keeping in mind that qualitative decision points matter - "the missing bridge for humanists to machine learning"

What matters for humanists in their moments of intuitive decision making for machine learning, supervised and unsupervised:

The idea of the holdout set and the qualitative decisions that shape it

(a) the intent of data transcribers
(b) the intent of researchers - the research goals shape the holdout set, parameters, subcorpus selection etc
(c) authorial intent (?) - less relevant, questionable

We are building the bridge between the world of quantitative machine learning decision points in stats
  e.g. cross validation, f1 scores, etc. and the qualitative decision points in humanities as noted above

3. Using BERT for sentence/line prediction in Dickinson data set as a means of understanding what difference data quality between Dickinson version/corpora versions makes to a model like BERT
  - How much "noise" can it tolerate in so much that it would alter the outcome of tasks that models like BERT are built for, e.g. sentence prediction

4. Return to discussion and example use of 'silhouette' for clustering