I'm doing something to the extreme so you don't have to.

====================================================================================================

Not all of Coleman's categories for data quality readily fit with the criteria of literary modeling. New categories can always be theorized and implemented based on a project, but 'consistency' and 'integrity' adapt easily to the practice.

What might 'consistency' look like in a project considering Emily Dickinson's works? Let's consider a regular problem in modeling literature: the composition of corpora. What's in and what's out? Lists of works of a corpus can vary from source to source let alone the actual textual contents of each of those works. One simple first stab for Dickinson would be to compare a less certain, but communal record of her oeuvre with that of a master list. Why would we want to do so? The stability and accountability of digital data sets for literary research as of this writing still has much room for growth. For instance, the notion of a data quality check itself also can consist of checking the accuracy of metadata. The status of Dickinson metadata is a common scenario. It is scattered around the internet or available in book form if one is able to access and transcribe it. Wikipedia's own list is widely available and somewhat comprehensive. (It doesn't delve into the kinds of detailed and critical distinctions made by Martha Nell Smith, for example.) It's also easily comparable to R.W. Franklin's and Thomas Johnson's compendiums of Dickinson's manuscripts – comprehensive, if criticized 'master' source lists.

A phenomenon emerges from a test that compares work titles between the lists. The metric itself, something I will call 'title consistency', is simply a minimum of matching sequential words between titles across two lists. The closest matching titles are determined and the percentage of discrepancy calculated. After shifting the minimum matching word threshold by ten percent intervals (10% of sequential words must match, 20% must match, etc.), the number of matches converges to a spot like a limit as it approaches zero. Likely a generalizable phenomenon, the curve produced has its own signature rate of match success for the corpus and metadata. Decreasing the threshold below ten percent does not move the needle towards a higher match. With the exception of missing manuscript IDs (Franklin IDs not present in the Wikipedia list), the behavior of the remaining three data quality measurements under the general metric of 'title consistency' (matching titles, mismatching titles, and percentage of titles discrepant from the master source list) can be described as almost asymptotic.

The Franklin version of Dickinson's corpus is generally regarded as consisting of 1,789 manuscripts (depending on one's definition of a complete Dickinson work). However the full digital collection available for download from the Emily Dickinson Archive consists of 4,825 transcriptions across several publications of poems and letters, 2,459 of which are attributed to Franklin's variorum, 'The Poems of Emily Dickinson'. Decreasing from 100% required word match we see the number of possible poem title matches increase, until they level off at 10% match at 1,782 poems – about a 1% discrepancy between the measurement and Franklin's master list. Ironically, or perhaps expectedly, relaxing the requirements allows more titles to be included as works identifiable and thus accountable to a bibliographic source list.

This is an early juncture in the modeling process where dataset curation can propagate errored models. Knowing the amount of discrepancy at least tempers and makes visible that potential error. For the case of the Dickinson corpus, it's small enough to understand the discrepancy. The missing seven poems turn out to be a case of Johnson (Franklin's compendium predecessor) and the Wikipedia authors choosing alternate titles for poems or choosing titles based on alternate manuscripts of the same poem. Their choices are interpretations of which of Dickinson's revisions seem like her final ones, and none the less valid ones. Alternates they may be, but Wikipedia's curated poem list displays a quality of research familiar to literary scholars: the ultimate subjectivity that is the composition of a data set. Johnson's prior choices and Wikipedia's list also provide a valuable service for the researcher wary of diving into tomes like Franklin and Johnson: a viable, alternative authority for making choices between variant poems. Depending on the level of one's work, these may not be decisions one wants to make.

Here I have chosen not to include a consistency measurement of comparing each work word for word as this often involves an unreasonable amount of work. Even for the relatively small Dickinson corpus, transcribing each work from Franklin's or Johnson's or Smith's to compare to digital editions is not only an arduous endeavor, but also potentially redundant given the knowledge that expert scholars and graduate students were involved in transcribing Dickinson's manuscripts for the Emily Dickinson Archive. In this case, the title consistency metric acts as a safeguard for that unknown potential for modeling error due to faulty data. The measurements themselves have allowed me to focus in on the seven mismatches and three to four missing manuscript IDs to make my own decisions about tidying up the corpus I want to model.

Just as Coleman points out, the work of data quality is not a single use standard, but rather a cyclical practice that works toward a satisfactory refinement. One inherently expects the work of data refinement to behave in this way, but seeing it in quantitative form shows where hard work pays off versus where it significantly diminishes into pointlessness. Not only that, but seeing the full range of consistency also allows a researcher to pick a point where they feel that they are able to balance consistency, the metric in question, with accuracy. A rewarding outcome of this practice, especially in terms of humanities data sets, is that the qualitative judgment of subject matter experts can be laid bare in a way that is accountable to modeling - the realm of quantification – while demonstrating the point of strong, theoretically grounded humanities interventions. And this series of decisions that eventually produces a new representation of cultural objects is one that is most fully developed by the practice of doing so. In other words, despite its quantitatively-informed foundations, computational modeling of literature involves developing a techne, a craft, or an art.


Consistency

Measurement title consistency between communal bibliographic record (Wikipedia) and Franklin's manuscript list - a 'source' list.

Reasoning: For the sake of argument, lists of works of a corpus can vary from place to place. Comparing the Emily Dickinson archive's transcription against Franklin would be next to pointless, though it could be done. We would find match near 100% and the next step would have to be comparing the digital copy to a transcription (or worse, an OCR) of the manuscript books – a step far too laborious and unreasonable, and likely redundant, for a very minimal payoff. In this instance, the digital transcriptions are known to be made by scholars working off of the manuscripts. (See Emily Dickinson Archive about section)

Running a test by varying a sequential, minimal word-matching threshhold for title comparisons, we see a phenonemon emerge. In varying the threshold by ten percent intervals the matching converges to a spot like a limit as it approaches zero. Decrementing the threshold step by less than ten percent does not move the needle towards a higher match. With the exception of missing titles (titles that are not present in the Franklin digital corpus regardless of match percentage), the behavior of the remaining three data quality measurements under the general metric of 'consistency' can be described as almost asymptotic.

The Franklin version of Dickinson's corpus is generally regarded as consisting of 1,789 manuscripts (depending on one's definition of a complete Dickinson work). However the full digital collection available for download from the Emily Dickinson Archive consists of 4,825 transcriptions across several publications of poems and letters, 2,459 of which are attributed to Franklin's variorum, 'The Poems of Emily Dickinson'. Beginning with 100% word match we see the number of possible poem title matches increase, until they level off at 10% match at 1,782 poems – about a 1% discrepancy between the measurement and Franklin's list. Ironically, or perhaps expectedly, relaxing the requirements allows more titles to be included as works identifiable and thus accountable to a bibliographic source list. This is a point where dataset curation can introduce errored models, but knowing the amount of discrepancy at least tempers and makes visible that potential error. For the case of the Dickinson corpus, it's small enough to understand the discrepancy. The missing seven poems turn out to be a case of Johnson (Franklin's variorum predecessor) and the Wikipedia authors choosing alternate titles for poems or titles based on alternate manuscripts of the same poem. (Their choices are interpretations of which of Dickinson's revisions seem like her final ones.)

Here I have chosen not to include a consistency measurement of comparing each work word for word as this is often involves an unreasonable amount of work. Even for the relatively small Dickinson corpus, transcribing each work from Franklin's to compare to digital editions is not only an arduous endeavor, but also rather redundant given the knowledge that expert scholars and graduate students were involved in transcribing Dickinson's manuscripts for the Emily Dickinson Archive.

Just as Coleman points out, the work of data quality is not a single use standard, but rather a cyclical practice that works toward a satisfactory refinement. One inherently expects the work of data refinement to behave in this way, but seeing it in quantitative form shows where hard work pays off versus where it significantly diminishes into pointlessness. Not only that, but seeing the full range of consistency also allows a researcher to pick a point where they feel that they are able to balance consistency, the metric in question, with accuracy. A rewarding outcome of this practice, especially in terms of humanities data sets, is that the qualitative judgment of subject matter experts can be laid bare in a way that is accountable to modeling - the realm of quantification – while demonstrating the point of strong, theoretically grounded humanities interventions. And this series of decisions that eventually produces a new representation of cultural objects is one that is most fully developed by the practice of doing so. In other words, despite its quantitatively-informed foundations, computational models of literature involves developing a techne, a craft, or an art.

Next step - Using low and high ranges of the dq metrics to produce alternate versions of a chronological model of the Dickinson corpus. The end-goal here is to demonstrate the effect of discrepant and missing works on model outputs. Since a model like a word embedding model that involves contextual word information, these differences – (important) varied across dq metric values – show how a model (a Dickinson chronological model in this case) can be altered by understanding the quality of the underlying data.

====================================================================================================

Integrity

