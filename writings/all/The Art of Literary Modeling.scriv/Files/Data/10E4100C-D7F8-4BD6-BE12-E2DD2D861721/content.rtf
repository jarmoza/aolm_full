{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Bold;\f1\froman\fcharset0 Palatino-Roman;\f2\fnil\fcharset0 LucidaGrande;
\f3\froman\fcharset0 Palatino-Italic;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b\fs24 \cf2 Todo:
\f1\b0 \
\
\pard\tx220\tx720\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl0\cf2 {\listtext	1.	}Do readings on contemporary information criterion\
\pard\tx940\tx1440\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl1{\listtext	
\f2 \uc0\u8259 
\f1 	}Read preface to Burnham and Anderson\'92s book 
\f3\i Model Selection and Multimodel Inference\
\pard\tx1660\tx2160\li2160\fi-2160\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl2
\f1\i0 {\listtext	
\f2 \uc0\u8259 
\f1 	}\'93Information theory includes the celebrated Kullback\'96Leibler \'93distance\'94 between two models (actually, probability distributions), and this represents a fundamental quantity in science.\'94 (vii-viii)\
{\listtext	
\f2 \uc0\u8259 
\f1 	}\'93In 1973, Hirotugu Akaike derived an estimator of the (relative) expectation of Kullback\'96Leibler distance based on Fisher\'92s maximized log-likelihood. His measure, now called Akaike\'92s information criterion (AIC)\'94 (viii)\
{\listtext	
\f2 \uc0\u8259 
\f1 	}We focus on Akaike\'92s information criterion (and various extensions) for selection of a parsimonious model as a basis for statistical inference. (viii)\
\pard\tx220\tx720\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl0{\listtext	2.	}- Do readings on Twain\'92s autobiography (popular reception, academic reception, academic critique) \
{\listtext	3.	}- Find elements of both data and metadata in autobiography tei files for analysis\
{\listtext	4.	}- Model those data and metadata elements\
{\listtext	5.	}- Examine code already done for chapter/fellowship in Twain and information criterion\
{\listtext	6.	}- Consider possible visualizations that can be drawn out from modeling results to demonstrate the differences between models of low and high quality\
\pard\tx560\sl264\slmult1\pardirnatural\partightenfactor0
\cf2 \
\

\f0\b Sources:
\f1\b0 \
\

\f3\i Introduction to AIC \'97 Akaike Information Criterion
\f1\i0 : {\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/introduction-to-aic-akaike-information-criterion-9c9ba1c96ced"}}{\fldrslt https://towardsdatascience.com/introduction-to-aic-akaike-information-criterion-9c9ba1c96ced}}\
Zajic, Alexandre; December 27, 2019\
\
AIC works by evaluating the model\'92s fit on the training data, and adding a penalty term for the complexity of the model (similar fundamentals to regularization). The desired result is to find the lowest possible AIC, which indicates the best balance of model fit with generalizability. This serves the eventual goal of maximizing fit on out-of-sample data.}