{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Bold;\f1\froman\fcharset0 Palatino-Roman;\f2\froman\fcharset0 Palatino-Italic;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b\fs26 \cf0 Thoughts from this morning
\f1\b0  (transcribed from audio note on March 31, 2025)\
\

\f0\b Hapax and Digital Editions
\f1\b0 \
\
Research and discussion needed in first chapter on data quality.\
\
Get some examples of hapax legomena and some of digital editions\
that show the strengths of digital editions and how hapax changes the works/views of the works.\
\
Find some critics that talk about hapax/those specific hapax.\
\

\f0\b On why data quality is important for literary studies and natural language models\
\

\f1\b0 Some thoughts on why we want good data for models and for the future of literary study:\
\
One of the problems that we face is that there is a lot of money behind computational models of language at the moment.\
\
Perhaps unsurprising that industry, once it realized it could build a model that could capture the essence of language in terms of syntax and surface meaning, it would take full advantage of it.\
\
The issue that we have that we face both in scientific and humanistic academica, for research purposes and for the purposes of future preservation of literary materials is that that capitalistic force acts as a degradation to our purposes \'96 which are both altruistic and for pure research and discovery.\
\
One of the things that humanities adds to the building of computational models of literature and for general preservation of literature in a digital future is that it does not solely rely on statistics and that it looks closely for meaning and that it thrives in detail and doesn\'92t consider detail to be what is considered in a kind of more cutthroat statistics to be the long tail of a model and not necessarily something that is highly influential in the meanings behind that model.\
\
What we find in literature - and perhaps this is a human trait - is that we place a lot of meaning into a very small package, often in the sentences, sometimes it is even mere, single words. And one of the concepts that speaks to that is coined with the latin phrase, 
\f2\i hapax legomena.
\f1\i0  Hapax legomena usually represents a single word or phrase or sentence that without which the purpose and meaning of a whole text would almost be entirely different. This has been employed to great effect by some of our greatest writers including William Shakespeare. Some of his plays often turn on the use of a single word.\
\
The struggle in the past has been to talk about that in a sensible way. And the struggle now is how to preserve those kinds of understandings in a world of literature where understandings of it - or, as we call models of it - are guided more on the force of central tendency, that is, the weight of how many words are used versus the force behind a singular word \'96 which can almost seem mystical in contrast.\
\
However, it is not impossible to preserve those kind of forces in our current world. What it requires though is more of a human touch. We need to look into our data sources very carefully, and there are a few reasons we would want to do so. There is always the question of, \'93What is the definitive or authoritative source of something?\'94 And we do want to preserve those multiple sources into the future, the arguments about them, and in general come up with an idea of what would be an authoritative source if we were to preserve it for, say, a future database \'96 where many people would want to access it for their own leisure. But also a source computational modeling tools can use to build their own language models and then those models can be used for humans to inquire on. This is basically what we\'92ve built with current large language models and their chat interfaces \'96 ChatGPT, Claude, any of these transformer models.\
\
The problem though is that these models, which are large, don\'92t have any computationally tractable way of accessing the quality of those models and the accuracy of those models. That may change in the future with things like quantum computing which will allow us to exceed current limits of power and computation. But at the moment, and for the foreseeable future, the ironic part about computational intractability is that it also aligns with the limits of capitalism. It\'92s not something that capitalism can overcome, that is to say, because capitalism relies on physical resources. It relies on energy. It relies on server farms, just that infrastructure in general \'96 that is, what we need to build those models. And then certainly to interrogate them in any reasonable way is to rely on cloud computing and high performance computing.\
\
So, one way to attack that problem is to make sure that our data is accurate, that it is of high quality from the start. And again, we can\'92t look to these large models to interrogate them for an understanding of the quality of our data because it is a computationally intractable problem. Reverse engineering answers that these models produce is actually something that is being looked at by many researchers around the world, and certainly there are some piecemeal solutions to that. But getting an idea of the sense of data quality is really an easy win towards that goal.\
\
And the idea of data quality is multifold because there are all sorts of interested parties but one of the most important among them in assessing data quality are people who are experts. They\'92re the subject matter experts overseeing those data sets. In this case, those are the literary studies people. And they can provide those assessments, but they need means of doing so that\'92s not purely algorithmic, that doesn\'92t look at a large body of natural language data and make some generalized assessments. We need subject matter experts to actually add their qualitative expertise into those quality assessments \'96 as counterintuitive as it may seem because we will inevitably be producing quantitative assessments to produce \'93objective\'94 comparisons of quality. To enhance that objectivity we need a subjective look at what makes those measures possible and what is reasonable for those measures. This is what a subject matter expert provides.\
\
So that\'92s an easy win for assessing data quality both for the sake of preservation of accurate digital editions in the future, and for allowing computational models, large language models, etc. to be able to build from accurate data and for it, as much as it can, to be able to make accurate assessments and interpretations from that data.\
\
And again, whether or not those models can make assessments to the degree a human can is up for debate. It\'92s also up for future models and researchers to develop. Can a model look at one word in a sentence and understand that the fact that one word exists within it actually doesn\'92t just change the meaning of the sentence, it might actually change the meaning of the entire work itself? And that is a tough question for us as humans, but given the resources of these models it may not be. And that\'92s a fine thing.\
\
Of course we don\'92t want to place limits on interpretations, but the reason that\'92s a fine thing actually goes back to research itself and it goes back to the idea of knowledge production. One of the problems we have with making these kinds of assessments or looking back to see what kinds of tools we could use to make these kinds of assessments \'96 whether it was in literary studies using theoretical frameworks, different types of readings, looking at different source and different critics, 
\f2\i or
\f1\i0 , if it\'92s just looking for research tools, research methods, going back into research literature is that we continually stumble across the limits of human lifespan and attention span. We can only look at so many things. And it\'92s quite possible and quite likely that in doing our research and making our interpretations we are duplicating somebody else\'92s work.\
\
There is value in doing the work itself, of course, because we build our skills, we build our knowledge set, and we build relationships from that. Those are very valuable things, but if we can avoid duplicating work so that we can build upon that past work or make new work then that would also be a very valuable thing. That\'92s one of the strengths of these large language models and the models that we build from them in the future. They give us the possibility of quick summarization, of being excellent research guides. They can tell us answers to questions like, \'93Hey. Has this been done before?\'94 \'93Has something like this been done before?\'94\
\
To get an answer quite quickly is a very powerful and very useful thing. But in order for that to happen accurately we need to make sure that the underlying data is accurate and that is not something that we can just rely on central tendency for. We do need to make sure that our data is good so that we can use it for all of these kinds of purposes into the future.\
\
}