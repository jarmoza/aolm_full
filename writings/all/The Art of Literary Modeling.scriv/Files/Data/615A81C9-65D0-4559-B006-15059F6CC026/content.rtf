{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;\f1\froman\fcharset0 Palatino-Italic;\f2\froman\fcharset0 Palatino-Bold;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 April 11, 2025\
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f1\i \cf0 Some notes from Coleman
\f0\i0 \
\
\'93While we began our project with the hope of establising repeatable processes for in-line measurements, ultimately we produced a set of measurement types that can be applied at different points in the general process of data assessment: initial assessments that establish the baseline condition of the data, the data model, and the metadata; process controls that assure the integrity of data as it moves between or within systems; in-line measurments taken as part of data processing; periodic measurments that test the integrity of data relationships within a database; and periodic assessments that analyze the sufficiency of database content to meet the ongoing needs of the organization.\'94\
\
This implies that measuring data quality for literary modeling and analysis can take place at several stages for a project: namely the initial in-take of data using (hopefully) generalized measurement scripts, and then continually after each round of textual clean up (if the researcher chooses to do so). In turn, measuring iteratively can convey the degrees of efficacy of each separate clean up process. Since the overall dq is meted out across several metrics this gives a more detailed look at the efficacy of those cleanup processes. This may then for future researchers indicate which process is more worth of their time. Then of course measuring dq at the final iteration, after all cleanup is done, can indicate the dq of the dataset at time of usage for modeling and for subsequent researchers who may want to reuse the now cleaned up dataset.\
\

\f1\i Spatialization and Data Quality Assessment (Voice memo from April 10)
\f0\i0 \
\
Since data quality assessment is done over several metrics, we can easily translate these measurements into a vector space. In turn, we can compare datasets across the various dimensions of that vector space to see where the strengths and weaknesses of each lay. This, of course, can be represented visually. Additionally, weighting schemes for the individual metrics may still be used, but those weightings would have to be treated \'96 in linear algebra terms \'96 as a subspace for proper comparison if it were desired for datasets with different weighting schemes to be compared in a similar vector space. That translation aside the weighting of metrics remains useful and meaningful when we consider how a subject matter expert weighs the relative importance of particular qualities of a dataset. [See the Piper/Algee-Hewitt Goethe paper for unique spatial representation of vector-based text models, entitled, {\field{\*\fldinst{HYPERLINK "https://www.researchgate.net/publication/268152032_The_Werther_Effect_1_Goethe_Objecthood_and_the_Handling_of_Knowledge"}}{\fldrslt \'93The Werther Effect 1: Goethe, Objecthood, and the Handling of Knowledge\'94}}]\

\f1\i \
Transcription from yesterday\'92s (April 10) writing session at Black Bean Deli
\f0\i0 \
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f2\b \cf0 Art of Literary Modeling - The Epistemological Argument
\f0\b0 \
\
Traditionally, the differences between the humanities and sciences has been seen through the lens of the character of the knowledge they produce. This was put to pen by C.P. Snow in his famous \'93The Two Cultures\'94 where the former appears to rely on and produce qualitative judgments while the latter relies on and produces quantitative ones. But these have increasingly been seen as false distinctions \'96 to a point \'96 humanistic knowledge has also increasingly been devalued in the public sphere as its cultural utility is made distant from its economic value. It is still within human judgment to distinguish between qualitative and quantitative knowledge even if the difference just seems to be the lack of fixed meanings of words being used as qualitative characterization in the mind of the beholder and across a quantifiable boundary (i.e. something that\'92s more in degree characteristic X than it is characteristic Y).\
The point here is that qualitative judgments/the scales imputed by qualitative judgment can be reasonably applied and mixed with quantitative measurements if but because the distinction between the two seem to be more a matter of the scale of degree of assumptions being done in the measurement itself. Also, to be considered is how a subject matter expert\'92s expertise may lessen the degree of inaccuracy of a more plainly qualitative measurement and in fact impute a more objective consideration for the measurement.\
With that in mind, we can also question the \'93objectivity\'94 of quantitative measurement in that there are always a series of subjectie interventions that must be made in order to make that quantitative measurement possible. Consider, for instance what it woudl mean to merely count words in a text, or to count unique words. Well, what is a word? Each observation made by the computing instrument must decide what a word boundary is and must confront the messines of  human language. Unsurprisingly, the researchers of well known texts frequently produce difference word counts for them as each of their counting algorithms may have differences in them affected by their own experience and their knowledge of the language context of the text in question. Beyond that, even generalized and reused word counting algorithms still \'93suffer\'94 from their own set of subjective interventions, and it would not matter if an LLM was doing the counting as their own bases for counting words are still produced by humans. Continued use of the LLM counting algorithm merely normalizes one subjective/qualitative form of counting words. So there is no escape from qualitative assessment or seeming \'93inaccuracy\'94, but merely the degree to which you are aware of the subjectivity/biases of the measurement(s) involved.\
Moving on from there to use measurements to make judgments on data quality of a natural language text, we can see that both qualitative and quantitative measurement can be mixed as long as the character of each is noted 
\f1\i and
\f0\i0  perhaps the degree or importance of each measurement in the overall data quality measurement is taken into consideration. For instance, it may be suitable to add weights to diminish or enhance the weight of a more subjective measurement ini the overall data quality calculation.\
This action of weighting is also clearly subjective, or, perhaps better-worded, an interpretive move that frequently happens in science and statistics. Though an even weighting (i.e. within a mean calculation) can be used this activity of weighting is clearly an advantageous act within the process of calculating data quality via a customized data quality framework. There are some measurements that will simply take precedence in the mind of the text\'92s subject matter expert and a higher weighitng for that measurement\'92s value is something naturally follows from that expert\'92s expectation. (This, of course, could be a flawed expectation but such is the nature of making judgments.)\
Let\'92s take a look at a study which might have benefited from a data quality assessment. <study here>\
You can see that these calculations and assessments serve two purposes. The first is to produce a quantitative valuation of a dataset for comparison purposes \'96 especially in a world increasingly with a heightened cultural sensibility for quantitative values. This is both a pure and impure purpose you\'92ll note. It seeks to draw attention to the usefulness and veracity of a dataset of natural language text, the underlying content of which is (often) far more meaningful than its reduction to a mere single number. Indeed, such a reduction is anathema to the subject matter expert and to the cultural and artistic value of the material. But none the less we do seek to convey the value of a dataset to those who seek to learn from it \'96 both in quantitative value in existential value. Whether one sensibility is valued over the other is something we as researchers and archivists can merely promote, but it would be folly to ignore either for the concept of \'93utility\'94 exists even in a qualitative or artist\'92s space. Such meta-judgements also betray themselves as indistinguishable when looked at closely enough. (In the contemporary context, there are clear implications towards use-value of a text as object in terms of capital \'96 fiscal or otherwise \'96 and while that cannot be ignored as industry absorbs large swatchs of NLP datasets, utility is just as universal from the humanistic perspective in terms of social and cultural capital of a particular author, genres, or texts.)\
\
}