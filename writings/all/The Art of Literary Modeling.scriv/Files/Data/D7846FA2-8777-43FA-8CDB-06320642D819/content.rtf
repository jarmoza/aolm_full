{\rtf1\ansi\ansicpg1252\cocoartf2820
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 Once results are computed for each overarching metric (including its submetrics), an evaluation function is called to combine the results in a logical way in order to produce one, unidimensional number for the metric\
\
Each submetric is weighted evenly. And each sub-submetric may calculate its own evaluation to a number in a way that is logical for that sub-submetric. (Weighted average)\
\
Submetrics are folded into a metric via averaging\
\
Each subsubmetric is similarly weighted evenly in creating the final submetric number and then averaged for the final submetric number\
\
Metadata sufficiency is an explicitly internal metric\
\
While textrecord count is one that relies on external comparison with the ur text\
\
Now on to visualizing and writing. The evaluations are now complete on the Project Gutenberg Huck Finn editions.\
\
Overall, they average a 72% quality rating based on metadata sufficiency and text record data quality metrics\
\
These can be separated by edition and also extended now to the Internet Archive editions for further testing}