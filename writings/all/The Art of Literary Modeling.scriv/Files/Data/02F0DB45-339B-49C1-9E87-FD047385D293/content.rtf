{\rtf1\ansi\ansicpg1252\cocoartf2820
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;\f1\froman\fcharset0 Palatino-Italic;\f2\froman\fcharset0 Palatino-Bold;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 Next steps will to write about the two data quality metrics used over Internet Archive, Project Gutenberg
\fs28 , and Mark Twain Project Online editions of 
\f1\i Adventures of Huckleberry Finn
\f0\i0 , and to discuss how the first parts of the Data Quality Assessment Machine (DQAM) (alternate title, \'91Workflow\'92?). This will include the production of graphs to speak about the raw and evaluatory outputs of each metric and then the calculation of their combined metric values.\
\
Afterwards, the latter two pieces of the DQAM should be implemented: DataQualityOutput and DataQualityAssessment. These pieces will consider the metrics and their types and their evaluatory values and automate the quantitative analysis/comparison produced in the above writing.\
\
A diagram of the DQAM should also be produced with descriptions of each of its modules/steps along the way.\
\

\f2\b Writing
\f0\b0 \
\
Two data quality metrics were selected for an initial assessment of a total of fourteen editions of 
\f1\i Adventures of Huckleberry Finn 
\f0\i0 from Internet Archive (IA) and Project Gutenberg (PG) as compared against the heavily-proofed XML edition from The Mark Twain Project Online (MTPO) at University of California, Berkeley: metadata sufficiency and comparing record counts to control records. One important note about this kind of measurement and comparative assessment is that it eschews a commonly held assumption that the most recent version of a digital text is likely the highest quality version of its digital edition. There are many differences across editions to consider aside from the edition\'92s publication date including the overall completeness of the edition, cleanliness of the data itself, the character and amount of metadata, and even the edition\'92s public availability. As we will see, these kinds of considerations closely align with the ideas of data quality measurement. Therefore it is useful to consider multiple editions from the same source in addition to editions from other sources.\

\f1\i Metadata Sufficiency
\f0\i0 \

\f1\i Comparing record counts to control records
\f0\i0 \
One of the challenges of this work is translating the premise of a data quality metric in its initial, more generic context from information science into a form that is relevant to a digitized text as well as the research premise of the project that this quality standard will be used for. So, for instance, measuring metadata sufficiency may require differing code across editions of the same work from different sources since there will more than likely be differences in what metadata was recorded by each digital repository. Furthermore, since data quality metrics themselves have subordinate measurements it is possible and even likely that those sub-measurements will not be compatible with every edition being compared in the data quality assessment. Editions of those works may therefore have to forego including those sub-measurements in the calculation of the overall data quality metric or default them to 100% or 0% (depending on what makes sense for that particular measurement). In addition to that challenge, some metrics may require the existence of a high(er) quality data source to act as baseline for comparison against other editions of unknown quality. Sometimes one lucks out and such an edition exists. Other times when it does not, the process of leave-one-out cross-validation (LOOCV), similar to what is done in machine learning, may be used to determine the highest quality edition that can act as a quality baseline. I will play out both scenarios below using metadata sufficiency and comparing record counts to control records to illustrate this process.  \
For the current experiment, we can say with high certainty that the MTPO edition of 
\f1\i Adventures of Huckleberry Finn 
\f0\i0 is the digital edition most likely to be the highest quality text. This is because it is one that has been worked on by many people over a long period of time and with less economic constraints than a publisher\'92s deadline. Considering the entire publishing history of the novel in its various forms, the MTPO edition stands apart. Because it is such 
\f1\i Adventures
\f0\i0  acts as a useful example of where a highly polished digital edition exists because of high publication volume and high amount of scholarship. This example evokes some fruitful speculation on the role of highly edited dig	ital editions in the world of text modeling. Is it worth the time of researchers to produce one if one does not exist? If one doesn\'92t exist, how else could we determine a so-called \'93ur\'94 edition (or \'93master\'94 edition) of a digital text from which to base measurements of quality?\
Using the MTPO edition as a baseline we can reasonably expect editions from other sources to be of lower quality but exactly how much lower quality they are and why is still both a scholarly useful and practical use case for data quality metrics. Such an edition is also materially different than other editions that are direct from OCR, edited after OCR, or transcribed by eye. We will see below what happens if there is no baseline edition to consider and perform LOOCV on 14 editions of 
\f1\i Adventures 
\f0\i0 being considered. For now however, let\'92s observe what happens when data quality is measured using these metrics and the MTPO edition as a baseline.\
For a metric like metadata sufficiency on a data quality assessment, metadata of the baseline text can be ignored since as a single edition it is inherently 100% sufficient. (The same would go for a lone edition being compared against the baseline.) With this metric requiring to compare mulitple editions from a single source, the experiment proceeds in that metadata sufficiency is measured across all editions of 
\f1\i Adventures
\f0\i0  from Internet Archive and then again across all editions from Project Gutenberg. Metadata sufficiency is measured by taking the mean of its three subordinate measurements: existence and completeness, clarity and quality, and consistency of representation.\
 \
<Breakout how metadata exists/sits/was gathered for IA and PG editions> \
\
Let\'92s break out the measurements here for the sake of the example. As we can see, the metrics themselves can be used for more than just the purpose of a final data quality assessment or seeking the highest quality digital edition. Quality ratings can be found for individual editions, editions from a particular source, or the entire collection of editions the researcher has accumulated. Metadata on each edition, if available, can also be useful to speak intelligently of the quality of a digital edition over metadata variables like \'93creation date\'94. For Internet Archive and Project Gutenberg editions of 
\f1\i Adventures
\f0\i0 , we find:\
\
< Data viz showing subsubmetrics, submetrics, and overall metadata sufficiency percentages for IA and PG>\
\
<Text record count analysis for IA, PG, and MTPO editions>\
\
<Discussion of overall metric calculation>\
\
<Move on to discussion of how LOOCV will be performed for dq assessment>\
\
<Graphs and discussion of LOOCV on 
\f1\i Adventures
\f0\i0 >\
\
<Next steps in this hypothetical process: DQ assessment framework machine explanation with final outputs, weighting, etc.>\
\
\
 The MTPO edition is in TEI XML format}