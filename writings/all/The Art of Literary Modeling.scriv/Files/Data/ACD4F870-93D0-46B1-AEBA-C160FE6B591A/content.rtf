{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Palatino-Bold;\f2\froman\fcharset0 Palatino-Roman;
\f3\froman\fcharset0 Palatino-BoldItalic;\f4\froman\fcharset0 Palatino-Italic;\f5\fnil\fcharset0 LucidaGrande;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue233;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c93333;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl264\slmult1\sb260\pardirnatural\partightenfactor0

\f0\fs24 \cf0 <$ScrKeepWithNext><$Scr_H::2><$Scr_Ps::0>
\f1\b\fs26 Jonathan Armoza\
<$ScrKeepWithNext>Art of Literary Modeling\
<$ScrKeepWithNext>Chapter 1 Opening Sections with Roadmap - Rough Draft\
<$ScrKeepWithNext>11/9/2019\
<$ScrKeepWithNext>\
<$ScrKeepWithNext>The Site for Modeling\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b0\fs24 \cf0 <!$Scr_H::2><!$Scr_Ps::0><$Scr_Ps::1>
\f2 \
	One difficulty in writing about this topic is that I am 
\f1\b defining a new research activity for a field
\f2\b0 . It requires another adjustment for humanities scholarship that relies on computational evidence. Self-reflection on the qualities of the evidence being used and the qualities of the model built upon that evidence can yield stronger results and a more sound basis from which to make interpretive claims. The hypothesis lays itself out plainly, but several questions arise when determining units and measurements for the quality of literary evidence. The first being, Why do this at all? 
\f3\i\b Evidence should have authority
\f2\i0\b0 . A significant basis for the production of literary analysis is ethos whether it be of the author themselves or the sources which an author cites. This prompts another question when considering quantitative measures in a field of study that traditionally produces and relies on evidence that is qualitatively-framed. What imbues quantitative evidence with authority? This work is entitled the 
\f3\i\b art
\f2\i0\b0  of literary modeling for this purpose. There is no generic heuristic for authority. The informed judgment of the creator/user/interpreter of a dataset produces that authority. 
\f3\i\b But
\f1\i0  
\f2\b0 the 
\f1\b products/evidence (?) 
\f2\b0 of this techne or craft can be quantified. \
	Where should scholars performing this craft look for inspiration or influence? As it turns out, the answer is both within the field and from outside of it. {\field{\*\fldinst{HYPERLINK "scrivcmt://EA0D4A09-BAC0-455A-90E4-BB269E1A117E"}}{\fldrslt As is routinely pointed in defenses of the subfield}}, humanists carry important ethical values of their own (not the only ethics/values) that inform the general culture of their field(s). I am not making an argument for or against a set of ethics/values here, but rather that specific sets of research values exist for the humanities and that they need be included in computational work. And this is for two reasons. The first is translational: so that the work can be communicated to fields outside of the humanities and reproduced by them. The second is so other scholars in the field can appreciate or even adopt the interpretations and given evidence to ensure the continuation of knowledge production begun by computational work lest it be shunted aside as parallel if disparate subculture itself. As for the former case of translation, we can borrow means of ascertaining these qualities from outside of the field, if but because fields who are based and versed in quantitative methods and evidence have produced such efforts. The most direct borrowing, and one that this work focuses on is the creation of a data quality framework. Laura Sebastian Coleman, an information scientist, has written on the creation of such {\field{\*\fldinst{HYPERLINK "scrivcmt://1DFC4916-58A0-43C5-9F42-E74C463E9352"}}{\fldrslt frameworks.}} Her academic research and position overseeing the data architectures of private insurance and healthcare companies has given her the abstraction and experience to pen a significant work on the {\field{\*\fldinst{HYPERLINK "scrivcmt://896E5B45-619E-45DC-A408-2523906C9F1B"}}{\fldrslt topic.}} 
\f4\i If reading about a healthcare information scientist in the beginning of a work on literary modeling seems askew \'96 stay tuned.
\f2\i0  And this is where the process of creating a data quality framework for a digital humanities project will begin. As we will see, such a framework does not stop with metrics of data quality. The effect is cumulative, where evidence of quality produces an ethos for subsequent research processes and subsequent research projects. Thinking of this framework as a physical foundation for the work to follow is a useful and (as it turns out) definitional metaphor for the construction of computational models of literature. Even the most complex of things must begin by necessity with a simple form.\
	The creation, modeling, and interpretation of literature all share a characteristic so universal and seemingly innocuous that it would hardly be worth noting if but for the transformational effect of that characteristic. These activities begin and end in socially and physically situated spaces. Space is the beginning of such argumentative activities and structures.  What space contains our arguments? In the late 1990s for instance,   author Michael Pollan ventured to create a space from which to write books \'96 and then he wrote a book about the process of creating that space. What does writing have to do with constructing a computational model of literature? In 
\f4\i A Place of My Own
\f2\i0 , Pollan explores the connections between the physical spaces we inhabit and how they alter our interpretations of the world around us. But also, tellingly, Pollan\'92s own journey depicts how that outside world and its history informs the creation of those spaces. By journey\'92s end, he gazes out a large window at a desk surrounded by his books. It bears beginning a work on the qualities of modeling literature with this ending. Like many works of fiction, the initially chosen perspective imparts a feeling, foreshadows what is to follow. The constructed vantage of an author is laden with consequence for the work of storytelling. In Conrad\'92s 
\f4\i Heart of Darkness
\f2\i0 , a boat ride up the Thames becomes the position from which readers are given to imagine the same journey throughout preceding history all the way to the time of Roman conquest. Chopin\'92s 
\f4\i The Awakening
\f2\i0  begins with a green and yellow parrot in a cage that can speak French, Spanish, English and \'93a language which nobody understood\'94 (1). These are hardly coincidental perspectives. They are deliberate. That is all to say, the work of representation begins with {\field{\*\fldinst{HYPERLINK "scrivcmt://E6BA9DC5-9540-4627-92D7-A7337D0904F0"}}{\fldrslt perspective.}} Pollan uncovers the subconscious, cultural roots of his desire for this particular writing house perspective, tracing it back to the introduction of the lockable writing desk during the European Renaissance, and then to rooms like studies for writing desks and books \'96 private spaces to think while surrounded by the ideas of others and from which one could look upon the world with remove. 
\f1\b Throughout this work I call attention to this analogy not to torture a metaphor of construction (and it will be revisited in different stages of constructing models for literature) but because modeling 
\f3\i is
\f1\i0  metaphorical. The language and motivation by which it occurs is intensely physical and spatial. They operationalize the process of creation. Yet, because of the abstract nature of the mathematics and materials involved (\'93data\'94), this motivation is often lost in the work.
\f2\b0  (possible as footnote?)	\
	Pollan\'92s architect partner \'91Charlie\'92 (Charles Myer of {\field{\*\fldinst{HYPERLINK "http://www.charlesmyer.com"}}{\fldrslt Charles R. Myer and Partners}}) notes at the project\'92s outset that this \'93first fact\'94 of perspective is a \'93key element\'94 of the building\'92s design (Pollan 31). The author prototypes perspective by taking a chair and positioning it at various prospective building points on the property. Transposing this into the context of utilizing or creating a dataset from a field of literature, we see that it is not even the foundation \'96 the dataset itself \'96 that ultimately and significantly informs the research to follow. The dataset\'92s boundaries has already been framed by its creator. It is the 
\f4\i placement and angle
\f2\i0  of our metaphorical chair on that foundation the projects an inward- and outward-reflecting 
\f3\i\b functional perspective 
\f2\i0\b0 amid the field of related literature. 
\f1\b What will we be able to see? And what will we want visitors to our research to see?
\f2\b0  Not only is this prototype an argument (Galey and Ruecker 2010, {\field{\*\fldinst{HYPERLINK "https://doi.org/10.1093/llc/fqq021"}}{\fldrslt https://doi.org/10.1093/llc/fqq021}}, {\field{\*\fldinst{HYPERLINK "https://academic-oup-com.proxy.library.nyu.edu/dsh/article/25/4/405/998338"}}{\fldrslt https://academic-oup-com.proxy.library.nyu.edu/dsh/article/25/4/405/998338}}), but the portion of a dataset we wish to visit and analyze is also an argument. Where do we place our chairs when we want to look at the landscape of a work of literature or a corpus of it? Fully understanding the perspective we want to have when approaching the creation of model on this site of inquiry can help us down the road in the modeling, visualization, and analytical processes. What are our assumptions about our data set? What are our hypotheses? Affixing our perspective at the beginning of the process allows for the objectivity necessary to understand our modeling results and interpretations, as well as giving us a better idea of where we may want to reposition the chair in the next iteration. (A model is easier to rebuild than a house \'96 so here the analogy falters and helpfully so.) And that fixity is imperative for the soundness of the building ahead. In this process in order to see where we are going we must remember where we came from, but the memory must retain a clarity. The work of selecting a portion of a dataset is not new to digital humanities work. Understanding and rating the qualities of the data and metadata in that dataset and of our selection from that dataset is, however. The construction of a data quality framework begins with this assessment.\
\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b \cf0 Data Quality and Laura Sebastian Coleman
\f2\b0 \
\
	But what is data quality? Coleman defines it as \'93the degree to which data meets the expectations of data consumers, based on their intended use of the data\'94 (Coleman, talk slide 8). She renders in perhaps surprisingly subjective terms what will ultimately become a series of quantitative measures. Since there is no escaping bias in measurement, why pretend one can eliminate all of it? Instead, we explicitly define the quality rating of our data in terms of the degree to which it meets our expectation and intended use. Assessment of quality comes through understanding of \'93processes that created\'94 our data, the \'93systems through which\'94 our data was created, \'93the concepts\'94 our data represents, and \'93the known and potential uses of [our] data\'94 (Coleman, talk slide 9). This evaluation is ultimately a means of identifying and understanding the implications of errors in our data. In terms of literary data sets, errors may be both structural and functional. For instance, outliers in categorical fields may be determined to be errors in measurement. One can imagine multiple scenarios for such an error, {\field{\*\fldinst{HYPERLINK "scrivcmt://8DFD8927-C8BE-4B4B-ACAE-44F58888E93C"}}{\fldrslt the predominant source of which for text analysis might be OCR errors}}. Another way of thinking about errors though are misclassified works in a corpus. For example, if a novel falls far from the lexical norms of writings from a period and place it may be the result of a highly unique author, or it could be something else like a misdating or an authorial misattribution. When the scale of inquiry goes into the hundreds and thousands of works, errors like these may not be far behind depending on our understanding of the processes of the creation of that data. As Coleman notes, who transcribed and produced the metadata for a work, and what prescribed methods by which that transcription and metadata production happened become key means to determining the expected correctness of those editorial processes. Further down the line, those errors may produce unintended consequences for modeling and interpreting models. {\field{\*\fldinst{HYPERLINK "scrivcmt://F845EC14-06D3-4BE2-A850-16A3D0BC42A7"}}{\fldrslt Up until this point, the preceding history of prominent computational text analysis }}in literary study has either written this off as negligible, statistical noise or produced flat values of \'93error\'94 (as is common in statistics), neither of which offer a reasonable or sound explanatory foundation from which to base further study of subjected corpora.\
	Furthermore, assessment of data quality is a cyclical process and imbues an unorthodox ethics for working with data. In moving from measurement goals to data collection, data quality calculation, and comparison with an expectation (e.g. a norm), conclusions from assessments inherently require the creation of 
\f3\i\b dataset variants
\f2\i0\b0  that contain 
\f3\i\b corrected
\f4\b0  
\f3\b errors
\f2\i0\b0 . While the manipulation of data can inspire terror in the objectively-minded, the reality is that real world data is messy and working with it requires the mature understanding that our datasets contain creation bias and selections of that data are also inherently biased. Consequently, the assumptions upon which our selected measurements stand contain those biases. If a datum confutes our expectation and intended use, and one can determine that this confutation is produced via error, then \'96 almost counterintuitively \'96 it is incumbent upon us to emend it to proceed ethically (and if possible to report on that error to the data creator). All of the above requires a somewhat intimate knowledge of the items in our data set; if not a knowledge rooted at the closely-read level, then a loose (but flexible) expectation of what a work may contain. There will always be unidentified errors due to lack of expertise (a finite resource), but the closer a researcher can get to understanding a data set, the more trust can be built between them and their intended audience(s) that the modeling and their interpretation of it is sound. Thus re-framed in quantitative measures is also the primary justification as to why people who are trained in literary study are those best positioned to create large-scale, computed models of literature.\
\
	Let\'92s begin with an age-old question from Emily Dickinson scholarship and see if we can get part way to answering it \'96 or, in the very least, what it would take to begin answering it. For the sake of this experiment, I\'92ll flavor the question a bit towards something that can be approximated via quantification: 
\f1\b Can Emily Dickinson\'92s word usage be used to determine a timeline for the authorship of her poems? 
\f2\b0 The stakes are high given the context, but remember, this is just a hypothesis to guide a data quality framework. Ultimately, the aim is not to answer the question of a definitive timeline, but to see if linguistic dimensions of her writing can be shown to estimate one.   Showing they 
\f4\i can
\f2\i0  do so is also irrelevant to the question. That\'92s because the goal of this research process is temporarily and purposefully unidirectional. If the linguistic dimensions I test prove unfruitful towards a positive result, that is a useful finding for further inquiry and one that should be reported on. With those lowered stakes, I\'92ll first select a few measurements which I think can be determinative of providing an answer to that hypothetical question. \
	The first step will be to get a good quantitative feel for Dickinson\'92s vocabulary. A classic bag of words approach to comparing the vocabulary and frequency of words in her poems is a helpful beginning for its simplicity. Here\'92s where you walk around the property and set your chair in various places. You\'92re getting a feel for the data set using some very basic properties and perspectives. And why? So you know your data before you start throwing it into some more advanced model. Don\'92t let some proposed computational model in-the-waiting guide your choices of selection and measurement; let it be reading that first guides you. In a bag of words model (and it is 
\f4\i already
\f2\i0  a model), one counts the instances of words in a textual body \'96 a poem in this example \'96  to form a list of the words and their respective counts. (Punctuation is, of course, also countable, and I will do so in the instance of Dickinson\'92s poems since punctuation is used to such effect.) Mathematically this list can be thought of as a geometrical construct called a vector. Think of it like a line that begins at the origin of an X,Y graph that extends in a particular direction with a set length. In physics, vectors are considered to represent a force moving in a particular direction with a 
\f4\i magnitude
\f2\i0 . The latter is helpful to consider for the next transformational step for a vector in our bag of words. In order to remove the complication length presents to comparability when counting the words of poems, the vectors undergo a transformation of scale. Each number of the list is divided by the sum of its numbers. This has a \'93normalizing\'94 effect that showcases the most basic, proportional components of each vector\'92s force. \
Next I want to examine how these vectors are related in geometric space. Since each list of numbers extends well beyond three or even four in size (i.e. the physical dimensions in which we experience existence) some imagination is required here, particularly if one wants to visualize the results. But at this point there is no need to do so. Let\'92s take a very basic and age-old algorithm, 
\f4\i K-means Clustering
\f2\i0 , to understand the similarities and differences of these vectors. There are more advanced and even more accurate/interpretable means of clustering vectors (displaying their closeness), but again, the thought here is to keep our methods as basic as possible in order to get a feel for the data. The Dickinson corpus of poems is one complicated by authorship, publication history and differing approaches to their study. The Emily Dickinson Archive provides lightly marked up TEI editions of each manuscript as identified by the several iterations of popular and academic publication of her works. One of the latest and most comprehensive is R.W. Franklin\'92s 
\f4\i The Poems of Emily Dickinson 
\f2\i0 (2005). Franklin\'92s edition proposes a core 1,789 \'93distinct poems\'94 (\'93Emily Dickinson Archive\'94). (I will outline the history of Dickinson\'92s work and its possible complications for our modeling a bit later on.) In order to perform k-means clustering however, one must pre-determine a number of clusters. Typically, the approach to determining the correct number is iterative and is in of itself a hypothesis; like proposing a room layout for our yet-to-be constructed building given all the considerations that went into our perspective. In the case of a building, we would consider possible views, the path of the sun and natural lighting. Here, the \'93perspective\'94 is R.W. Franklin\'92s selection of works and editorializing. To give a preview of the problem, Franklin\'92s collection is both looked to as a watershed for Dickinson studies and subsequently it also much critiqued as too restrictive. The beginnings of our literary model\'92s perspective is already both bibliographic and ideological \'96 as are all models of history.\
\

\f1\b Consistency\
\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2\b0 \cf0 	Not all of Coleman's categories for data quality readily fit with the criteria of literary modeling. New categories can always be theorized and implemented based on a project, but 'consistency' and 'integrity' adapt easily to the practice.\
\
	What might 'consistency' look like in a project considering Emily Dickinson's works? Let's consider a regular problem in modeling literature: the composition of corpora. What's in and what's out? Lists of works of a corpus can vary from source to source let alone the actual textual contents of each of those works. One simple first stab for Dickinson would be to compare a less certain, but communal record of her oeuvre with that of a master list. Why would we want to do so? The stability and accountability of digital data sets for literary research as of this writing still has much room for growth. For instance, the notion of a data quality check itself also can consist of checking the accuracy of metadata. The status of Dickinson metadata is a common scenario. It is scattered around the internet or available in book form if one is able to access and transcribe it. Wikipedia's own list is widely available and somewhat comprehensive. (It doesn't delve into the kinds of detailed and critical distinctions made by Martha Nell Smith, for example.) It's also easily comparable to R.W. Franklin's and Thomas Johnson's compendiums of Dickinson's manuscripts \'96 comprehensive, if criticized 'master' source lists.\
\
	A phenomenon emerges from a test that compares work titles between the lists. The metric itself, something I will call 'title consistency', is simply a minimum of matching sequential words between titles across two lists. The closest matching titles are determined and the percentage of discrepancy calculated. After shifting the minimum matching word threshold by ten percent intervals (10% of sequential words must match, 20% must match, etc.), the number of matches converges to a spot like a limit as it approaches zero. Likely a generalizable phenomenon, the curve produced has its own signature rate of match success for the corpus and metadata. Decreasing the threshold below ten percent does not move the needle towards a higher match. With the exception of missing manuscript IDs (Franklin IDs not present in the Wikipedia list), the behavior of the remaining three data quality measurements under the general metric of 'title consistency' (matching titles, mismatching titles, and percentage of titles discrepant from the master source list) can be described as almost asymptotic.\
\
	The Franklin version of Dickinson's corpus is generally regarded as consisting of 1,789 manuscripts (depending on one's definition of a complete Dickinson work). However the full digital collection available for download from the Emily Dickinson Archive consists of 4,825 transcriptions across several publications of poems and letters, 2,459 of which are attributed to Franklin's variorum, 'The Poems of Emily Dickinson'. Decreasing from 100% required word match we see the number of possible poem title matches increase, until they level off at 10% match at 1,782 poems \'96 about a 1% discrepancy between the measurement and Franklin's master list. Ironically, or perhaps expectedly, relaxing the requirements allows more titles to be included as works identifiable and thus accountable to a bibliographic source list.\
\
	This is an early juncture in the modeling process where dataset curation can propagate errored models. Knowing the amount of discrepancy at least tempers and makes visible that potential error. For the case of the Dickinson corpus, it's small enough to understand the discrepancy. The missing seven poems turn out to be a case of Johnson (Franklin's compendium predecessor) and the Wikipedia authors choosing alternate titles for poems or choosing titles based on alternate manuscripts of the same poem. Their choices are interpretations of which of Dickinson's revisions seem like her final ones, and none the less valid ones. Alternates they may be, but Wikipedia's curated poem list displays a quality of research familiar to literary scholars: the ultimate subjectivity that is the composition of a data set. Johnson's prior choices and Wikipedia's list also provide a valuable service for the research wary of diving into tomes like Franklin and Johnson: a viable, alternative authority for making choices between variant poems. Depending on the level of one's work, these may not be decisions one wants to make.\
\
	Here I have chosen not to include a consistency measurement of comparing each work word for word as this is often involves an unreasonable amount of work. Even for the relatively small Dickinson corpus, transcribing each work from Franklin's or Johnson's or Smith's to compare to digital editions is not only an arduous endeavor, but also potentially redundant given the knowledge that expert scholars and graduate students were involved in transcribing Dickinson's manuscripts for the Emily Dickinson Archive. In this case, the title consistency metric acts as a safeguard for that unknown potential for modeling error due to faulty data. The measurements themselves have allowed me to focus in on the seven mismatches and three to four missing manuscript IDs to make my own decisions about tidying up the corpus I want to model.\
\
	Just as Coleman points out, the work of data quality is not a single use standard, but rather a cyclical practice that works toward a satisfactory refinement. One inherently expects the work of data refinement to behave in this way, but seeing it in quantitative form shows where hard work pays off versus where it significantly diminishes into pointlessness. Not only that, but seeing the full range of consistency also allows a researcher to pick a point where they feel that they are able to balance consistency, the metric in question, with accuracy. A rewarding outcome of this practice, especially in terms of humanities data sets, is that the qualitative judgment of subject matter experts can be laid bare in a way that is accountable to modeling - the realm of quantification \'96 while demonstrating the point of strong, theoretically grounded humanities interventions. And this series of decisions that eventually produces a new representation of cultural objects is one that is most fully developed by the practice of doing so. In other words, despite its quantitatively-informed foundations, computational modeling of literature involves developing a techne, a craft, or an art.\
\
	In that light, let\'92s take a look at a few plausible data quality metrics under Coleman\'92s categories that can help us build a foundation for a model of Dickinson\'92s poems, namely \'91consistency\'92 and \'91integrity.\'92 
\f4\i (Code notes: See dickinson_consistency.py and dickinson_integrity.py)
\f2\i0  The kinds of data quality metrics that emerge for humanities studies are either self-enclosed or look to a master or source list for verification. In the case of consistency for the Emily Dickinson Archive corpus, we can think of a 
\f4\i consistency
\f2\i0  in at least three ways, the simple counts of documents across collections or the matching of titles and manuscript IDs across collections. 
\f4\i Integrity
\f2\i0  is a far more enclosed metric, looking for consistency in metadata across files and the accountability of the words of the corpus to a dictionary of almost 500K English words.\
\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b \cf0 Notes and the Work Ahead 
\f2\b0 \
To do:\
 (- Find the 1 duplicate poem in the 1790-work filtered corpus)\
- Discuss k-means (what it\'92s doing), the idea of cluster silhouette (and average silhouette), display silhouette graphs (See {\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"}}{\fldrslt https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html}})\
- Look at top N most frequent words and determine cut off based on legomena\
- Consider legomena as another criterion for identifying uniqueness\
- Use top N most frequent words to k-means cluster and scikit SelectFromModel() functionality to determine graphable scatter plot (for cluster graph)\
    - Using top N should aid silhouette analysis\
- Graph silheoutte averages by doing continued iteration on clusters increasing by one example [do X k-means computations varying from 2 clusters to 200 clusters] and then graphing to understand the shape of averages (where it levels off/stabilizes, for instance\
- Some examination of the poems in the poem clusters themselves to bring out qualities through close reading of them\
- This work is to be understood as getting a feel for the dataset in order to move to the next point:\
- Use this basic information to choose a few simple notions from Coleman\'92s data quality framework(s)\
- Include citations from works on Exploratory Data Analysis\
	- Initial Data Analysis (IDA, a subset of EDA)\
			- Common in statistics\
			- Measures: \cf2 \ul \ulc2 https://en.wikipedia.org/wiki/Cronbach%27s_alpha\cf0 \ulnone \
- Discussion of Dickinson dataset(s) origins\
- Discussion of Dickinson dataset(s) themselves\
- Postulating data quality metrics for the Dickinson dataset(s)\
- Data quality assessment of the Dickinson dataset(s) based on that framework\
\
Second Part\
\
- Demonstrate whether or not based on the cluster analysis and scholarly research if there are any suggested poem authorship timelines\
- Findings from the models\
- MetaFindings that utilize the data quality metrics to better inform those original findings\
\
Project Gutenberg section\
\pard\tx220\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}The problem of provenance \
\pard\tx940\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}Specifically of the story of Gutenberg and how the editorial community works. Examples of trying to trace provenance\
\pard\tx220\tx720\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}What is a text? Does a few typos/ocr errors erase its thingness? \
\pard\tx940\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}A close reading scholar would likely say no, but if we are to decompose a text into its word components in order to say something about them, then degree of exactitude does matter\
{\listtext	
\f5 \uc0\u8259 
\f2 	}If we are to make a model of those components for further interpretation and use then that effects and implications of that degree of exactitude stretch further into the future/distance/present.\
{\listtext	
\f5 \uc0\u8259 
\f2 	}If the criterion for making interpretations becomes this form of measurement then (reasonable and appropriate levels of) determining exactitude become incumbent and ethical acts\
{\listtext	
\f5 \uc0\u8259 
\f2 	}This is not necessarily \'91solving\'92 DH, but it is an attempt to add some structure to its computational beliefs, interpretations, and conclusions.\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b \cf0 Talking about the work on the Dickinson corpus curation\
\pard\tx220\tx720\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl0
\f2\b0 \cf0 {\listtext	\uc0\u8226 	}integrity\
\pard\tx940\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl1\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}integrity by match of digital files to bibliography (manuscript book order)\
{\listtext	
\f5 \uc0\u8259 
\f2 	}integrity by year (this might be for model quality) - matching estimated year by Franklin/Johnson to lexical year model estimation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\li2160\fi-2160\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl2\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}integrity by match of manuscript id number to year distribution list\
\pard\tx220\tx720\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl0\cf0 {\listtext	\uc0\u8226 	}consistency\
\pard\tx940\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl1\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}title matching\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\li2160\fi-2160\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl2\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}Known manuscripts missing from digital corpus\
\pard\tx2380\tx2880\tx3600\tx4320\li2880\fi-2880\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl3\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}known_non_franklin_ids = ["A13-6","A13-8", "excluded","A13-2"] (see dickinson_consistency.py)\
\pard\tx940\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls2\ilvl1\cf0 {\listtext	
\f5 \uc0\u8259 
\f2 	}text matching\
{\listtext	
\f5 \uc0\u8259 
\f2 	}corpus-manuscript composition\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b \cf0 Consequences of Data Quality Curation on Modeling\
\
Work done on November 25\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2\b0 \cf0 - Built data quality \'91integrity\'92 metric (dickinson_integrity.py)\
- Built generalized DickinsonPoem collection object DickinsonCollection (dickinson_collection.py)\
- Some more on the consistency metric (dickinson_consistency.py)\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b \cf0 \
Next steps:\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2\b0 \cf0 - Use initial metrics to discuss Coleman\'92s data quality framework in terms of digital humanities projects - e.g. writing\
- Call 112 Patchogue Nissan 2000 about reward, talk with manager\
- Pentatonix album\
- Flu shot\
- Figure out which of the manuscript IDs and revisit (1742/1789 matches between digital corpus and the transcribed Franklin manuscript list)\
\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b \cf0 *** Where you stop in a data quality assessment depends on how much ground-data you have to test against. It may be the case that there are no further steps after assessment \'96 if one can make one at all. And if one can\'92t, then the best one can do is to offer one\'92s informed sense of the partiality, cumulative discrepancies or errors of the dataset.
\f2\b0 \
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0 \cf0 <!$Scr_Ps::1>}