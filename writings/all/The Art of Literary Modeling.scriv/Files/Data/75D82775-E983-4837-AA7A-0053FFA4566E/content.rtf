{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\froman\fcharset0 Palatino-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs24 \cf2 Data quality as a function of processes happening to a text over time\
and changing the structure/characteristics of the data over time\
  - each graph is the next stage in the text cleaning process\
\
First idea - change in slope to word frequency vectors as a text is cleaned up\
- in very discrete small steps\
- slope is gradient field vector (each component is partial derivative with respect to its dimension - x,y,z, etc.\
\
\
Next, determine information loss/likelihood function in order to perform model selection via information criteria\
\
Could be understanding the difference in regression lines as new text frequency vectors are added with further cleanup\
  - change in correlation between the vector (use PCA) and text time\
\
Secondary idea - change in magnitude of vector over text time\
  - each graph is the next stage in the text cleaning process\
\
If you can quantify the information loss over time, we can get discrete measures of how much these changes will change further modeling down the line\
\
Data quality becomes a function of text cleaning vs information loss\
  - Some things like the header/footer may be excusable but maybe not more internal cleaning methods. For instance, removing stop words or lemmatizing may be too lossy.}