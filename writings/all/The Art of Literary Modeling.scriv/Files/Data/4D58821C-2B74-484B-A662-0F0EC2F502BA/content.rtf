{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Bold;\f1\froman\fcharset0 Palatino-Roman;\f2\fswiss\fcharset0 Helvetica;
\f3\froman\fcharset0 Palatino-Italic;\f4\froman\fcharset0 Palatino-BoldItalic;\f5\froman\fcharset0 Times-Roman;
\f6\fnil\fcharset0 LucidaGrande;\f7\fnil\fcharset0 LucidaGrande-Bold;}
{\colortbl;\red255\green255\blue255;\red251\green0\blue23;\red16\green128\blue214;\red0\green0\blue0;
\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgenericrgb\c98600\c0\c9100;\csgenericrgb\c6100\c50000\c84000;\csgray\c0;
\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b\fs28 \cf0 Chapter 1: Data Quality for Literary Models
\f1\b0\fs24 \
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b \cf0 Section title: 
\f1\b0 Introduction\

\f0\b Section purpose:
\f1\b0  Introduces the ideas of why this chapter exists. (Should this be moved to the general dissertation introduction?)\
\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf0 <$Scr_Ps::0>
\f1 	The study of literature in the West has gone through varying critical paradigm shifts over the last century. \cf2 <Needs examples cited to be relevant claim?>\cf0 \
\
Within the last twenty years, the scope and variety of studied literature has not only expanded but rapidly has also become more available in digital form. \cf2 <Needs citation to be relevant claim?>\cf0  \
\
In tandem, the machines we read them on have become capable of running more and more advanced algorithms that process an increasing amount of them for the purposes of study. \
\
These changes have altered the physical, cognitive, and cultural processes used in their interpretation. \
\
Despite a change of medium for its objects of study, this transition is yet again an instance of a once new, critical school becoming the old, and then meeting a new one. \cf3 <Is this a strawman? Are postmodern theory people necessary the old school unless they embrace modern digital study?>\cf0 \
\
This liminal moment begets a push and pull between means of defining meaning and quality for that literature. \cf4 What is literature when it becomes digital data? Does our reading of it change when it does? \cf0 Where do we look for the answer to and how do we understand the old question, \'93What is \'91literary\'92?\'94 \cf4 Is it even a relevant question anymore? \cf0 It is an idea created, stratified, and embellished across socioeconomic class of the author, publisher, and reading audience. Business and various interested institutions, academic and otherwise have constructed the notion \'96 a function of both economy and social prestige. \
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf4 <!$Scr_Ps::0>
\f1 \
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf0 <$Scr_Ps::0>
\f1 	One of those paradigms in the early twentieth century that had doubt cast upon it is formalism and its various iterations. At its heart, formalism promises that meaningful structure and aesthetic can be found objectively in the writing on the page. The doubt came from the situatedness of the author\'92s prose and the person reading and interpreting it. These are both reasonable doubts for formalism\'92s hermeneutic tenets. However, the structural and aesthetic analysis that can be done often denote irrefutable qualities that do clearly exist in the writing, regardless of that situatedness. In fact, looking back at the doubt cast upon those formalist methods, we find that the doubt itself is just as culturally situated as its target. We must therefore conclude that our analysis and interpretations of literature irrevocably denote qualities imbued by our own times and experiences, that meaningfulness itself is situated. How we choose to engage in analytical and interpretive method is a matter style but also a function of its intended outputs. With that understanding, we can incorporate seemingly alien methods like algorithms and artificial intelligence to look at the literatures that we value, and from there what can we do to determine perhaps new meaningful structural and aesthetic qualities in the paradigm of reading literature as data.\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf4 <!$Scr_Ps::0>
\f1 \
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf0 <$Scr_Ps::0>
\f1 	A new layer of qualitative comprehension is produced via our treatment of literature as data. Depending on the context of study, there are numerous ways to understand our expectations of a dataset. Consider a business that runs web services for their clients that relies on a a daily stream of new data. While there are similarities in the relationship between the business/client and author/reader, the factor of time becomes much more important to the business. Time exists in a literary data set in multiple forms 
\fs22 \'96
\fs24  be it publication history or in the more abstract notion of narrative time \'96 but the action of prediction has a very distinct utility for the business\'92 data whereas for the literary data set it 
\f3\i may
\f1\i0  bear a less crucial one. Still, as the past years of research in digital humanities and cultural analytics have used prediction to great effect ine exploratory data analysis, uncovering macro trends in a multitude of characteristics of literature and the people involved in its creation. One could ask what purpose would it serve to predict something that is already concrete and whole other than one to hone the action of prediction itself? In addition to uncovering macrotrends, cultural analytics at that scale serve that methodological purpose as well. Aside from newfound materials for a deceased author, there is no daily data coming in to refresh our understanding of their works. In that way of thinking, it becomes to look inward at the work itself and just what it is that depicts meaningful quality for the study of text as data. Those qualities might differ from work to work, from author to author, from researcher to researcher. There may be, however, some generally useful, a posteriori categories of quality to attend to that could help us bolster that understanding of \'93literary.\'94\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf0 <!$Scr_Ps::0>
\f1 \
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b \cf0 Section title:
\f1\b0  Why quality?
\f0\b \
Section purpose:
\f1\b0  \cf4 Intro on reasoning for data quality assessments
\f0\b \cf0 \
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b0 \cf0 \
	One significant challenge for qualitative interpretation of computational models of literature is the difficulty in ascertaining the condition of the underlying data set(s) from which those models are produced. \cf4 Analysis of the complex data more typical to the contexts of humanities study (e.g. texts, artworks) \'96 referred to more often as 
\f3\i hermeneutics
\f1\i0  \'96 has always used quantitative measurement even when it appeared in \'93qualitative\'94 form. The work of comparison that analysis involves makes the use of proportionality inevitable. In truth, the difference between \'93qualitative\'94 and \'93quantitative\'94 analysis is not very clear, nor are the terms themselves so well-delineated. This confused basis sitting at the root of research signals the urgency of clearly understanding how our culture and its objects are described \'96 and thus also 
\f3\i defined
\f1\i0  in an age where information is ineffably inflected by computation. As the heuristics of data processing and information delivery scale upward, our sense of the quality of that data and the conclusions we can draw from it also fades more and more. Yet our needs for information are sometimes so pressing, be they research-, business-, or journalism-driven, that the question lingers as to the quality of the data we rely on for that information. And there are many means researchers and industry professionals have developed to assess that quality. However, the key to all of those means is the 
\f3\i context
\f1\i0  of the data and resultant information \'96 the context of a data source, the context of the tools used to measure and filter that data source, and the cultural context from where interpretations of those measurements and filtering occurs from. For computational text analysis, a form of humanities data analysis experienced now by people around the world via news reporting, a comprehensive means of data quality assessment has not been developed. One reason for this is that this form of data analysis, often due to its small data scale and purported lack of need for the exactitudes of science and business, has been deemed \'93soft\'94 or too perspectival to be verifiable  \'96 and thus not as worthy of attention and methodological development. This question of data quality can no longer be allowed to linger given the now-widespread consumption and influence of humanities data analysis. There are ramifications beyond the potential for inaccurate interpretations and conclusions for research that include how humanities disciplines are viewed and in turn sustained through funding and cultural support. The writing below is an attempt to provide an answer for the question of what data quality for the humanities could look like. It is an exploration where the \'93qualitative\'94 interests of humanities research take precedent even if they inevitably stand on the shoulders of quantitative methods \'96 because, of course, they always have just not so explicitly.\
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b Section title:
\f1\b0  Data quality in other fields\

\f0\b Section purpose:
\f1\b0  Discussion of concepts as they exist in those fields ad how they are continguent on context\
\
\pard\tx560\tx1120\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf0 <$Scr_Ps::0>
\f1 	Where should scholars performing this craft look for inspiration or influence? As it turns out, the answer is both within the field and from outside of it. {\field{\*\fldinst{HYPERLINK "scrivcmt://DFA5B68D-27C3-4463-A341-9FE2E4142EB4"}}{\fldrslt As is routinely pointed in defenses of the subfield}}, humanists carry important ethical values of their own (not the only ethics/values) that inform the general culture of their field(s). I am not making an argument for or against a set of ethics/values here, but rather that specific sets of research values exist for the humanities and that they need be included in computational work. And this is for two reasons. The first is translational: so that the work can be communicated to fields outside of the humanities and reproduced by them. The second is so other scholars in the field can appreciate or even adopt the interpretations and given evidence to ensure the continuation of knowledge production begun by computational work lest it be shunted aside as parallel if disparate subculture itself. As for the former case of translation, we can borrow means of ascertaining these qualities from outside of the field, if but because fields who are based and versed in quantitative methods and evidence have produced such efforts. The most direct borrowing is the creation of a data quality framework. Laura Sebastian Coleman, an information scientist, has written on the creation of such {\field{\*\fldinst{HYPERLINK "scrivcmt://3AFA1F72-36F5-4DF8-A07A-641B12714DC2"}}{\fldrslt frameworks.}} Her academic research and position overseeing the data architectures of private insurance and healthcare companies has given her the abstraction and experience to pen a significant work on the {\field{\*\fldinst{HYPERLINK "scrivcmt://54D21120-D6A8-4BDC-99BB-DCEE84061737"}}{\fldrslt topic.}}  This is where the process of creating a data quality framework for a digital humanities project will begin. As we will see, such a framework does not stop with metrics of data quality. The effect is cumulative, where evidence of quality produces an ethos for subsequent research processes and subsequent research projects. Thinking of this framework as a physical foundation for the work to follow is a useful and (as it turns out) definitional metaphor for the construction of computational models of literature. Even the most complex of things must begin by necessity with a simple form.\
	But what is data quality for information scientists? Coleman defines it as \'93the degree to which data meets the expectations of data consumers, based on their intended use of the data\'94 (Coleman, talk slide 8). She renders in perhaps surprisingly subjective terms what will ultimately become a series of quantitative measures. Since there is no escaping bias in measurement, why pretend one can eliminate all of it? Instead, we explicitly define the quality rating of our data in terms of the degree to which it meets our expectation and intended use. Assessment of quality comes through understanding of \'93processes that created\'94 our data, the \'93systems through which\'94 our data was created, \'93the concepts\'94 our data represents, and \'93the known and potential uses of [our] data\'94 (Coleman, talk slide 9). This evaluation is ultimately a means of identifying and understanding the implications of errors in our data. In terms of literary data sets, errors may be both structural and functional. For instance, outliers in categorical fields may be determined to be errors in measurement. One can imagine multiple scenarios for such an error, {\field{\*\fldinst{HYPERLINK "scrivcmt://BACCDDF4-3A45-4675-95CD-DCA4BCA2F1A2"}}{\fldrslt the predominant source of which for text analysis might be OCR errors}}. Another way of thinking about errors though are misclassified works in a corpus. For example, if a novel falls far from the lexical norms of writings from a period and place it may be the result of a highly unique author, or it could be something else like a misdating or an authorial misattribution. When the scale of inquiry goes into the hundreds and thousands of works, errors like these may not be far behind depending on our understanding of the processes of the creation of that data. As Coleman notes, who transcribed and produced the metadata for a work, and what prescribed methods by which that transcription and metadata production happened become key means to determining the expected correctness of those editorial processes. Further down the line, those errors may produce unintended consequences for modeling and interpreting models. {\field{\*\fldinst{HYPERLINK "scrivcmt://82553D03-2F33-4A39-AA7D-EBC0632B0B5A"}}{\fldrslt Up until this point, the preceding history of prominent computational text analysis }}in literary study has either written this off as negligible, statistical noise or produced flat values of \'93error\'94 (as is common in statistics), neither of which offer a reasonable or sound explanatory foundation from which to base further study of subjected corpora.\
	Furthermore, assessment of data quality is a cyclical process and imbues an unorthodox ethics for working with data for literary studies. In moving from measurement goals to data collection, data quality calculation, and comparison with an expectation (e.g. a norm), conclusions from assessments inherently require the creation of 
\f4\i\b dataset variants
\f1\i0\b0  that contain 
\f4\i\b corrected
\f3\b0  
\f4\b errors
\f1\i0\b0 . While the manipulation of data can inspire terror in the objectively-minded, the reality is that real world data is messy and working with it requires the mature understanding that our datasets contain creation bias and selections of that data are also inherently biased. Consequently, the assumptions upon which our selected measurements stand contain those biases. If a datum confutes our expectation and intended use, and one can determine that this confutation is produced via error, then \'96 almost counterintuitively \'96 it is incumbent upon us to emend it to proceed ethically (and if possible to report on that error to the data creator). All of the above requires a somewhat intimate knowledge of the items in our data set; if not a knowledge rooted at the closely-read level, then a loose (but flexible) expectation of what a work may contain. There will always be unidentified errors due to lack of expertise (a finite resource), but the closer a researcher can get to understanding a data set, the more trust can be built between them and their intended audience(s) that the modeling and their interpretation of it is sound. Thus re-framed in quantitative measures is also the primary justification as to why people who are trained in literary study are those best positioned to create large-scale, computed models of literature.\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\fi720\sl264\slmult1\pardirnatural\partightenfactor0

\f2 \cf4 <!$Scr_Ps::0>
\f1 \
Coleman bases the data quality assessment framework she describes on several previous models for data quality constructed by information science researchers. Looking to the data quality framework described in 
\f5\fs26\fsmilli13333 \cf5 \expnd0\expndtw0\kerning0
\'93Beyond Accuracy: What Data Quality Means to Data Consumers\'94 
\f1\fs24 \cf4 \kerning1\expnd0\expndtw0 by Richard Wang and Diane Strong, Coleman lists four principle categories of data quality which will be useful for our purposes. Each category has its own concerns and specific \'93dimensions.\'94 The categories and subordinate dimensions were all the product of extensive survey work with data consumers, and they each hold differing definitional value depending on the type of data being considered and its intended use. These categories are 
\f3\i intrinsic
\f1\i0 , 
\f3\i contextual
\f1\i0 , 
\f3\i representational
\f1\i0 , and 
\f3\i accessibility data quality
\f1\i0 . Intrinsic data quality is \'93the extent to which data values are in conformance with the actual or true values\'85Intrinsically good data is accurate, correct, and objective, and comes from a reputable source.\'94 (Coleman, Appendix B, 1). Its dimensions include 
\f3\i accuracy
\f1\i0 , 
\f3\i objectivity
\f1\i0 , 
\f3\i believability
\f1\i0 , and 
\f3\i reputation
\f1\i0 . Contextual data quality is \'93the requirement that data quality must be considered within the context of the task at hand, understood largely as the extent to which data are applicable (pertinent) to the task of the data user\'85The focus of contextual DQ is the data consumer\'92s task, not the context of representation itself\'94 (Coleman, Appendix B, 1). Its dimensions include 
\f3\i amount of value-added
\f1\i0 , 
\f3\i relevancy
\f1\i0 , 
\f3\i timeliness
\f1\i0 , 
\f3\i completeness
\f1\i0  and 
\f3\i appropriate amount of data
\f1\i0 . Representational data quality is \'93the extent to which data is presented in an intelligible and clear manner\'94 (Coleman, Appendix B, 1). \'93[T]he system must present data in such a way that it is easy to understand (represented concisely and consistently) so that the consumer is able to interpret the data\'94 (ibid). Its dimensions include 
\f3\i interpretability
\f1\i0 , 
\f3\i ease of understanding
\f1\i0 , 
\f3\i representational consistency
\f1\i0 , and 
\f3\i representational conciceness
\f1\i0 . And finally, accessibility data quality \'93emphasizes the importance of the role of systems.\'94 It is \'93understood as the extent to which data is available to or obtainable by the data consumer\'94 (Coleman, Appendix B, 1).  Its dimensions include 
\f3\i accessibility
\f1\i0  and 
\f3\i access security
\f1\i0 .\
\
What makes such a configuration of categories readily useful for literary study is its reflection of the roles and agencies of all those involved in the creation, filtering, and consumption of literary data. \
\
Each data quality category\'92s dimensions in turn allows us the means of measuring the success of one of those agents in living up to the 
\f3\i principles
\f1\i0  behind each role-contextualized standard of quality. A few generalized, example possibilities illustrate this realization nicely. For instance, intrinsic data quality may be a measure of the success of matching an iteration (digitized copy, alternate edition, etc.) of a text to a physical or digital source that is considered to be the primary/reputable edition. Contextual data quality may be whether the metrical characteristics of the data set make it viable for the selected modeling task to follow. Representational data quality may ask several questions. Is the data collected from the text easily interpretable/understandable? Is it arranged in such a way to make it easily interpretable/understandable? And for when that metadata is displayed via interfaces, is it presented in such a way to make it easily interpretable/understandable? (And yes, all of the aforementioned can be quantified and/or categorized.) One can also imagine a whole ranking for accessibility quality via several avenues and understandings on physical accessibility. Is the consumer of literary data able to access a literary data set? The answer to which represents a well-known but, without a data quality configuration, overlooked standard for quality when performing computational text analysis. What is the source of the data set and what are the barriers in place to access it? Is it on a free site? Free but requires a login? Institutional? Paywalled? Limited to individual requests? Or maybe even not available at all? While such a standard may not immediately display its effect on quality of subsequent modeling, surely a \'93low\'94 rating for accessibility quality would help us point researchers towards more accessible data sets. While some categories of data quality and their respective measurements may more inform the modeling process, others may more inform the internal and external ethics of the data. From here it will be useful to discuss each category of data quality and play them out via example scenarios for the sake of both demonstration and discussion of the ramifications that follow their implementation.\
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b Section title:
\f1\b0  Intrinsic-Data Quality;  Or, How to Count with Words\

\f0\b Section purpose:
\f1\b0  Introduction of the \'93intrinsic\'94 category of data quality and a demonstration of it over 
\f3\i The Adventures of Huckleberry Finn
\f1\i0 \
\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\fi720\sl264\slmult1\pardirnatural\partightenfactor0
This story has to begin with the counting of words. What do we count when we count words? And why are we doing it? Are we obliterating meaning in the search of new truths? Coleridge felt strongly on the matter, just in a different context. In his writing, 
\f3\i A dissertation on the science of method
\f1\i0 , he speaks of the alphabetical ordering of knowledge in encyclopedias as the destruction of original information \'96 and certainly the destruction of data quality. His metaphor rings familiar with regard to criticisms of the counting of words, of computational text analysis in general: \'93[T]he desired information is divided into innumerable fragments scattered over many volumes, like a mirror broken on the ground, presenting instead of one, a thousand images, but none entire.\'94 (Coleridge, A Dissertation on the Science of Method, 72) But can those original images, the truths of reading be recovered and enhanced by the act? Let\'92s begin with a simple example to illustrate the problem: the first paragraph of 
\f3\i The Adventures of Huckleberry Finn
\f1\i0  by Mark Twain.\
\
\'93You don't know about me without you have read a book by the name of The\
Adventures of Tom Sawyer; but that ain't no matter. That book was made\
by Mr. Mark Twain, and he told the truth, mainly. There was things\
which he stretched, but mainly he told the truth. That is nothing. I\
never seen anybody but lied one time or another, without it was Aunt\
Polly, or the widow, or maybe Mary. Aunt Polly--Tom's Aunt Polly, she\
is--and Mary, and the Widow Douglas is all told about in that book, which\
is mostly a true book, with some stretchers, as I said before.\'94\
\
Here narrator Huck Finn introduces himself by making a reference to Twain\'92s previously successful book starring Huck\'92s friend Tom Sawyer, acknowledging that his audience would not have heard of him were it for that book. But, as he says, that doesn\'92t matter because in the adventure to follow his readers will get to know him and his story well. He notes that while the author of that narrative, Mr. Twain, stretched the truth it was mostly accurate. Besides, most people with the exception of some virtuous outliers (Aunt Polly, Mary, the Widow Douglas) lie from time to time. You might be wondering why I\'92m providing such a plain retelling of the above paragraph. Let\'92s tokenize and remove so-called \'93stopwords\'94 from that paragraph \'96 processes most text modeling and analysis tends to do.\
\
read book\
adventures tom sawyer matter book\
mr. mark twain told truth \
stretched told truth\
lied time aunt\
polly widow mary aunt polly tom aunt polly\
mary widow douglas told book\
true book stretchers\
\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0
With all of these words buried away, what meanings can we glean from this same passage cleaned by our tool? We can guess, but without foreknowledge of the play, it\'92s a rorshack. Maybe the amount of our tool\'92s pruning was excessive though. One could imagine something like this below instead.\
\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\fi720\sl264\slmult1\pardirnatural\partightenfactor0
you don't know me without you have read book name\
adventures tom sawyer ain't matter book was made\
mr. mark twain he told truth mainly was things\
he stretched mainly he told truth nothing I\
never seen anybody lied one time another without it was aunt\
polly widow maybe mary aunt polly tom's aunt polly she\
is mary widow douglas is all told book\
is mostly true book stretchers as I said before\
\
A little more readable if still confusing? The point here is not about how we might construct a better stopword list, but rather the semantic toll on the original data and the subsequent semantic toll on the model for further analysis and interpretation the loss of data quality incurs. The scale of that loss often goes unmeasured, because, well frankly, we don\'92t bother to measure it. We say things like, \'93It\'92s good enough.\'94 or claim that there is not enough suitable basis to judge error. But as you can see above, it very much might not be enough to shrug our shoulders and move our analytical heuristic to its next step: a fancy modeling method. What happens when there are poor digital editions \'96 lossy from things like faulty optical character recognition, differences across editions, and overzealousness with methods like tokenization, stemming, lemmatization, and yes, stopword removal? Over the course of this chapter, we\'92re going to take a look at the status of all of the plain text editions of 
\f3\i The Adventures of Huckleberry Finn
\f1\i0  from \'93Project Gutenberg\'94 and \'93The Internet Archive.\'94 These sites are frequently looked upon as bottom of the barrel either for the sake of provenance or digitization errors and through that we will see how data quality metrics can be used to both bolster our evidentiary bona fides and even help produce higher quality text data for modeling.\
\
\pard\tx220\tx720\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl0
\f0\b {\listtext	\uc0\u8226 	}Idea: DataLad versioning for DH data as part of chapter 1\
{\listtext	\uc0\u8226 	}Idea:  Canonical Correlation Analysis (CCA) 
\f1\b0 and/or
\f0\b  Partial Least Squares (PLS) 
\f1\b0 for comparing data sets\'92 different variables and potential correlation\
\pard\tx940\tx1440\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl1{\listtext	
\f6 \uc0\u8259 
\f1 	}See: {\field{\*\fldinst{HYPERLINK "https://twitter.com/ar0mcintosh/status/1415627852360396802?s=20"}}{\fldrslt \cf4 https://twitter.com/ar0mcintosh/status/1415627852360396802?s=20}} and https://arxiv.org/abs/2107.06867\
\pard\tx220\tx720\li720\fi-720\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl0{\listtext	\uc0\u8226 	}
\f0\b Ideas from Mohammed\'92s talks:
\f1\b0  \
\pard\tx940\tx1440\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl1{\listtext	
\f6 \uc0\u8259 
\f1 	}Analytical flexibility: how much methodology affects the robustness of results\
{\listtext	
\f6 \uc0\u8259 
\f1 	}Data-driven vs. state-based methods\
\pard\tx1660\tx2160\li2160\fi-2160\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl2{\listtext	
\f6 \uc0\u8259 
\f1 	}State-based methods \
{\listtext	
\f6 \uc0\u8259 
\f1 	}Want to assess the simliarity between their states\
\pard\tx940\tx1440\tx1680\li1440\fi-1440\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl1
\f0\b {\listtext	
\f7 \uc0\u8259 
\f0 	}
\f1\b0 Assessment - Is their a ground truth?\
\pard\tx1660\tx2160\li2160\fi-2160\sl264\slmult1\pardirnatural\partightenfactor0
\ls1\ilvl2{\listtext	
\f6 \uc0\u8259 
\f1 	}Can use null hypothesis testing - but null hypothesis testing applicable to data-driven methods not applicable to state-based methods\
{\listtext	
\f6 \uc0\u8259 
\f1 	}And similar thing about testing applicable to state-based methods, they are not applicable to data-driven methods\
\pard\tx560\tx1120\tx1680\sl264\slmult1\pardirnatural\partightenfactor0

\f0\b \
\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\fi720\sl264\slmult1\pardirnatural\partightenfactor0

\f1\b0 \
\
Shakespeare\'92s 
\f3\i Richard III
\f1\i0  begins with Richard speaking on his brother Edward\'92s accession to the throne of England. \'93Now is the winter of our discontent\'94 \'96 it\'92s a famous one and hopefully familiar enough in sound to you to notice any changes.  It continues on another 37 lines from there. Cleaned up by a text analysis tool, the same speech might look quite different. For instance, here Richard\'92s original first sentence compared with that \'93cleaned up\'94 version.\
\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt \clshdrawnil \clbrdrt\brdrs\brdrw20\brdrcf4 \clbrdrl\brdrs\brdrw20\brdrcf4 \clbrdrb\brdrs\brdrw20\brdrcf4 \clbrdrr\brdrs\brdrw20\brdrcf4 \clpadl100 \clpadr100 \gaph\cellx4320
\clvertalt \clshdrawnil \clbrdrt\brdrs\brdrw20\brdrcf4 \clbrdrl\brdrs\brdrw20\brdrcf4 \clbrdrb\brdrs\brdrw20\brdrcf4 \clbrdrr\brdrs\brdrw20\brdrcf4 \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0
Now is the winter of our discontent\
Made glorious summer by this sun of York;\
And all the clouds that lour'd upon our house\
In the deep bosom of the ocean buried.\cell 
\pard\intbl\itap1\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0
winter discontent\
glorious summer sun york\
clouds lour'd house\
deep bosom ocean buried\cell \lastrow\row
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\fi720\sl264\slmult1\pardirnatural\partightenfactor0
\
\
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0
Let\'92s observe what has happened here. On the left in the original, Richard claims that his family\'92s time of \'93discontent\'94 is over. This new time period has been made \'93glorious\'94 by Edward\'92s accession (and the murder of Henry VI). The metaphorical skies have cleared, his family\'92s problems buried away. The sentence is in two parts divided by the semicolon, the second part a reiteration of the idea in the first. With all of these words buried away, what meanings can we glean from this same passage cleaned by our tool? We can guess, but without foreknowledge of the play, it\'92s a rorshack. Maybe the amount of our tool\'92s pruning was excessive though. One could imagine something like this below instead.\
\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt \clshdrawnil \clbrdrt\brdrs\brdrw20\brdrcf4 \clbrdrl\brdrs\brdrw20\brdrcf4 \clbrdrb\brdrs\brdrw20\brdrcf4 \clbrdrr\brdrs\brdrw20\brdrcf4 \clpadl100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0
now is winter of our discontent\
made glorious summer by sun of york;\
all clouds lour'd upon our house\
In deep bosom of ocean buried.\cell \lastrow\row
\pard\tx0\tx0\tx0\tx0\tx0\tx0\tx0\tx0\sl264\slmult1\pardirnatural\partightenfactor0
\
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl264\slmult1\pardirnatural\partightenfactor0
Let\'92s take a visual look at what has happened to the word counts.\
}