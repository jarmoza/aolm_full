<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="BF0B4750-DD1C-4E97-8900-3CADEF87F334">
            <Title>February 12, 2019</Title>
            <Text>Done
	1.	Implementing data quality profile script and testing on 19cAm volumes from New York Public Library
	2.	dq_initial_assessment.py - data quality profile script
	⁃	dq_initial_assessment_nyp.py - nyp metadata schema
	⁃	htrc_ef_metadata_lens_nyp.py - Adapts HTRC feature reader to pull metadata for dq assessment
	⁃	nyp_dq_profile.json - profile of 22,796 19cAm volumes from New York Public library
To do
	1.	Refine script and data profiling
	2.	Test on smaller data set
	3.	Implement the difference between profiling and assessment
	4.	Refine the profiling schema for issues addresses in Coleman book like granularity

</Text>
        </Document>
        <Document ID="E502BE76-97E3-41BD-8CC1-CE2675455F4E">
            <Title>January 25, 2021</Title>
            <Text>Done
	1.	Created keywords for project
	2.	Worked on tagging all documents with keywords in dissertation journal
	3.	Created body style for project
	4.	Began work formatting all texts with heading/body styles
	⁃	Finished formatting work on dissertation journal
</Text>
        </Document>
        <Document ID="80904571-AFDD-4E1D-B74C-6CD3D2E3D3B8">
            <Title>Dataset Notes</Title>
        </Document>
        <Document ID="39284AC4-3924-4F86-9795-1F42F72F1DAF">
            <Title>Emily Dickinson Correspondences (UVA)</Title>
        </Document>
        <Document ID="064FB6A7-3E7D-4982-82A5-DC19AD46BF9E">
            <Title>July 30, 2019</Title>
            <Text>Notes
	1.	Prediction vs explanation
	2.	“Perspectival modeling” (Underwood, Appendix B)
	3.	the interpretability of a model
</Text>
        </Document>
        <Document ID="CCA82AC3-7CF6-491D-8A24-EC44575F7C85">
            <Text>On frequency analysis

	Frequency analysis acts as a primary tool for computational analysis of literature. It’s important to be very specific about what we mean we talk about that. The notion of a “word” in a text typically gets generalized into the term, “token.” The reason for that shift is apparent once you practice this kind of analysis. It can be challenging to understand when a unique word is really being used in conjunction with another to relay a compound concept to a reader. So, for instance, if an author wrote the term “chewing gum”, a less-than-careful word-counting algorithm might look at that and record it as two words – which it is. And if we were to apply a less-than-careful part of speech tagger to the same text, it might understand “chewing” as a verb and “gum” as noun. All the meanwhile, an English speaker would interpret that correctly as a noun phrase that employs an adjective to denote a real world object: “gum” that is used for “chewing.” For an English speaker those last few sentences describe something very obvious, but for an algorithm that is built to count words, it’s not. Thus the description of terms in a text as “tokens” becomes imperative. Thus the objective of frequency analysis is to count tokens. And the counting of tokens for the sake of precision – what we also call quality – can be a very handcrafted art. 
	Literary scholars face the challenge of confronting statistical philosophy when performing this kind of analysis. Is it really necessary to attempt to make our text-turned-data as high quality as possible for subsequent modeling, analysis, and interpretation? For a techne that intends to be as precise as possible, with analyses sometimes turning on the existence of a single word, the answer to that question must be an emphatic “yes.” In fact, even if it can be proven that, statistically, a model produced from two data sets is more or less the same it remains a judgement call for the scholar if that “more or less the same” is sufficient for them to proceed. The factors of time and accuracy in research must be weighed in concert just as much as the content itself. They often dictate the efficacy of our work up to a certain degree.  How much time am I willing to spend ensuring my data is of high quality before I proceed with the modeling of it? How much quality is necessary for a particular modeling technique or for the type of analysis and interpretation I intend to provide? These questions are implicit information, but checking back in with ourselves on our current answers to them is not merely “process text.” Instead, the effort poses a subsequent philosophical challenge or question for all stripes of humanities scholars: What are my own criteria for quality when it comes to my object of research? Not much differs between leaning on the alleged soundness of a modeling technique and leaning on the theorem of central tendency. For practitioners of a field where qualitative and thus culturally-impactful understandings of its objects are the desired outcomes, both assumptions about the quantitative, methodological soundness can lead us astray. Much like a non-digital piece of literary scholarship that might turn on hapax legomena, so too do its digital counterparts often turn on small differences in measurement. Arguments of “more” and “less” are deployed far more frequently and implicitly than discussions of how and why they are deployed appear. It is up to humanities scholars to vet quantitative soundness by (im)posing our own qualitative criteria for interpretation against it.

	One good example of this is the productive line of quantitative analysis and reasoning in stylometrics that has led to thoughtful metrics such as Burrows’ Delta. &lt;Explanation of delta here&gt;
While delta has been used as a means of understanding the closeness between documents and thus likelihood of attribution for works of unclear authorship, it also can be a touchstone in this quest for producing a measure of literary data quality. Since delta 



Next Steps/Prose

	1.	Prose explaining Bolts of Melody vs Emily Dickinson Archive 
	⁃	Token frequency comparison as component of data quality
	⁃	Unitary frequency analysis quality(ies) vs quality from comparative frequency analysis
	2.	Code up comparisons
	⁃	Explain the work and the thought process behind the work
	⁃	What’s are its goal(s)?
	3.	Prose and examples of more advanced frequency measures
	⁃	Mendenhall’s Characteristic Curves of Composition
	⁃	Kilgariff’s Chi-Squared Method
	⁃	John Burrows’ Delta Method
	⁃	See Laramee’s stylometry intro for details
	⁃	Why use them and how they are related to data quality in this context


Fuzzy string matching techniques

	1.	Character overlap
	⁃	Jaccard measure
	⁃	"the percentage of unique characters that two strings share when compared to the total number of unique characters in both strings” (Taming Text, FSM, 4.1.1.)
	⁃	Jaro-Winkler distance
</Text>
        </Document>
        <Document ID="F194F9C9-E6AB-462C-87A6-B1ED0AC68142">
            <Text>Introduction - Siting a site for modeling (the fundamental task of research)

Ideas about data quality in the humanities

Ideas about data quality in the sciences

Hathi Trust data set (HTDS) and 19th century American Literature

Bidirectional quality

Data quality work thus far in the digital humanities

Data quality frameworks from information science

Application of Sebastian Coleman’s framework to HTDS

Metrics for the foundation of data quality for the humanities</Text>
        </Document>
        <Document ID="B1BA27BF-4F59-4470-B8B6-86D8FA6237A3">
            <Title>Annotated Scriptography</Title>
            <Text>Script categories

Measurements

	1.	data_quality/core/count_words_and_plot.py

Processing

data_quality
	core
		count_words_and_plot.py - Takes an input text file, tokenizes it, counts its unique words and plots their usage frequencies in a bar graph


Object-related

</Text>
        </Document>
        <Document ID="337AAC7C-8606-4C4F-8661-ACD6FA549C0C">
            <Title>Moby-Dick</Title>
        </Document>
        <Document ID="516192A7-2515-415A-B8BA-FAC259095283">
            <Title>Chapter 2 - Model Quality</Title>
            <Text>Chapter 2. Model Quality

What if the successful prediction of part of a literary data set was a determiner of the soundness of a literary model? 

This kind of evolutionary framing animates possibilities for the study of literature via computation. 

The soundness we are invested in ensuring in our objects of study becomes all the more necessary when the same kinds of objects studied at larger scales.

 In parallel, it becomes increasingly important to pay close attention to the qualities of models created from those data sets the more complex those models become. 

Though a model itself presents a simplification of a complex system of interacting literary elements – words, sentences, paragraphs, chapters, books, etc. – the terms with which those models are proposed, created, and interpreted are filters and lenses under which research into those elements unfolds. What is hidden from a model’s view of its literary components? Which does a model highlight and bring into focus? Where do we decide to interject the model? And when?

These decisions are ones made for non-computational modeling of literature as well. 

This locative and demonstrative staging of a model is the subject of the work to follow. 

Taking a step closer to the page will demonstrate the volatility and precarity of model qualities – two characteristics that though at first seemingly aberrant will become the means by which we can open up new understandings of the texts and literatures we have studied for well over a century and for the new literatures of the future. 

Whereas the small changes in data quality among the works from New York Public Library and HathiTrust produced distinctly different models and potential conclusions from those models, altering model types and their parameters to include humanities considerations of representation will construct entirely different perspectives on the same data set.

It’s not much of a surprise that formal model selection criteria is an old playground for statisticians. 

In fact, it is the formality of it that derives from schools of thought of representation that may be the most surprising to humanities scholars. For us, a series of “turns” in literary study have marked cultural changes in perception in fields of literary study.  These cycles of conflict between new and old schools of thought are well documented by Russian formalist Tomashevsky.  see Jurij Streidter

Michael Pollan windows

</Text>
        </Document>
        <Document ID="8E933A1E-A2D5-4241-BB6D-FC14BCE33C46">
            <Title>Preparatory</Title>
        </Document>
        <Document ID="DF4C8904-5C20-4DD0-AFD2-F0FA00CCC412">
            <Title>Sum2021Fell/Application/ProjDesc/Outline &amp; Notes</Title>
            <Text>Model quality/qualities of/for literary modeling

Mark Twain autobiography

Resources

Twain reviews/criticisms
Info sci - look for model quality scholars to reference from your notes

Vol 1. 760 pages
Vol 2. 776 pages
Vol 3. 792 pages

1000 words

	1.	Mark Twain autiobiography description - popular response
	2.	This sets up the problem of a large, opaque work
	3.	The problem of model selection for computational modeling of literature
	4.	Project proposition on model selection via model quality assessment for project/humanities
	5.	Description of previous chapter on data quality and overall dissertation
	6.	Discussion of adaptation of information science methods for humanities purposes (dq metrics, dq assessment, new “literary” data quality valuation)
	7.	“Information criteria” for selected model as means of quality estimation of model output
	8.	Combining IC with proofing via humanities qualitative assessment
	⁃	close reading sections highlighted by the model and producing valuation kind of like an annotated bibliography for these highlighted sections of the model/underlying text
	9.	Prospects for new insights about the Twain autobiography and new notion of sound evidentiary basis for the highlighting/selection of sections for analysis/comparison/contrast/etc.
	10.	Prospects for other researchers to use these model metrics and heuristic to make assessments of their own models and use of those assessments as (part) basis for the interpretive claims made
</Text>
        </Document>
        <Document ID="F1424286-C543-4A74-AC35-8C545112F136">
            <Title>Submissions</Title>
        </Document>
        <Document ID="BA989A73-1B7F-4D8E-9790-9C9AF83EE09D">
            <Title>March 19, 2019</Title>
            <Text>Done
	1.	Read article “Big? Smart? Clean? Messy? Data in the Humanities” by Christof Schöch
	2.	Set up work in Scrivener
	⁃	Created ‘Source Notes’ folder to contain notes on articles read
	⁃	Created ‘Source List from Readings’ document to note sources found while reading articles, books, etc.
	⁃	Created Source note for Schöch article on Data in the Humanities
	⁃	Created ‘Ideas List’ document
	⁃	Created ‘Daily Log’ document
	3.	Examined Dickinson TEI and txt dataset
	⁃	The idea is to get a deep understanding of the files present (e.g. which files are from which edition, what’s inside them, etc.) as well as some useful summary statistics to ground analysis later on
	⁃	Can be considered a precursor study for data quality measures
	⁃	Created new corpus folder for raw and processed data, scripts, stats, etc at ~/Documents/Digital_Humanities/dickinson
	⁃	Wrote script xml_analysis.py to look at tags in the TEI files of the Emily Dickinson files from Emily Dickinson Archive
	⁃	Counts blank lines (in &lt;l&gt; tags) in the files
	⁃	Lists all kinds of tei tags found and counts them
	⁃	Generates html documents for each tag with occurrence count and links to the tei files in which they are found.
	⁃	Generates an index.html to easy navigation of all tag html files
	⁃	Examined meanings of several editorial tei tags employed in the corpus
	⁃	Findings:
	⁃	4826 files
	⁃	Will need to be de-duped/separated across different publications
	⁃	889 blank lines
	⁃	27 tei tags (occurrences listed below)
	⁃	'l': 51729, 'app': 12809, 'lem': 12809, 'lg': 12032, 'lb': 10083, 'p': 9650, 'text': 4825, 'fileDesc': 4825, 'TEI': 4825, 'title': 4825, 'body': 4825, 'sourceDesc': 4825, 'publicationStmt': 4825, 'teiHeader': 4825, 'titleStmt': 4825, 'div': 4825, 'rdg': 1909, 'add': 1221, 'pb': 817, 'em': 488, 'del': 8, 'publication': 2, 'variant': 2, 'date': 2, 'pages': 2, 'published': 2, 'u': 1
</Text>
        </Document>
        <Document ID="0E12CCA9-694F-4388-8C9C-D4C77AF249AA">
            <Title>Data Quality</Title>
        </Document>
        <Document ID="3C5B7E58-0222-41E6-9358-41CF46116A29">
            <Title>Project Description</Title>
        </Document>
        <Document ID="F271D12F-9F8C-4BA6-A534-52335A4ACA51">
            <Title>March 27, 2019</Title>
            <Text>Done
	1.	Set up Paperpile account and Organized some pdfs into it
	2.	Wrote Introductory Thoughts on what a ‘literary model’ is
	⁃	See Why ‘Literary’? 
	3.	Reading introduction to Flanders/Jannidis Data Modeling book
	4.	Corpus is split up by publication in curated/split folder
	⁃	Corpus text counts by listed publication in TEI file:
	⁃	{u'The Poems of Emily Dickinson, Variorum Edition, Franklin, 1998': 2459, u'The Poems of Emily Dickinson, Johnson, 1955': 1776, u'Poems by Emily Dickinson: Second Series, Higginson and Todd, 1891': 166, u'Poems by Emily Dickinson: Third Series, Todd, 1896': 165, u'The Single Hound,  Bianchi, 1914': 143, u'Poems by Emily Dickinson, Todd and Higginson, 1890': 115, u'New Emily Edition!': 1}
	⁃	(New Emily Edition is a test file from the archive workers 4825.tei - removed from raw tei corpus)
</Text>
        </Document>
        <Document ID="7ED0E743-FC6A-4005-905F-B42E27E37F3B">
            <Title>What I have 2025</Title>
            <Text>Jonathan Armoza
jarmoza@gmail.com
Website w/portfolio: jonathanarmoza.com
Github account: github.com/jarmoza
Dissertation code repository: github.com/jarmoza/aolm_full (access request required)

Dissertation Overview
 
“The Art of Literary Modeling” (AoLM) is an attempt to create and describe several new research activities for digital humanists regarding defining and measuring the quality of literary datasets and computational models from those datasets. (AoLM describes these datasets as “literary” intentionally in that these collections of textual data exist and function for different purposes in the humanities.) This has several motivations, both old and new. The first is that in previous history in the field, work on defining high quality datasets has been to identify “dirty” data (i.e. OCR errors, issues with generalizability of typeset and characters) and to work to “clean” that data in order to process it efficiently and to produce something where errors or non-standard aspects have been removed/ironed out. However, while this work is useful in moving the research process along, AoLM argues that these research outputs then rely on ahistorical sources in that they have been modified in order to proceed to what are seen as the more “valuable” pieces of digital humanities text modeling research: modeling of literature, analyses of those models, and literary-historical conclusions based on them. Looking at datasets that have been modified in this way, in the past they have been deemed “good enough” in order to proceed along that workflow. AoLM states that, “good enough” is an oversight that is partly based on the economics of academic research and that instead there should be well-defined means of establishing the quality of those datasets, and in turn well-defined means of establishing the quality of models based upon them. (A researcher’s work could include establishing the quality of a data set and subsequent model of it, or could rely on the work of pay-it-forward researchers whose work consists of understanding the quality of available digital literary dataset – and even working on the quality of models from it for subsequent use.)
AoLM borrows from the fields of Information Science, Statistics, and Computer Science alongside lessons learned in the humanities from incorporating their methods. In particular for data quality, AoLM looks to data quality frameworks as produced by Information Scientists both in academia and in private industry. The dissertation borrows and adopts data quality metrics. This is not to produce a set list of highly generalizable calculations for researchers, but rather to demonstrate how these data quality metrics can be custom-tailored to the datasets and research purposes of individual researchers, and then how they can be utilized together to understand overall quality of aspects of datasets and whole datasets themselves – standards which can then be used to determine comparative quality of datasets and be used as objective grounds for modeling to follow. The dissertation proceeds along similar grounds for model quality using measurements from Statistics and Computer Science called “information criterion”, and also incorporates lessons from creating data quality frameworks to establish similar qualitative standards for models themselves.
The underlying example material for the work describing these new research activities is a set of 19th century American literature – Mark Twain’s Adventures of Huckleberry Finn and his Autobiography Vol. 1, Herman Melville’s Moby-Dick, Emily Dickinson’s poetry and correspondence, and a collection of lesser known 19th century works from the New York Public Library. These selections (aside from the NYPL collection) are made both because of my initimate familiarity with them, but also because of their high rate of publication, numerous editions, and numerous copies. Also as important as the works themselves, however, are their sources. Since data quality standards consider both physical attributes and lesser-considered external aspects of quality (i.e. accessibility as quality measurement), AoLM looks to both highly public and more private digital repositories including university libraries and research centers (i.e. Mark Twain Project Online at UC Berkeley, Emily Dickinson Archive at Amherst College, etc.), Project Gutenberg, The Internet Archive, and Hathi Trust Digital Library. Research has also included investigating how these sources have produced these digital works, including both standard research via the internet and the sites’ respective forums as well as an interview with Tanya Clement, a well-established digital humanities scholar who was involved in the creation of the Emily Dickinson correspondence site and Emily Dickinson Archive.

Writing Status

Thus far writing for AoLM consists of introductory writing, copious notes on sources and processes, some interview notes, early drafts of portions of the first chapter on data, some writing on the second chapter on model quality, some early analysis of data and model quality measurements, and attempts to gather pieces of all of this work together. One of the reasons for the scattered writing in addition to side projects to sustain myself (see ‘What I have been doing’ below) is because much of AoLM relies on prototyping code for how data quality metrics from information science can be translated into useful metrics for both textual data and humanities research. (See ‘Code’ section below for a description of this work.) 

Datasets

Emily Dickinson, poetry and correspondence
	- Bingham correspondence
	- Emily Dickinson Archive
	- Project Gutenberg

New York Public Library 19th century American dataset from HathiTrust Digital Library

Herman Melville; Moby-Dick, many editions from each of:
	- Melville Electronic Library
	- Project Gutenberg
	- Internet Archive

Mark Twain
	- Autobiography vol 1-3 from Mark Twain Project Online at University of California, Berkeley
	- Adventures of Huckleberry Finn, many editions from each of:
		- Mark Twain Project Online
		- Project Gutenberg
		- Internet Archive

Code Status

Coding these ideas have required numerous attempts, some of which have succeeded and others not. In order for the code to be successful, the code itself must be generalizable and - using my background as a software engineer, I have been able to do this: the idea is that I can have a script read through just about any of the literary datasets described above (as well as their metadata), be processed into generalized data structures (modified as little as possible), measured through generalized data quality metrics that are appropriate for each dataset, and then weighed together both mathematically into higher level measurements and weighed analytically. Generalizing data ingestion (downloading, scraping, reading from the resultant formats) has been a notable portion of the work. In addition thinking through and prototyping data quality metrics has proven challenging in that sometimes data quality metrics must be custom coded to consider unique characteristics of particular datasets (i.e. creating metrics of Huckleberry Finn across three separate sources with three very different formats itself required carving out special exceptions so that the works could be reasonably compared together). While much of that challenge is behind me, I would say defining this new activity in of itself in that it has not been done before has presented its own challenge. All of this code work has been meticulously updated and maintained via my experience with project organization on github.com (This is currently a private repository and requires a request to access.)

What I Have Been Doing Since End of Coursework

Since my reading exam and my dissertation proposal (2017-2018) I have been engaged in several side research projects in digital humanities and in language for neuroscience. This is partly from part-time opportunities that arose given my skillset and academic network, and starting in 2021 partly from the need for at least some income to support myself. This work included: (1) creating an interactive digital archive of Scottish national poet, Edwin Morgan’s collage books at University of Edinburgh with colleagues Bridget Moynihan and Anouk Lang, (2) creating and refining language datasets for MRI neuroscience/machine learning experiments with Pierre Bellec at University of Montreal, (3) work on a data/metadata alignment toolset for researchers involved in using datasets across multiple sources with Sebastian Urchs and Jean Baptiste Poline at McGill University. (I have been able to coopt much of these works into my own digital humanities research and development for AoLM.) In 2023, I worked on data pipelines and interactive site work for an early modern machine learing project at Carnegie Mellon under Christopher Warren – although that had to be cut short for health reasons (see Health section below). Generally, I have continued to work on digital humanities data visualization tools for my own work, given many talks across Canada, the United States, and in Europe on my past DH work, and published (or co-published) at least 6 papers – including my own book chapter regarding a digital humanities project on Moby-Dick and the work of one of its most famous scholars and editors, Harrison Hayford. (That work began with Jennifer Baker in our department and Dave Blei in the Statistics/Computer Science departments at Columbia in 2016) and even won an award from the Canadian digital humanities association for the beginnings of that work in 2017. In 2021, I was able to springboard an NYU DH Showcase project to do some early model quality work on Twain’s autobiography. Needless to say, I have been busy, and unfortunately much of the above work even if related to my own dissertation has come at the cost of time I have been able to dedicate to its coding and writing.

Projects

	•	“Catalog of Distinctive Type” and “Print and Probability”, Carnegie Mellon (remote), (Fall 2023-Spring 2024), publication pending in Debates in Digital Humanities 2025
	•	 “The Art of Literary Modeling” – Data and model quality for the humanities, PhD dissertation, 2018-present
	•	“Neurobagel” An ecosystem for distributed dataset harmonization and search (Fall 2021-Summer 2023)
	•	“Courtois-Neuromod: The Courtois Project on Neuronal Modeling” (Language dataset creation and MRI reading experiment design) (Fall 2017-Spring 2020)
	•	“Working from Scraps: A Web Prototype for the Scrapbooks of Edwin Morgan”, Fall 2017-Spring 2018
	•	“TWiC” and “TWiC: Close Reading” – visualization tools for exploring topic models of literary datasets (2014-2015, 2017 respectively)

Publications

	•	“Print and Probability” book chapter co-author, publication pending in Debates in Digital Humanities 2025
	•	 “The Three Terms Task (3TT) - an Open Benchmark to Compare Human and Artificial Semantic Representations” in Nature: Scientific Data, March 2023
	•	“Hayford’s Duplicates: Cobbling a Model of Melville’s Moby-Dick” (published as chapter in Computational Stylistics in Poetry, Prose, and Drama (2023))
	•	“A Standardized Protocol for Efficient and Reliable Quality Control of Brain Registration in Functional MRI Studies”, co-author, in PsyArXiv, February 2020
	•	“MIST: A multi-resolution parcellation of functional brain networks”, co-author, in MNI Open Research, July 2019
	•	“Working from Scraps: Copyright and Materiality as Creative Constraints for Digitally Remediating the Morgan Scrapbooks” (co-author, publication TBD)
	•	“Nonnegative Matrix Factorization for Digital Humanists: Modeling the Parts of Speech of Emily Dickinson's Fascicles”, (publication in Digital Studies / Le champ numérique offered in 2017, but passed on for future publication after more work done)

Talk List

	•	NYU Digital Humanities Showcase, “Model Quality for Digital Humanities”, New York University, Fall 2021
	•	Courtois-Neuromod Science Day, “Semantic Word Triples Generated from Embedding Models”, University of Montréal, Winter 2020
	•	Plotting Poetry (and Poetics) 3, “Hayford’s Duplicates: Operationalizing a Literary Theory of Herman Melville’s Moby-Dick”, University of Lorraine, Fall 2019
	•	Colloquium: Presentations of Current Research in Digital Humanities, “Nonnegative Matrix Factorization for Digital Humanities”, University of Basel, Winter 2019
	•	Computational Literary Genre Stylistics Conference, “Nonnegative Matrix Factorization for Digital Humanities”, University of Würzburg, Winter 2019 
	•	SACHI Seminar, “How to Closely Read a Topic Model: Visualizing the Poetry of Emily Dickinson”, University of St. Andrews, Fall 2017
	•	Digital Scholarship Seminar, “Probabilistic Matrix Factorization for Humanists”, University of Edinburgh, Fall 2017
	•	Digital Humanities 2017, “How to Close Read a Topic Model”, Summer 2017
	•	Canadian Society of Digital Humanities, “Probabilistic Matrix Factorization for Humanists”, Summer 2017

Awards

•	NYU Lind American Literature Fellowship (Fall 2024-Spring 2025)
•	NYU  Digital Humanities Summer Fellowship, Summer 2021
•	2017 Ian Lancashire Promise Award for Outstanding Graduate Research, Canadian Society for Digital Humanities


Next Steps

Moving forward with whatever time I have left or is granted to me, I plan on devoting all of my time to finishing the dissertation. Recently, work has been proceeding on data quality measurement for Adventures of Huckleberry Finn and has been producing a series of metrics that is being used to produce analysis writing and investigation into why various aspects of editions of the work produce lower quality measurements. This work will be integrated into the first chapter and lessons from the measurements and their incorporation into a larger data quality framework will be used for both chapter 1 and also the model quality second chapter. Which works from the above listed datasets that are to be featured for particular examples of quality measurement are in flux, but it does seem that works with the highest number of digital editions/copies available (e.g. Huckleberry Finn and Moby-Dick) are the ones that will be used the most while less available digital editions will be used as examples of what one can do to establish quality standards when there are not numerous editions available. Twain’s Autobiography will be featured in the second chapter in that its large size will produce a good example of what happens when researchers cannot attend to data quality in its fullest, but can at least attend to model quality in a more comprehensive fashion. In summary, there is still much analysis writing to be done for chapter 1 as well as integration of that writing with introductory text on the state of digital humanities research and the ideas of data quality. And much of chapter 2 remains to be written in addition to the code experiments necesary for defining model quality as a research activity. Chapter 3 of AoLM is dedicated to documenting the dataset and coding work involved in the dissertation.

Health

Starting in mid 2022 I began having some significant sleep issues. About the time of the pandemic it seems I had begun to (re)develop some significant environmental allergies that ended up or coincided with me also being diagnosed severe sleep apnea. This really disrupted my ability to work often having to take time off of even any part-time work, and had its own side effects associated with sleep loss. (Professor Augst had recommended that I request a pause in the PhD while I had health issues, but at the time I had wanted to try to continue working. In retrospect, he was quite correct.) In fall 2023, I moved down to Orlando, Florida for personal reasons and my allergies abated enough for me to continue working. However, in early 2024 I started having difficulty with my hands and arms – immobility, weakness, and pain. I began physical therapy for this but I was diagnosed as having a herniated disc in my neck. The effect of this was I not able to continue with any work on a computer keyboard as it seemed to be worsening my condition. Thankfully, the Carnegie Mellon position mentioned above included a form of disability insurance. I was put on short term disability beginning from June through September and though I did attempt to continue PhD work, the condition limited my ability to both work on the keyboard and concentrate for long periods of time. Shortly after that my doctor recommended I try a steroidal treatment for my neck as the more conservative approach of physical and occupational therapy was not having the desired effect. With one dose in October and then a successive dose in early December I have been able to get back to work on the dissertation.</Text>
        </Document>
        <Document ID="1DA8D6F2-E2CE-44CF-A895-3D763C963867">
            <Title>Sebastian-Coleman - Notes on Data Quality Initial Assessment</Title>
            <Text>Chapter 7: Initial Data Assessment

Components

	1.	Column profiling
	2.	Structure profiling
	3.	Production of deliverables from an initial assessment

Assessment Scenarios

“The general purpose of data quality assessment remains the same: to understand the condition of data and to draw a conclusion about its impact in relation to expectations or particular purposes or both.” (95)

A need for continual review (e.g. as a source updates its data, the assessment should be run again and compared with previous assessments)
	⁃	“Initial assessment provides metadata that is the foundation for other assessments. To ensure this foundation is firm, you should plan from the beginning to systematically and consistently capture findings” (98)

Particular aspects of data in need of quality monitoring may be selected as a place needed for focus in the assessment

An Ongoing Process for Future DQ Considerations

“Initial data assessment produces valuable metadata (including specific baseline measurements) about the content, structure, and condition of the data being assessed. One goal of an initial assessment is to produce a set of recommendations for data quality improvement and for ongoing measurement and control.“ (97)

Good Advice

“As part of the analysis, you should also identify which data elements and rules appear to be most critical and most at risk. As you investigate questions raised by the assessment, you should actively seek input from business users about data criticality.” (98)


Initial Assessment

Data Expectations
	⁃	Producer expectations vs. Consumer expectations
	⁃	“It is important to identify the expectations of both data producers and data consumers since these likely differ and identifying such differences is necessary to resolving them.” (100)

Data Standards
	⁃	“Data quality standards are assertions that relate directly to quality dimensions. Standards for a given system should cohere. One of the outcomes of an assessment might be to identify contradictory standards. Standards should also be measurable. Another outcome of an assessment might be to associate specific measurements with data that is expected to conform to standards.” (99)

Data Profiling

Column Property Profiling

	⁃	Identifying data type, length, format
	⁃	Distribution analysis
	⁃	“Percentage calculations also help in identifying other relationships within the data. Columns that have similar distributions may have a dependent relationship on each other, for example.” (100)
	⁃	“Each column should have a defined domain of values” (101)
	⁃	Inspecting high frequency and outlier values for interest/expectation/errors
	⁃	“cardinality of the column (distinct number of values)” (102)
	⁃	“percentage of defaulted records” (102)
	⁃	“number of data format patterns for each column” (102)
	⁃	Baseline expectations from prior source metadata if possible
	⁃	If not available then you rely on intuition/common sense “to confirm both your actual findings and your assessment of their implications” (102)
	⁃	Column qualities to observe
	⁃	Validity
	⁃	Completeness
	⁃	Consistency
	⁃	“The presence of values that are not identified as valid values.”
	⁃	“Cardinality different from expected (high cardinality where low is expected or low cardinality where high is expected).” 
	⁃	“Less-than-complete population of a column where you expect complete population.” 
	⁃	“The presence of multiple data patterns in one column.”
	⁃	Invalid Values
	⁃	“Validity and domain rules are a form of data expectation. If you know nothing about a column except its valid values, you can still draw some conclusions about the quality of the data based on a distribution analysis of those values.” (102)
	⁃	The absence of valid values
	⁃	Unusual cardinality - requires an investigation of column’s status
	⁃	Cardinality of one
	⁃	High cardinality (all unique, multiple data sources with different value sets)
	⁃	A column being used for multiple purposes
	⁃	Mislabeled columns
	⁃	Problem with the data source
	⁃	“When two fields have the same exact cardinality, they should be compared. They may contain redundant data.” (104)
	⁃	Columns with Less-Than-Complete Population
	⁃	“Under what conditions is the column expected to be populated, and under what conditions is it not expected to be populated?” (104)
	⁃	“Knowing and documenting these conditions can also help in the investigation of dependency rules and other data relationships.” (104)
	⁃	Possibility of multiple default values depending on how they were entered by source - defined default vs functional default
	⁃	The reason for that multiplicity should be investigated
	⁃	Multiple Data Format Patterns in One Column

Structural Profiling

	⁃	Referential Integrity
	⁃	“If all records on the child table do have references on the parent table, then referential integrity exists between the two tables.  This relationship is sometimes called an inheritance rule.” (106)
	⁃	Valid domains/validity of data value (ranges)
	⁃	Data completeness with regards to referential integrity
	⁃	“The presence or absence of orphan records can be used as a measure of completeness. These relationships also depend on data consistency.”
	⁃	Things to look for:
	⁃	Missing references
	⁃	Parentless children
	⁃	Childless parents
	⁃	Misdiagnosing problems that seem to exist in the ‘sample’ data but do not exist in the ‘production’ (or total) data
	⁃	Or vice-versa, problems exist in the full set that do not exist in the sample set
	⁃	Dependency Relationships
	⁃	Positive or mutually dependent (a relationship that requires the presence of information)
	⁃	Negative or mutually exclusive (a relationship that prohibits the presence of information)
	⁃	Conditional relationships
	⁃	Often more work to describe, and if not known ahead of an assessment hard to identify
	⁃	Things to Look for
	⁃	Dependency relationships that are absent when expected or present when not expected
	⁃	“It may be that the attributes represent something different from what the metadata indicates they do. Or that analysis has identified a business rule that no one had explicitly described before. Discovery of such rules and relationships is one of the benefits of profiling data. It is critical to document such discoveries so that this knowledge can be shared.” (107)
	⁃	“dependency rules provide insight into data consistency, as well as validity and completeness.” (108)
	⁃	Differences in Granularity and Precision
	⁃	“Across such systems, it is not unusual that data representing the same concept is presented at different levels of grain or different levels of precision.” (108)
	⁃	Granularity
	⁃	“Granularity refers to the degree to which information is represented by constituent parts. A simple illustration can be made with phone numbers.” (108)
	⁃	Challenge for granularity is with how more complex concepts are represented by the data
	⁃	Precision
	⁃	“precision refers to how fine the data is in a given field.” (108) or “fineness of measurement” (108)
	⁃	Fields can have similar granularity but different precision (“the sets of valid values offer different degrees of precision”) (108)
	⁃	What to look for:
	⁃	“Instead, as part of the initial assessment of data, you should identify the fact that data representations differ, and then you should define the specific ways they differ from each other. These differences represent risk because they break the expectation for similarity between data representing the same concept and because integrating data of different granularity requires developing a data model that can appropriately accommodate the differences.” (108)
	⁃	Relationship Cardinality
	⁃	“the ways in which entities can be related to each other” (109)
	⁃	#
	⁃	Also, cardinality of relationships can be put in terms of optionality
	⁃	#
	⁃	“Assertions about the cardinality of relationships are a form of expectation about data. If there are differences between the cardinality you expect and the cardinality that exists in the data…then you should determine the reasons for the differences and their impact on uses of the data” (109)

Goals of an Initial Assessment include the creation of a data model and a data dictionary
	⁃	“As most people who work in data management know, few data stores will have metadata that perfectly reflects the condition of the data. A more likely scenario is that profiling will result in the detection of unexpected conditions in the data and gaps or inaccuracies in the metadata or both.” (110)

From Profiling to Assessment
	⁃	“Profiling will produce a set of facts about data, but it does not, in and of itself, constitute assessment. To be meaningful, assessment requires comparison. To characterize the quality of data, you need to compare the facts you have discovered up against expectations about the data and draw conclusions about how well the data meets those expectations.” (110)
	⁃	Expectations and adherence for audiences
	⁃	“Profiling will produce a set of facts about data, but it does not, in and of itself, constitute assessment. To be meaningful, assessment requires comparison. To characterize the quality of data, you need to compare the facts you have discovered up against expectations about the data and draw conclusions about how well the data meets those expectations.” (110)

Deliverables from Initial Assessment
#
	“The primary goal of initial assessment is to characterize the condition of the data you are assessing. From input gathered during the process, you should also have a clear understanding of which data is most critical to your organization. Usually, assessment is directly connected to a project. Projects may include building a data store, integrating new data into an existing one, or, analyzing the condition of existing data for purposes of identifying improvement opportunities. Some criteria for quality should be associated with project goals.
	“The primary deliverables from an initial assessment are a summary of assessment results: the measurements that were taken against the data and the degree to which the data conform to expectations (whether those expectations are general or in the form of requirements or standards). The results summary should also identify opportunities for improvement, recommendations for data standardiza- tion, and recommendations for ongoing measurement and control. Opportunities for improvement and standardization can be identified for individual data elements, data rules, the data model, or meta- data and references data (see Figure 7.3). Recommendations for ongoing measurement should not only describe how data should be measured; they should also include an assessment of the criticality of any data elements or rules to be measured. Recommendations for controls should describe how to reduce risks associated with source data or data processing.
	“Output from the initial assessment will also include valuable metadata; for example, improved definitions of data elements, documented business processes, and documented understanding of tech- nical processes. This metadata is also required as input for ongoing measurement and control.” (110-111)
</Text>
            <Notes>Good Advice
 - Which dimensions of a data set will serve as most critical for assessment and which are most at risk of undermining an assessment will be informed by both your interests and those of the potential (known) communities interested in using your research data. Of course, one can never know all of those future uses and future parties.

Data Expectations
 - Resolving the differences between your own expectations of the quality of the data and the expectations of research audiences - readers and users of it - will be helpful in providing as comprehensive a quality standard as possible for future modeling.

Data Standards
 - Standards can be used as a means of judging the qualties of comparable data sets and in turn models produced from those comparable data sets. Comparability is/can be a function of the establishment of those measurable standards.

Column Profiling

We tend to think of raw data as the “actual” data but given that the data itself was crafted by practitioners from a primary source (non-digital), it makes sense to understand the quality of that particular transition (transcription) – and thus its effects on usage of that data

Primary Source 
Sebastian-Coleman
Supporting Sources 
- Ball on Thomas Jefferson
- Ramsay on visual mapping of bridges

What are the “salient aspects” of data that we choose? Why are they salient?</Notes>
        </Document>
        <Document ID="EACF3D13-D2FB-4C26-9A4B-2BC018728415">
            <Title>Prose</Title>
        </Document>
        <Document ID="4D58821C-2B74-484B-A662-0F0EC2F502BA">
            <Title>Submission - Week of December 19, 2021</Title>
            <Text>Chapter 1: Data Quality for Literary Models

Section title: Introduction
Section purpose: Introduces the ideas of why this chapter exists. (Should this be moved to the general dissertation introduction?)

	The study of literature in the West has gone through varying critical paradigm shifts over the last century. &lt;Needs examples cited to be relevant claim?&gt;

Within the last twenty years, the scope and variety of studied literature has not only expanded but rapidly has also become more available in digital form. &lt;Needs citation to be relevant claim?&gt; 

In tandem, the machines we read them on have become capable of running more and more advanced algorithms that process an increasing amount of them for the purposes of study. 

These changes have altered the physical, cognitive, and cultural processes used in their interpretation. 

Despite a change of medium for its objects of study, this transition is yet again an instance of a once new, critical school becoming the old, and then meeting a new one. &lt;Is this a strawman? Are postmodern theory people necessary the old school unless they embrace modern digital study?&gt;

This liminal moment begets a push and pull between means of defining meaning and quality for that literature. What is literature when it becomes digital data? Does our reading of it change when it does? Where do we look for the answer to and how do we understand the old question, “What is ‘literary’?” Is it even a relevant question anymore? It is an idea created, stratified, and embellished across socioeconomic class of the author, publisher, and reading audience. Business and various interested institutions, academic and otherwise have constructed the notion – a function of both economy and social prestige. 

	One of those paradigms in the early twentieth century that had doubt cast upon it is formalism and its various iterations. At its heart, formalism promises that meaningful structure and aesthetic can be found objectively in the writing on the page. The doubt came from the situatedness of the author’s prose and the person reading and interpreting it. These are both reasonable doubts for formalism’s hermeneutic tenets. However, the structural and aesthetic analysis that can be done often denote irrefutable qualities that do clearly exist in the writing, regardless of that situatedness. In fact, looking back at the doubt cast upon those formalist methods, we find that the doubt itself is just as culturally situated as its target. We must therefore conclude that our analysis and interpretations of literature irrevocably denote qualities imbued by our own times and experiences, that meaningfulness itself is situated. How we choose to engage in analytical and interpretive method is a matter style but also a function of its intended outputs. With that understanding, we can incorporate seemingly alien methods like algorithms and artificial intelligence to look at the literatures that we value, and from there what can we do to determine perhaps new meaningful structural and aesthetic qualities in the paradigm of reading literature as data.

	A new layer of qualitative comprehension is produced via our treatment of literature as data. Depending on the context of study, there are numerous ways to understand our expectations of a dataset. Consider a business that runs web services for their clients that relies on a a daily stream of new data. While there are similarities in the relationship between the business/client and author/reader, the factor of time becomes much more important to the business. Time exists in a literary data set in multiple forms – be it publication history or in the more abstract notion of narrative time – but the action of prediction has a very distinct utility for the business’ data whereas for the literary data set it may bear a less crucial one. Still, as the past years of research in digital humanities and cultural analytics have used prediction to great effect ine exploratory data analysis, uncovering macro trends in a multitude of characteristics of literature and the people involved in its creation. One could ask what purpose would it serve to predict something that is already concrete and whole other than one to hone the action of prediction itself? In addition to uncovering macrotrends, cultural analytics at that scale serve that methodological purpose as well. Aside from newfound materials for a deceased author, there is no daily data coming in to refresh our understanding of their works. In that way of thinking, it becomes to look inward at the work itself and just what it is that depicts meaningful quality for the study of text as data. Those qualities might differ from work to work, from author to author, from researcher to researcher. There may be, however, some generally useful, a posteriori categories of quality to attend to that could help us bolster that understanding of “literary.”


Section title: Why quality?
Section purpose: Intro on reasoning for data quality assessments

	One significant challenge for qualitative interpretation of computational models of literature is the difficulty in ascertaining the condition of the underlying data set(s) from which those models are produced. Analysis of the complex data more typical to the contexts of humanities study (e.g. texts, artworks) – referred to more often as hermeneutics – has always used quantitative measurement even when it appeared in “qualitative” form. The work of comparison that analysis involves makes the use of proportionality inevitable. In truth, the difference between “qualitative” and “quantitative” analysis is not very clear, nor are the terms themselves so well-delineated. This confused basis sitting at the root of research signals the urgency of clearly understanding how our culture and its objects are described – and thus also defined in an age where information is ineffably inflected by computation. As the heuristics of data processing and information delivery scale upward, our sense of the quality of that data and the conclusions we can draw from it also fades more and more. Yet our needs for information are sometimes so pressing, be they research-, business-, or journalism-driven, that the question lingers as to the quality of the data we rely on for that information. And there are many means researchers and industry professionals have developed to assess that quality. However, the key to all of those means is the context of the data and resultant information – the context of a data source, the context of the tools used to measure and filter that data source, and the cultural context from where interpretations of those measurements and filtering occurs from. For computational text analysis, a form of humanities data analysis experienced now by people around the world via news reporting, a comprehensive means of data quality assessment has not been developed. One reason for this is that this form of data analysis, often due to its small data scale and purported lack of need for the exactitudes of science and business, has been deemed “soft” or too perspectival to be verifiable  – and thus not as worthy of attention and methodological development. This question of data quality can no longer be allowed to linger given the now-widespread consumption and influence of humanities data analysis. There are ramifications beyond the potential for inaccurate interpretations and conclusions for research that include how humanities disciplines are viewed and in turn sustained through funding and cultural support. The writing below is an attempt to provide an answer for the question of what data quality for the humanities could look like. It is an exploration where the “qualitative” interests of humanities research take precedent even if they inevitably stand on the shoulders of quantitative methods – because, of course, they always have just not so explicitly.

Section title: Data quality in other fields
Section purpose: Discussion of concepts as they exist in those fields ad how they are continguent on context

	Where should scholars performing this craft look for inspiration or influence? As it turns out, the answer is both within the field and from outside of it. As is routinely pointed in defenses of the subfield, humanists carry important ethical values of their own (not the only ethics/values) that inform the general culture of their field(s). I am not making an argument for or against a set of ethics/values here, but rather that specific sets of research values exist for the humanities and that they need be included in computational work. And this is for two reasons. The first is translational: so that the work can be communicated to fields outside of the humanities and reproduced by them. The second is so other scholars in the field can appreciate or even adopt the interpretations and given evidence to ensure the continuation of knowledge production begun by computational work lest it be shunted aside as parallel if disparate subculture itself. As for the former case of translation, we can borrow means of ascertaining these qualities from outside of the field, if but because fields who are based and versed in quantitative methods and evidence have produced such efforts. The most direct borrowing is the creation of a data quality framework. Laura Sebastian Coleman, an information scientist, has written on the creation of such frameworks. Her academic research and position overseeing the data architectures of private insurance and healthcare companies has given her the abstraction and experience to pen a significant work on the topic.  This is where the process of creating a data quality framework for a digital humanities project will begin. As we will see, such a framework does not stop with metrics of data quality. The effect is cumulative, where evidence of quality produces an ethos for subsequent research processes and subsequent research projects. Thinking of this framework as a physical foundation for the work to follow is a useful and (as it turns out) definitional metaphor for the construction of computational models of literature. Even the most complex of things must begin by necessity with a simple form.
	But what is data quality for information scientists? Coleman defines it as “the degree to which data meets the expectations of data consumers, based on their intended use of the data” (Coleman, talk slide 8). She renders in perhaps surprisingly subjective terms what will ultimately become a series of quantitative measures. Since there is no escaping bias in measurement, why pretend one can eliminate all of it? Instead, we explicitly define the quality rating of our data in terms of the degree to which it meets our expectation and intended use. Assessment of quality comes through understanding of “processes that created” our data, the “systems through which” our data was created, “the concepts” our data represents, and “the known and potential uses of [our] data” (Coleman, talk slide 9). This evaluation is ultimately a means of identifying and understanding the implications of errors in our data. In terms of literary data sets, errors may be both structural and functional. For instance, outliers in categorical fields may be determined to be errors in measurement. One can imagine multiple scenarios for such an error, the predominant source of which for text analysis might be OCR errors. Another way of thinking about errors though are misclassified works in a corpus. For example, if a novel falls far from the lexical norms of writings from a period and place it may be the result of a highly unique author, or it could be something else like a misdating or an authorial misattribution. When the scale of inquiry goes into the hundreds and thousands of works, errors like these may not be far behind depending on our understanding of the processes of the creation of that data. As Coleman notes, who transcribed and produced the metadata for a work, and what prescribed methods by which that transcription and metadata production happened become key means to determining the expected correctness of those editorial processes. Further down the line, those errors may produce unintended consequences for modeling and interpreting models. Up until this point, the preceding history of prominent computational text analysis in literary study has either written this off as negligible, statistical noise or produced flat values of “error” (as is common in statistics), neither of which offer a reasonable or sound explanatory foundation from which to base further study of subjected corpora.
	Furthermore, assessment of data quality is a cyclical process and imbues an unorthodox ethics for working with data for literary studies. In moving from measurement goals to data collection, data quality calculation, and comparison with an expectation (e.g. a norm), conclusions from assessments inherently require the creation of dataset variants that contain corrected errors. While the manipulation of data can inspire terror in the objectively-minded, the reality is that real world data is messy and working with it requires the mature understanding that our datasets contain creation bias and selections of that data are also inherently biased. Consequently, the assumptions upon which our selected measurements stand contain those biases. If a datum confutes our expectation and intended use, and one can determine that this confutation is produced via error, then – almost counterintuitively – it is incumbent upon us to emend it to proceed ethically (and if possible to report on that error to the data creator). All of the above requires a somewhat intimate knowledge of the items in our data set; if not a knowledge rooted at the closely-read level, then a loose (but flexible) expectation of what a work may contain. There will always be unidentified errors due to lack of expertise (a finite resource), but the closer a researcher can get to understanding a data set, the more trust can be built between them and their intended audience(s) that the modeling and their interpretation of it is sound. Thus re-framed in quantitative measures is also the primary justification as to why people who are trained in literary study are those best positioned to create large-scale, computed models of literature.

Coleman bases the data quality assessment framework she describes on several previous models for data quality constructed by information science researchers. Looking to the data quality framework described in “Beyond Accuracy: What Data Quality Means to Data Consumers” by Richard Wang and Diane Strong, Coleman lists four principle categories of data quality which will be useful for our purposes. Each category has its own concerns and specific “dimensions.” The categories and subordinate dimensions were all the product of extensive survey work with data consumers, and they each hold differing definitional value depending on the type of data being considered and its intended use. These categories are intrinsic, contextual, representational, and accessibility data quality. Intrinsic data quality is “the extent to which data values are in conformance with the actual or true values…Intrinsically good data is accurate, correct, and objective, and comes from a reputable source.” (Coleman, Appendix B, 1). Its dimensions include accuracy, objectivity, believability, and reputation. Contextual data quality is “the requirement that data quality must be considered within the context of the task at hand, understood largely as the extent to which data are applicable (pertinent) to the task of the data user…The focus of contextual DQ is the data consumer’s task, not the context of representation itself” (Coleman, Appendix B, 1). Its dimensions include amount of value-added, relevancy, timeliness, completeness and appropriate amount of data. Representational data quality is “the extent to which data is presented in an intelligible and clear manner” (Coleman, Appendix B, 1). “[T]he system must present data in such a way that it is easy to understand (represented concisely and consistently) so that the consumer is able to interpret the data” (ibid). Its dimensions include interpretability, ease of understanding, representational consistency, and representational conciceness. And finally, accessibility data quality “emphasizes the importance of the role of systems.” It is “understood as the extent to which data is available to or obtainable by the data consumer” (Coleman, Appendix B, 1).  Its dimensions include accessibility and access security.

What makes such a configuration of categories readily useful for literary study is its reflection of the roles and agencies of all those involved in the creation, filtering, and consumption of literary data. 

Each data quality category’s dimensions in turn allows us the means of measuring the success of one of those agents in living up to the principles behind each role-contextualized standard of quality. A few generalized, example possibilities illustrate this realization nicely. For instance, intrinsic data quality may be a measure of the success of matching an iteration (digitized copy, alternate edition, etc.) of a text to a physical or digital source that is considered to be the primary/reputable edition. Contextual data quality may be whether the metrical characteristics of the data set make it viable for the selected modeling task to follow. Representational data quality may ask several questions. Is the data collected from the text easily interpretable/understandable? Is it arranged in such a way to make it easily interpretable/understandable? And for when that metadata is displayed via interfaces, is it presented in such a way to make it easily interpretable/understandable? (And yes, all of the aforementioned can be quantified and/or categorized.) One can also imagine a whole ranking for accessibility quality via several avenues and understandings on physical accessibility. Is the consumer of literary data able to access a literary data set? The answer to which represents a well-known but, without a data quality configuration, overlooked standard for quality when performing computational text analysis. What is the source of the data set and what are the barriers in place to access it? Is it on a free site? Free but requires a login? Institutional? Paywalled? Limited to individual requests? Or maybe even not available at all? While such a standard may not immediately display its effect on quality of subsequent modeling, surely a “low” rating for accessibility quality would help us point researchers towards more accessible data sets. While some categories of data quality and their respective measurements may more inform the modeling process, others may more inform the internal and external ethics of the data. From here it will be useful to discuss each category of data quality and play them out via example scenarios for the sake of both demonstration and discussion of the ramifications that follow their implementation.


Section title: Intrinsic-Data Quality;  Or, How to Count with Words
Section purpose: Introduction of the “intrinsic” category of data quality and a demonstration of it over The Adventures of Huckleberry Finn

This story has to begin with the counting of words. What do we count when we count words? And why are we doing it? Are we obliterating meaning in the search of new truths? Coleridge felt strongly on the matter, just in a different context. In his writing, A dissertation on the science of method, he speaks of the alphabetical ordering of knowledge in encyclopedias as the destruction of original information – and certainly the destruction of data quality. His metaphor rings familiar with regard to criticisms of the counting of words, of computational text analysis in general: “[T]he desired information is divided into innumerable fragments scattered over many volumes, like a mirror broken on the ground, presenting instead of one, a thousand images, but none entire.” (Coleridge, A Dissertation on the Science of Method, 72) But can those original images, the truths of reading be recovered and enhanced by the act? Let’s begin with a simple example to illustrate the problem: the first paragraph of The Adventures of Huckleberry Finn by Mark Twain.

“You don't know about me without you have read a book by the name of The
Adventures of Tom Sawyer; but that ain't no matter. That book was made
by Mr. Mark Twain, and he told the truth, mainly. There was things
which he stretched, but mainly he told the truth. That is nothing. I
never seen anybody but lied one time or another, without it was Aunt
Polly, or the widow, or maybe Mary. Aunt Polly--Tom's Aunt Polly, she
is--and Mary, and the Widow Douglas is all told about in that book, which
is mostly a true book, with some stretchers, as I said before.”

Here narrator Huck Finn introduces himself by making a reference to Twain’s previously successful book starring Huck’s friend Tom Sawyer, acknowledging that his audience would not have heard of him were it for that book. But, as he says, that doesn’t matter because in the adventure to follow his readers will get to know him and his story well. He notes that while the author of that narrative, Mr. Twain, stretched the truth it was mostly accurate. Besides, most people with the exception of some virtuous outliers (Aunt Polly, Mary, the Widow Douglas) lie from time to time. You might be wondering why I’m providing such a plain retelling of the above paragraph. Let’s tokenize and remove so-called “stopwords” from that paragraph – processes most text modeling and analysis tends to do.

read book
adventures tom sawyer matter book
mr. mark twain told truth 
stretched told truth
lied time aunt
polly widow mary aunt polly tom aunt polly
mary widow douglas told book
true book stretchers

With all of these words buried away, what meanings can we glean from this same passage cleaned by our tool? We can guess, but without foreknowledge of the play, it’s a rorshack. Maybe the amount of our tool’s pruning was excessive though. One could imagine something like this below instead.

you don't know me without you have read book name
adventures tom sawyer ain't matter book was made
mr. mark twain he told truth mainly was things
he stretched mainly he told truth nothing I
never seen anybody lied one time another without it was aunt
polly widow maybe mary aunt polly tom's aunt polly she
is mary widow douglas is all told book
is mostly true book stretchers as I said before

A little more readable if still confusing? The point here is not about how we might construct a better stopword list, but rather the semantic toll on the original data and the subsequent semantic toll on the model for further analysis and interpretation the loss of data quality incurs. The scale of that loss often goes unmeasured, because, well frankly, we don’t bother to measure it. We say things like, “It’s good enough.” or claim that there is not enough suitable basis to judge error. But as you can see above, it very much might not be enough to shrug our shoulders and move our analytical heuristic to its next step: a fancy modeling method. What happens when there are poor digital editions – lossy from things like faulty optical character recognition, differences across editions, and overzealousness with methods like tokenization, stemming, lemmatization, and yes, stopword removal? Over the course of this chapter, we’re going to take a look at the status of all of the plain text editions of The Adventures of Huckleberry Finn from “Project Gutenberg” and “The Internet Archive.” These sites are frequently looked upon as bottom of the barrel either for the sake of provenance or digitization errors and through that we will see how data quality metrics can be used to both bolster our evidentiary bona fides and even help produce higher quality text data for modeling.

	•	Idea: DataLad versioning for DH data as part of chapter 1
	•	Idea:  Canonical Correlation Analysis (CCA) and/or Partial Least Squares (PLS) for comparing data sets’ different variables and potential correlation
	⁃	See: https://twitter.com/ar0mcintosh/status/1415627852360396802?s=20 and https://arxiv.org/abs/2107.06867
	•	Ideas from Mohammed’s talks: 
	⁃	Analytical flexibility: how much methodology affects the robustness of results
	⁃	Data-driven vs. state-based methods
	⁃	State-based methods 
	⁃	Want to assess the simliarity between their states
	⁃	Assessment - Is their a ground truth?
	⁃	Can use null hypothesis testing - but null hypothesis testing applicable to data-driven methods not applicable to state-based methods
	⁃	And similar thing about testing applicable to state-based methods, they are not applicable to data-driven methods




Shakespeare’s Richard III begins with Richard speaking on his brother Edward’s accession to the throne of England. “Now is the winter of our discontent” – it’s a famous one and hopefully familiar enough in sound to you to notice any changes.  It continues on another 37 lines from there. Cleaned up by a text analysis tool, the same speech might look quite different. For instance, here Richard’s original first sentence compared with that “cleaned up” version.

Now is the winter of our discontent
Made glorious summer by this sun of York;
And all the clouds that lour'd upon our house
In the deep bosom of the ocean buried.
winter discontent
glorious summer sun york
clouds lour'd house
deep bosom ocean buried


Let’s observe what has happened here. On the left in the original, Richard claims that his family’s time of “discontent” is over. This new time period has been made “glorious” by Edward’s accession (and the murder of Henry VI). The metaphorical skies have cleared, his family’s problems buried away. The sentence is in two parts divided by the semicolon, the second part a reiteration of the idea in the first. With all of these words buried away, what meanings can we glean from this same passage cleaned by our tool? We can guess, but without foreknowledge of the play, it’s a rorshack. Maybe the amount of our tool’s pruning was excessive though. One could imagine something like this below instead.

now is winter of our discontent
made glorious summer by sun of york;
all clouds lour'd upon our house
In deep bosom of ocean buried.

Let’s take a visual look at what has happened to the word counts.
</Text>
        </Document>
        <Document ID="9AE3F4C5-50CE-4DB6-9C46-A3B31375B16B">
            <Title>Adventures of Huckleberry Finn</Title>
        </Document>
        <Document ID="32B87E5D-DC77-4402-AF66-B1E8FC73BB6D">
            <Text>The questions I’m interested in answering

How can we make useful/imaginative interpretations of and conclusions about local information (read: novel, poem, article, etc.)?

Let’s say we have 100 ideas, each of them represented by a bag of words. Let’s pick some arbitrary division of them. Of the 100 ideas, 2 of them are “good” (maybe they led to successful novels, music, inventions, etc.) and 98 of them are “okay” to “bad.” How could I locate those 2 good ideas among  the 100? Well, I could use internal information only - word frequencies, topics, sentence structure, parts of speech patterning, etc. But at such a small scale (in both number of things and counts of words), ascertaining whether or not I have representative sample given that smallness is not possible. Similarly, it may not be possible to distinguish notable differences given the small sample and small counts of the features (the modeling method we are using to distinguish ideas, in this case).
So, where can we look to make that determination? The answer must be outside, from evidence (in this case, written evidence) emanating from the culture in which those ideas were formulated. We need to understand the contexts within which those ideas originated. Let’s say you have or create a model (via similar, chosen modeling method) about a larger set of evidence from the outside world. For instance, you have a word vector model of thousands of written elements from that culture - the composition of which can be circumscribed for relevance.
Our goal is to create a mapping (via some kind of transformation; linear, etc.) between the outside world model space and the model of the 100 ideas (space). The next task is to be able to locate an appropriate subspace in the model to calculate the translation. We can locate “good” ideas in the outside world model and formulate a subspace of that model based on those elements that pertain to those “good” ideas. Once we do that we can further filter the subspace. For instance, locating every word in the 100 ideas in the “good” ideas elements (documents, etc.).. The values of those words in the outside world model (i.e. words in a word vector model) can be directly compared to the values of those words in the comparable ideas model. 
Once we have a set of transformation values, we can create a summary rating for the closeness of each idea in the 100 idea set to the good idea elements from the outside world model. The closer in value, the more likely an idea from the 100 idea set is to be similarly “good.”

Further considerations
	1.	Is this akin to a more manual approach to supervised learning? 
	2.	Is this akin to (k-means) clustering?
	3.	Furthermore, this value of this heuristic can be tested via cross validating approaches that test to see if good idea elements from the outside world model are successfully rated as good ideas when they are taken out of the sample good ideas set.</Text>
        </Document>
        <Document ID="3F88FB9D-4A02-4271-9DD3-535DFF72BD3B">
            <Title>September 11, 2020</Title>
            <Text>Notes
Notes on a project for Dickinson literary quality

This is the first writing for a project about searching out in the poems of Emily Dickinson what I am going to call “literary quality.” This is a metaphorical construct for understanding large amounts of literature with a particular structural conceit: the balance between innate, personal knowledge of a set of works and the necessary personal remove to be able to quantitatively parse that set for the purposes of new, transformative understanding and readings of those works.

That’s the hope we always have as literary scholar, of course, when approaching a collection of literature. A new, transformative reading. Typically that goal is accomplished through an interpretation informed by personal reading experience and new comparative study (e.g. against other works, other authors, other eras, other spaces and often in relation to biography).

So let’s begin our recipe.

First step

Process Emily Dickinson’s works into sets ordered by the publication orders we have available.

Second step

Look at previously established data quality metrics and apply them to each set

Third step

Perform a comparative analysis of metrics across sets

Fourth step

Look at the poems that were determinative in any differences surveyed during that comparative analysis and speak about their condition, placement

Fifth step

Write on their significance in the Dickinson ouevre and in Dickinson scholarship

Sixth step

Produce a new Dickinson publication ordering that utilizes the versions of those determinative poems that would help produce – as a collection – the highest possible data quality (re: those previous metrics).

Seventh step

Show how that new collection fares under/alters close readings and computational modeling of this new collection</Text>
        </Document>
        <Document ID="5D6F3DE4-D9A9-4DE2-9531-AA9B2411920B">
            <Title>Model Quality</Title>
        </Document>
        <Document ID="6553BF9C-1978-4BBC-8AF7-D3DCBB93D564">
            <Title>January 14, 2025</Title>
            <Text>This work is not for everybody – meaning not everybody will want to engage in this kind of quality-defining work
However, this work is for everybody – in the sense that its outputs produce a level of confidence in datasets being used for modeling operations whereas without it we would have little precise confidence in the underlying data

Defining data quality requires work – both in assessing texts, refining them (if warranted), and writing reproducible measurement code so that the current dataset and any other edition/iteration of it can be assessed with the same criteria and the same mechanism(s). Typically datasets are located, assessed, cleaned and prepped for use with particular experiments and projects. This dissertation suggests that defining/assessing the qualities of a dataset and its overall data quality is a new research activity in need of formalization, and that it is an activity that can bear fruit for many future researchers interested in the dataset (subject matter) being visited by these activities/exercises defined within this dissertation (and beyond).</Text>
        </Document>
        <Document ID="AACFDAA7-FD55-4A41-A987-D6DCA9CC9423">
            <Title>Data Quality in Legacy DH</Title>
            <Text>Notable mentions of data quality in digital humanities writing:

1. “Now is the Future Now? The Urgency of Digital Curation in the Digital Humanities” (2013) in Digital Humanities Quarterly
	- Abstract:
In their seminal report, Our Cultural Commonwealth (2006), the American Council of Learned Societies underscored the need for scholars engaged in digital humanities work to leverage their access to data both to expand their audience to the general public and to generate new research questions. “Now is the Future Now?” argues that the progress made in digital humanities toward these goals has depended and will depend not only on digital data, but also on their appropriate curation. The article defines digital humanities, data, so-called Big Data, and digital curation. Next it examines digital curation initiatives in the sciences and in the humanities that occurred before the release of Our Cultural Commonwealth. It then considers and evaluates the digital curation work undertaken in the sciences and in the humanities after the report’s publication. In theory and in practice digital curation has benefited substantially from practices developed and tested first in the natural sciences and subsequently adapted for and extended in the humanities. Finally, the piece explores the future work necessary to facilitate symbiosis between digital curation and digital humanities. Collaboration and cooperation, transcending geographical, disciplinary, and institutional boundaries, data sharing, policies and planning, education and training, sustainability — all remain pressing issues in 2013.
	- Topics: digital curation
2. The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards (2018) in arXiv
	- Abstract: 
Artificial intelligence (AI) systems built on incomplete or biased data will often exhibit problematic outcomes. Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label (the Label) is a diagnostic framework that lowers the barrier to standardized data analysis by providing a distilled yet comprehensive overview of dataset "ingredients" before AI model development. Building a Label that can be applied across domains and data types requires that the framework itself be flexible and adaptable; as such, the Label is comprised of diverse qualitative and quantitative modules generated through multiple statistical and probabilistic modelling backends, but displayed in a standardized format. To demonstrate and advance this concept, we generated and published an open source prototype with seven sample modules on the ProPublica Dollars for Docs dataset. The benefits of the Label are manyfold. For data specialists, the Label will drive more robust data analysis practices, provide an efficient way to select the best dataset for their purposes, and increase the overall quality of AI models as a result of more robust training datasets and the ability to check for issues at the time of model development. For those building and publishing datasets, the Label creates an expectation of explanation, which will drive better data collection practices. We also explore the limitations of the Label, including the challenges of generalizing across diverse datasets, and the risk of using "ground truth" data as a comparison dataset. We discuss ways to move forward given the limitations identified. Lastly, we lay out future directions for the Dataset Nutrition Label project, including research and public policy agendas to further advance consideration of the concept.</Text>
        </Document>
        <Document ID="34155603-147D-4F24-865B-700ABFCE86A5">
            <Title>Application</Title>
        </Document>
        <Document ID="BB7B70F8-E0C6-46D8-BCD1-299294685ED2">
            <Title>March 26, 2025</Title>
            <Text>What I mean by the "art" of literary modeling - a lofty title. Modeling of literature is a practice, an art form just like any other.
We make human decisions based on our experience and knowledge, and those are well worth documenting in what seems a scientific or
quantitative process. Each of these decisions affect our data, our models, and our interpretations. *Interpretations* That's something
well worth highlighting because that is the strength of a humanities approach to the computational modeling of literature. We have to
remember and remind one another of the humanity that went into the creating of that literature and the humanity that goes into our 
understandings - plural - of it.

NOTE: JSON file describing metric evaluation weights to be included along as output alongside data quality framework outputs

'/Users/weirdbeard/Documents/school/aolm_full/data/lexicon/coha/lexicon.txt'</Text>
        </Document>
        <Document ID="BEA17C1D-E655-4CBF-9CEE-6C89957C98E5">
            <Title>Pollan - Site for Modeling</Title>
            <Text>
The Site for Modeling

Where do we place our chairs when we want to look at the landscape of a work of literature or a corpora of literature? Fully understanding the perspective we want to have when approaching the creation of model on this site of inquiry can help us down the road in the modeling, visualization, and analytical processes. What are our assumptions about our data set? What are our hypotheses? Fixing these things at the beginning of the process – fixing our perspective will allow for the objectivity necessary to understand our results and interpretations, as well as giving us a better idea of where we want to reposition the chair. After all, a model is easier to rebuild than a house.
</Text>
        </Document>
        <Document ID="D0FA431C-3C52-46C1-B0BE-D750BE6654FA">
            <Title>May 14, 2019</Title>
            <Text>	
Introduction prose
A model represents. It is constructed and positioned. It does many things. It moves from the complex to the simple. Sure. But it also sits in space. It shares contexts – physical, technological, and cultural. A model conveys not only that which it represents, but its authorship and lineage, the milieu in which it was formed and for what purpose(s) it was formed.  Consider also that the act of modeling is undertaken by a practitioner in a particular discipline. The selected perspective of a model then is not just its relation to that which it represents, or also to the personality that formed it. The disciplinary context that provided that personality their &lt;cause to model&gt; drives its existence. As denizens of the Information Age confronted with the impracticality of the profound amounts of information we have acquired, we turned to modeling. The act was always there, but its primitive and implicit nature were belied by the ego of complexity. That is something that must be declared from the outset – this new understanding that arose from the ashes of that age. In order to deal with the world, we model it. It is an inescapable fact that the world is more complex than our brains and our senses can deal with. We select characteristics of things. We measure them. We group those things together by those characteristics. Analyze, interpret, criticize, and repeat. The qualitative natures of those acts however are still realities we are grappling to entertain and to utilize with an informed ethics. For virtually any scientific discipline, the action of modeling itself is reflexive and genetic. In comparatively rarer instances, it is also philosophized. In fields that deal with the construction of physical objects, like architecture or mechanical engineering, the model is so intrinsic and foundational, that the fields could scant exist without the notion. In literary studies, a field so-described as more ‘qualitative’ and ‘interpretive’ in its outputs, the actions of modeling are very implicit. 

Notes
Martha Nell Smith (mention Coleman) returns us to the metaphor for Emily’s project and her own of housebuilding, of poem building, of collection building. So let us continue on to build a literary model. And first we will start by marking our site.

Martha Nell Smith on the project of Emily Dickinson and the Dickinson Editing Collective’s project of transcribing/digitizing her work
"Because the Plunge from the Front Overturned Us: The Dickinson Electronic Archives Project"
Studies in the Literary Imagination 32:1 (Spring 1999)
http://archive.emilydickinson.org/plunge1.html
“In poems such as "Myself was formed - a / Carpenter - " (F 22; P 488), Dickinson herself compares the work of a writer to that of a builder, and in lyrics such as "I dwell in Possibility - " (F 22; P 657) compares poetry itself to a "House." In this metaphorical scheme of things, a poem's "front" is comparable to that of a house. Constructing a house, the last things built are those things most easily visible--the frontispiece of brick, wood, paint, doors, shutters, porches, roofs; making a poem for conventional public distribution, the last things "built" are the pages of the book in which it appears, the frontispiece of typesetting, arrangement, font, paper type.”

To do
Two things needed:
1. Example of modeling in the sciences
	Emily Dickinson in Nature
	•	ref. mood disorder - Vivid imaginings by Kay Redfield Jameson — Book review of Eccentrics by David Weeks and Jamie James (1995)
     Emily Dickinson in NLP journals
	•	in ACL - Gagliano et al - “Intersecting Word Vectors to Take Figurative Language to New Heights”
2. Example of modeling in the humanities (pre-digital)</Text>
        </Document>
        <Document ID="93471D41-0E3E-4C9B-AA6B-39B0917F8F58">
            <Title>Preparatory</Title>
        </Document>
        <Document ID="CC37D200-62D7-48D5-9D45-F1BC97363387">
            <Title>Prose</Title>
        </Document>
        <Document ID="19DF8594-A0D1-4D9F-AE29-8BB52CD9A90A">
            <Title>Mocnik - A grounding-based ontology of data quality measures</Title>
            <Text>	◆	On why data is created: “data is often created with having a purpose in mind: What is the desired scope of the data?” (1)
	◆	On fitness of purpose the data set was created for: “Not only data quality but also the fitness for a purpose is thus examined” (1)
	◆	On inexpert data creators: “People contributing to VGI and AGI usually have very different aims and different abilities, which is in contrast to conventional datasets that are often created by a few experts who follow strict rules and guidelines” (2)
	⁃	“quality of such heterogeneous datasets can accordingly not be assessed in its entirety by the interpretation of the data with a fixed ontology and a subsequent comparison to reference data” (2)
	⁃	
	◆	On heterogeneity of creation process within a data set: “The internal characteristics of a heterogeneous dataset seem thus to be important with respect to data quality. Among these characteristics are the process that generated the data, the history of the data, the methods to retrieve the data, the sources from which the data is aggregated, and the different purposes for which the data has been collected” (2)
	◆	On external vs internal data quality assessment: “One obvious choice is to compare, for example, a map to the actual environment or to a reference dataset that is assumed to be of high quality. On the other side, also the internal characteristics of a dataset can be used to assess its quality” (2)
	⁃	Intrinsic and extrinsic measures: “As a distinction between “approaches for intrinsic assessment [. . . ] that do not use external reference data” [42] and intrinsic measures that can be evaluated ‘without the usage of any reference data’ [5] the concepts of intrinsic and extrinsic measures have been introduced.” (2)
	◆	On where information comes from and the relation to data quality: “In this article, we extend this question to where the information comes from that allows us to draw conclusions about data quality.” (3)
	⁃	“Thus, the where question from above refers to the source from which additional information is gained in order to be used by the measure. Sources for such additional information can be our perception of the environment, heuristic principles, other data from the same dataset, etc.” (3)
	◆	 
	◆	
	◆	IDEA - Data quality metric - Alignment ratio across multiple digital editions of the same text
</Text>
        </Document>
        <Document ID="495AD012-338D-4825-BA97-B6EDF0F4343C">
            <Title>April 25, 2019</Title>
            <Text>To do
	1.	Must include works for first chapter:
	⁃	C.P. Snow “Two Cultures”
	⁃	F.R. Leavis’ response to “Two Cultures”
	⁃	Miranda Hickman also cites P.D. Leavis
</Text>
        </Document>
        <Document ID="3E849A8C-1DA4-4685-B928-2F43047A8B30">
            <Title>Prose</Title>
        </Document>
        <Document ID="DB89B126-9EFD-45C5-8C3A-D5BB5DC45F1F">
            <Title>Gone through</Title>
        </Document>
        <Document ID="E5744F81-F08C-45DF-AFE4-2C2ECF4D199F">
            <Title>Application Instructions</Title>
            <Text>Completed applications should total no more than 1,250 words, to include 

1) a description of the project you intend to pursue, including an intellectual justification of the goals, description of methods to be used, and description of sources; and 

2) a biographical statement of applicant’s areas of scholarly and professional interests in the digital humanities, including relevant experience.

Applicants will also be asked to provide the following:

3) A 250-word abstract describing the project for an interdisciplinary audience.

4) Current CV.

5) NYU transcript (official or unofficial copies accepted).

6) Letter from advisor or DGS indicating that they are aware of the student’s application.</Text>
        </Document>
        <Document ID="143FC4C6-9213-456B-9682-7AF2D22A5E6A">
            <Title>Chollet - Deep Learning with Python</Title>
            <Text>============================

Chollet (60): “Picking the right network architecture is more an art than a science; and although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. The next few chapters will both teach you explicit principles for building neural networks and help you develop intuition as to what works or doesn’t work for specific problems.”

Chollet (223): “As always, deep learning is more an art than a science. We can provide guidelines that suggest what is likely to work or not work on a given problem, but, ultimately, every problem is unique; you’ll have to evaluate different strategies empirically. There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate.”</Text>
        </Document>
        <Document ID="7B1506A4-8DB3-4355-A80F-AA8F985CC50D">
            <Title>April 9,  2025</Title>
            <Text>- Wrote a function to calculate dq metrics for the ur edition (MTPO). You can only calculate so many of them however because many dq metrics depend on comparing to other editins (intra-collection, i.e. metadata suffiency) or against an ur edition (extra-collection, i.e. text records to control records). In that case, the dq metric value is 100% (or 1.0). However, metrics that are compared against sources outside of both the ur edition and compared collections can be done (i.e. lexical validity)
- It’s not clear how useful it is to calculate the dq metrics for an ur edition, but it could possibly give a rough idea of generic measurements of quality of the edition. These can be used in comparison to the dq metric values for compared editions</Text>
        </Document>
        <Document ID="A33A2A02-AB5A-4D5C-BD3F-F7469E4EF934">
            <Text>December 20, 2024

When I was younger, I would watch Star Trek: The Next Generation. There were all sorts of literary figures and things on this show. Some, you wouldn’t even think were literary. What do I mean by the word “literary”? Well, if you think about it it’s anthing associated with the writing or reading of history. History. That’s a funny word too. Any gender association with the word aside, it’s our story. Humanity’s story. So who gets to tell it? And what will they be telling when they do? That’s why Star Trek: The Next Generation and any other science fiction like it is such a notable place to start this story about literary modeling. You have Patrick Stewart, the actor who himself is very literary. A famed stage actor whose own life and his character Captain Jean-Luc Picard turns much of his thought and decision-making on humanity’s literary figures and books. You have Levar Burton whose life, aside from award-winning acting, is almost entirely associated with storytelling and literacy. His famed television show Reading Rainbow taught generations of children the importance of reading and of stepping into other people’s shoes. He continues that tradition to this day with a storytelling podcast. Burton’s character on Star Trek, Chief Engineer Geordi LaForge, is also very closely associated with literature and storytelling but not in the same traditional ways that Jean-Luc Picard (Patrick Stewart) is. As an engineer aboard the starship Enterprise in the particular 24th century the story is set, Geordi oversees the ship’s many holodecks. What is a holodeck? Holodecks are rooms where the prospective technology of holography is used to place people in that room into wholly different environements, fictive, distant in space and/or time. These environments can feature anything imaginable too: water, sunsets, historical figures like Plato or Mark Twain, fictional figures like Ahab or Sherlock Holmes, storylines with plots, operas, trees, animals,… The only limit is one’s imagination and the ship’s computer’s ability to generate it using light waves to model and mimic the sight, sound, feel, smell, and even taste of those things. Given our technology at this point in the 21st century it’s actually not so difficult to believe that mimicking these things via light is possible. We know that because of how statistical modeling has been merged with machine learning so that we can model the world. Some of the major impediments between us and this fictive 24th century technology are the computing power to create these models – and to do so convincingly – and the technology that emits light in such a way that it could convincingly be used to represent virtual environments, virtual objects, and virtual people. But there is another impediment that was not really mentioned in Star Trek: The Next Generation or subsequent Star Trek shows. Sure, we had a few chances to see characters talk of building holographic worlds and storylines for other characters to experience, but we did not hear much of anything about who and how the historical and literary information that is available to the ship’s computer came into being. 
When I would be watching the show, and to this day, I would often ponder the answer to those questions because it does matter who provided those stories and that information and how they did it. That’s because in order retrieve, process, and store that data each step along the way requires notable means of subjective synthesis. There are many versions of stories and of history. How do we decide which to choose? You might be saying to yourself at this point, “Well, who really cares? Star Trek: The Next Generation is an entirely fictional story.” You’re not wrong. But given the state of the technology we have today where machine learning has birthed deep learning and large language models, we can already see the beginnings of those syntheses. Ten years ago at Dartmouth, I was giving one of my first digital humanities talks. The topic was a plot-generating book by a 1920’s pulp novelist. In the 1920’s, fiction publishing was really being turned into an industry. Authors were churning out more books than ever before to a hungry reading public, and also so that everyone in the publishing industry could make a consistent living. William Wallace Cook made a book called Plotto which contained an intricate and deep plot generating system which authors could use to construct the bones of their stories before filling in the details. At the time, there was also much optimism about automation and the beginnings of roboticism were taking hold of the public’s imagination. One newspaper story, associated with Cook’s new plot-constructing book even featured the image of a robot, stating, “In the future, robots will tell our stories.” I made sure to put that one up on a slide in my talk. I did so partly as a joke, but also because I could already foresee how machine learning could be used to generate convincing stories. In the future, we will continue to have human authors – the reason being that history remains our story as humans and humans will remain interested in other humans’ stories. However, it may not be a human who is doing the telling of those stories. And that is our real world connection to Star Trek: The Next Generation. We know that in our future, “robots” or “artifical intelligence” will be telling our stories from statistical models of the texts that we compose and that have been composed by our predecessors. The focus of my writing within is on the literary models from which our stories will be told and how the quality of those models can ensure that those stories are synthesized and told as accurately as possible to the future humans hearing them. Who fed those stories and information to our future computers and how did they do it? How does humanity interface with a digital future? I believe we are at a time in history where some of us will get to be those people and to decide how we do so.</Text>
        </Document>
        <Document ID="ACD4F870-93D0-46B1-AEBA-C160FE6B591A">
            <Title>Art of Literary Modeling - Beginning - October 21 2019</Title>
            <Text>Jonathan Armoza
Art of Literary Modeling
Chapter 1 Opening Sections with Roadmap - Rough Draft
11/9/2019

The Site for Modeling

	One difficulty in writing about this topic is that I am defining a new research activity for a field. It requires another adjustment for humanities scholarship that relies on computational evidence. Self-reflection on the qualities of the evidence being used and the qualities of the model built upon that evidence can yield stronger results and a more sound basis from which to make interpretive claims. The hypothesis lays itself out plainly, but several questions arise when determining units and measurements for the quality of literary evidence. The first being, Why do this at all? Evidence should have authority. A significant basis for the production of literary analysis is ethos whether it be of the author themselves or the sources which an author cites. This prompts another question when considering quantitative measures in a field of study that traditionally produces and relies on evidence that is qualitatively-framed. What imbues quantitative evidence with authority? This work is entitled the art of literary modeling for this purpose. There is no generic heuristic for authority. The informed judgment of the creator/user/interpreter of a dataset produces that authority. But the products/evidence (?) of this techne or craft can be quantified. 
	Where should scholars performing this craft look for inspiration or influence? As it turns out, the answer is both within the field and from outside of it. As is routinely pointed in defenses of the subfield, humanists carry important ethical values of their own (not the only ethics/values) that inform the general culture of their field(s). I am not making an argument for or against a set of ethics/values here, but rather that specific sets of research values exist for the humanities and that they need be included in computational work. And this is for two reasons. The first is translational: so that the work can be communicated to fields outside of the humanities and reproduced by them. The second is so other scholars in the field can appreciate or even adopt the interpretations and given evidence to ensure the continuation of knowledge production begun by computational work lest it be shunted aside as parallel if disparate subculture itself. As for the former case of translation, we can borrow means of ascertaining these qualities from outside of the field, if but because fields who are based and versed in quantitative methods and evidence have produced such efforts. The most direct borrowing, and one that this work focuses on is the creation of a data quality framework. Laura Sebastian Coleman, an information scientist, has written on the creation of such frameworks. Her academic research and position overseeing the data architectures of private insurance and healthcare companies has given her the abstraction and experience to pen a significant work on the topic. If reading about a healthcare information scientist in the beginning of a work on literary modeling seems askew – stay tuned. And this is where the process of creating a data quality framework for a digital humanities project will begin. As we will see, such a framework does not stop with metrics of data quality. The effect is cumulative, where evidence of quality produces an ethos for subsequent research processes and subsequent research projects. Thinking of this framework as a physical foundation for the work to follow is a useful and (as it turns out) definitional metaphor for the construction of computational models of literature. Even the most complex of things must begin by necessity with a simple form.
	The creation, modeling, and interpretation of literature all share a characteristic so universal and seemingly innocuous that it would hardly be worth noting if but for the transformational effect of that characteristic. These activities begin and end in socially and physically situated spaces. Space is the beginning of such argumentative activities and structures.  What space contains our arguments? In the late 1990s for instance,   author Michael Pollan ventured to create a space from which to write books – and then he wrote a book about the process of creating that space. What does writing have to do with constructing a computational model of literature? In A Place of My Own, Pollan explores the connections between the physical spaces we inhabit and how they alter our interpretations of the world around us. But also, tellingly, Pollan’s own journey depicts how that outside world and its history informs the creation of those spaces. By journey’s end, he gazes out a large window at a desk surrounded by his books. It bears beginning a work on the qualities of modeling literature with this ending. Like many works of fiction, the initially chosen perspective imparts a feeling, foreshadows what is to follow. The constructed vantage of an author is laden with consequence for the work of storytelling. In Conrad’s Heart of Darkness, a boat ride up the Thames becomes the position from which readers are given to imagine the same journey throughout preceding history all the way to the time of Roman conquest. Chopin’s The Awakening begins with a green and yellow parrot in a cage that can speak French, Spanish, English and “a language which nobody understood” (1). These are hardly coincidental perspectives. They are deliberate. That is all to say, the work of representation begins with perspective. Pollan uncovers the subconscious, cultural roots of his desire for this particular writing house perspective, tracing it back to the introduction of the lockable writing desk during the European Renaissance, and then to rooms like studies for writing desks and books – private spaces to think while surrounded by the ideas of others and from which one could look upon the world with remove. Throughout this work I call attention to this analogy not to torture a metaphor of construction (and it will be revisited in different stages of constructing models for literature) but because modeling is metaphorical. The language and motivation by which it occurs is intensely physical and spatial. They operationalize the process of creation. Yet, because of the abstract nature of the mathematics and materials involved (“data”), this motivation is often lost in the work. (possible as footnote?)	
	Pollan’s architect partner ‘Charlie’ (Charles Myer of Charles R. Myer and Partners) notes at the project’s outset that this “first fact” of perspective is a “key element” of the building’s design (Pollan 31). The author prototypes perspective by taking a chair and positioning it at various prospective building points on the property. Transposing this into the context of utilizing or creating a dataset from a field of literature, we see that it is not even the foundation – the dataset itself – that ultimately and significantly informs the research to follow. The dataset’s boundaries has already been framed by its creator. It is the placement and angle of our metaphorical chair on that foundation the projects an inward- and outward-reflecting functional perspective amid the field of related literature. What will we be able to see? And what will we want visitors to our research to see? Not only is this prototype an argument (Galey and Ruecker 2010, https://doi.org/10.1093/llc/fqq021, https://academic-oup-com.proxy.library.nyu.edu/dsh/article/25/4/405/998338), but the portion of a dataset we wish to visit and analyze is also an argument. Where do we place our chairs when we want to look at the landscape of a work of literature or a corpus of it? Fully understanding the perspective we want to have when approaching the creation of model on this site of inquiry can help us down the road in the modeling, visualization, and analytical processes. What are our assumptions about our data set? What are our hypotheses? Affixing our perspective at the beginning of the process allows for the objectivity necessary to understand our modeling results and interpretations, as well as giving us a better idea of where we may want to reposition the chair in the next iteration. (A model is easier to rebuild than a house – so here the analogy falters and helpfully so.) And that fixity is imperative for the soundness of the building ahead. In this process in order to see where we are going we must remember where we came from, but the memory must retain a clarity. The work of selecting a portion of a dataset is not new to digital humanities work. Understanding and rating the qualities of the data and metadata in that dataset and of our selection from that dataset is, however. The construction of a data quality framework begins with this assessment.

Data Quality and Laura Sebastian Coleman

	But what is data quality? Coleman defines it as “the degree to which data meets the expectations of data consumers, based on their intended use of the data” (Coleman, talk slide 8). She renders in perhaps surprisingly subjective terms what will ultimately become a series of quantitative measures. Since there is no escaping bias in measurement, why pretend one can eliminate all of it? Instead, we explicitly define the quality rating of our data in terms of the degree to which it meets our expectation and intended use. Assessment of quality comes through understanding of “processes that created” our data, the “systems through which” our data was created, “the concepts” our data represents, and “the known and potential uses of [our] data” (Coleman, talk slide 9). This evaluation is ultimately a means of identifying and understanding the implications of errors in our data. In terms of literary data sets, errors may be both structural and functional. For instance, outliers in categorical fields may be determined to be errors in measurement. One can imagine multiple scenarios for such an error, the predominant source of which for text analysis might be OCR errors. Another way of thinking about errors though are misclassified works in a corpus. For example, if a novel falls far from the lexical norms of writings from a period and place it may be the result of a highly unique author, or it could be something else like a misdating or an authorial misattribution. When the scale of inquiry goes into the hundreds and thousands of works, errors like these may not be far behind depending on our understanding of the processes of the creation of that data. As Coleman notes, who transcribed and produced the metadata for a work, and what prescribed methods by which that transcription and metadata production happened become key means to determining the expected correctness of those editorial processes. Further down the line, those errors may produce unintended consequences for modeling and interpreting models. Up until this point, the preceding history of prominent computational text analysis in literary study has either written this off as negligible, statistical noise or produced flat values of “error” (as is common in statistics), neither of which offer a reasonable or sound explanatory foundation from which to base further study of subjected corpora.
	Furthermore, assessment of data quality is a cyclical process and imbues an unorthodox ethics for working with data. In moving from measurement goals to data collection, data quality calculation, and comparison with an expectation (e.g. a norm), conclusions from assessments inherently require the creation of dataset variants that contain corrected errors. While the manipulation of data can inspire terror in the objectively-minded, the reality is that real world data is messy and working with it requires the mature understanding that our datasets contain creation bias and selections of that data are also inherently biased. Consequently, the assumptions upon which our selected measurements stand contain those biases. If a datum confutes our expectation and intended use, and one can determine that this confutation is produced via error, then – almost counterintuitively – it is incumbent upon us to emend it to proceed ethically (and if possible to report on that error to the data creator). All of the above requires a somewhat intimate knowledge of the items in our data set; if not a knowledge rooted at the closely-read level, then a loose (but flexible) expectation of what a work may contain. There will always be unidentified errors due to lack of expertise (a finite resource), but the closer a researcher can get to understanding a data set, the more trust can be built between them and their intended audience(s) that the modeling and their interpretation of it is sound. Thus re-framed in quantitative measures is also the primary justification as to why people who are trained in literary study are those best positioned to create large-scale, computed models of literature.

	Let’s begin with an age-old question from Emily Dickinson scholarship and see if we can get part way to answering it – or, in the very least, what it would take to begin answering it. For the sake of this experiment, I’ll flavor the question a bit towards something that can be approximated via quantification: Can Emily Dickinson’s word usage be used to determine a timeline for the authorship of her poems? The stakes are high given the context, but remember, this is just a hypothesis to guide a data quality framework. Ultimately, the aim is not to answer the question of a definitive timeline, but to see if linguistic dimensions of her writing can be shown to estimate one.   Showing they can do so is also irrelevant to the question. That’s because the goal of this research process is temporarily and purposefully unidirectional. If the linguistic dimensions I test prove unfruitful towards a positive result, that is a useful finding for further inquiry and one that should be reported on. With those lowered stakes, I’ll first select a few measurements which I think can be determinative of providing an answer to that hypothetical question. 
	The first step will be to get a good quantitative feel for Dickinson’s vocabulary. A classic bag of words approach to comparing the vocabulary and frequency of words in her poems is a helpful beginning for its simplicity. Here’s where you walk around the property and set your chair in various places. You’re getting a feel for the data set using some very basic properties and perspectives. And why? So you know your data before you start throwing it into some more advanced model. Don’t let some proposed computational model in-the-waiting guide your choices of selection and measurement; let it be reading that first guides you. In a bag of words model (and it is already a model), one counts the instances of words in a textual body – a poem in this example –  to form a list of the words and their respective counts. (Punctuation is, of course, also countable, and I will do so in the instance of Dickinson’s poems since punctuation is used to such effect.) Mathematically this list can be thought of as a geometrical construct called a vector. Think of it like a line that begins at the origin of an X,Y graph that extends in a particular direction with a set length. In physics, vectors are considered to represent a force moving in a particular direction with a magnitude. The latter is helpful to consider for the next transformational step for a vector in our bag of words. In order to remove the complication length presents to comparability when counting the words of poems, the vectors undergo a transformation of scale. Each number of the list is divided by the sum of its numbers. This has a “normalizing” effect that showcases the most basic, proportional components of each vector’s force. 
Next I want to examine how these vectors are related in geometric space. Since each list of numbers extends well beyond three or even four in size (i.e. the physical dimensions in which we experience existence) some imagination is required here, particularly if one wants to visualize the results. But at this point there is no need to do so. Let’s take a very basic and age-old algorithm, K-means Clustering, to understand the similarities and differences of these vectors. There are more advanced and even more accurate/interpretable means of clustering vectors (displaying their closeness), but again, the thought here is to keep our methods as basic as possible in order to get a feel for the data. The Dickinson corpus of poems is one complicated by authorship, publication history and differing approaches to their study. The Emily Dickinson Archive provides lightly marked up TEI editions of each manuscript as identified by the several iterations of popular and academic publication of her works. One of the latest and most comprehensive is R.W. Franklin’s The Poems of Emily Dickinson (2005). Franklin’s edition proposes a core 1,789 “distinct poems” (“Emily Dickinson Archive”). (I will outline the history of Dickinson’s work and its possible complications for our modeling a bit later on.) In order to perform k-means clustering however, one must pre-determine a number of clusters. Typically, the approach to determining the correct number is iterative and is in of itself a hypothesis; like proposing a room layout for our yet-to-be constructed building given all the considerations that went into our perspective. In the case of a building, we would consider possible views, the path of the sun and natural lighting. Here, the “perspective” is R.W. Franklin’s selection of works and editorializing. To give a preview of the problem, Franklin’s collection is both looked to as a watershed for Dickinson studies and subsequently it also much critiqued as too restrictive. The beginnings of our literary model’s perspective is already both bibliographic and ideological – as are all models of history.

Consistency

	Not all of Coleman's categories for data quality readily fit with the criteria of literary modeling. New categories can always be theorized and implemented based on a project, but 'consistency' and 'integrity' adapt easily to the practice.

	What might 'consistency' look like in a project considering Emily Dickinson's works? Let's consider a regular problem in modeling literature: the composition of corpora. What's in and what's out? Lists of works of a corpus can vary from source to source let alone the actual textual contents of each of those works. One simple first stab for Dickinson would be to compare a less certain, but communal record of her oeuvre with that of a master list. Why would we want to do so? The stability and accountability of digital data sets for literary research as of this writing still has much room for growth. For instance, the notion of a data quality check itself also can consist of checking the accuracy of metadata. The status of Dickinson metadata is a common scenario. It is scattered around the internet or available in book form if one is able to access and transcribe it. Wikipedia's own list is widely available and somewhat comprehensive. (It doesn't delve into the kinds of detailed and critical distinctions made by Martha Nell Smith, for example.) It's also easily comparable to R.W. Franklin's and Thomas Johnson's compendiums of Dickinson's manuscripts – comprehensive, if criticized 'master' source lists.

	A phenomenon emerges from a test that compares work titles between the lists. The metric itself, something I will call 'title consistency', is simply a minimum of matching sequential words between titles across two lists. The closest matching titles are determined and the percentage of discrepancy calculated. After shifting the minimum matching word threshold by ten percent intervals (10% of sequential words must match, 20% must match, etc.), the number of matches converges to a spot like a limit as it approaches zero. Likely a generalizable phenomenon, the curve produced has its own signature rate of match success for the corpus and metadata. Decreasing the threshold below ten percent does not move the needle towards a higher match. With the exception of missing manuscript IDs (Franklin IDs not present in the Wikipedia list), the behavior of the remaining three data quality measurements under the general metric of 'title consistency' (matching titles, mismatching titles, and percentage of titles discrepant from the master source list) can be described as almost asymptotic.

	The Franklin version of Dickinson's corpus is generally regarded as consisting of 1,789 manuscripts (depending on one's definition of a complete Dickinson work). However the full digital collection available for download from the Emily Dickinson Archive consists of 4,825 transcriptions across several publications of poems and letters, 2,459 of which are attributed to Franklin's variorum, 'The Poems of Emily Dickinson'. Decreasing from 100% required word match we see the number of possible poem title matches increase, until they level off at 10% match at 1,782 poems – about a 1% discrepancy between the measurement and Franklin's master list. Ironically, or perhaps expectedly, relaxing the requirements allows more titles to be included as works identifiable and thus accountable to a bibliographic source list.

	This is an early juncture in the modeling process where dataset curation can propagate errored models. Knowing the amount of discrepancy at least tempers and makes visible that potential error. For the case of the Dickinson corpus, it's small enough to understand the discrepancy. The missing seven poems turn out to be a case of Johnson (Franklin's compendium predecessor) and the Wikipedia authors choosing alternate titles for poems or choosing titles based on alternate manuscripts of the same poem. Their choices are interpretations of which of Dickinson's revisions seem like her final ones, and none the less valid ones. Alternates they may be, but Wikipedia's curated poem list displays a quality of research familiar to literary scholars: the ultimate subjectivity that is the composition of a data set. Johnson's prior choices and Wikipedia's list also provide a valuable service for the research wary of diving into tomes like Franklin and Johnson: a viable, alternative authority for making choices between variant poems. Depending on the level of one's work, these may not be decisions one wants to make.

	Here I have chosen not to include a consistency measurement of comparing each work word for word as this is often involves an unreasonable amount of work. Even for the relatively small Dickinson corpus, transcribing each work from Franklin's or Johnson's or Smith's to compare to digital editions is not only an arduous endeavor, but also potentially redundant given the knowledge that expert scholars and graduate students were involved in transcribing Dickinson's manuscripts for the Emily Dickinson Archive. In this case, the title consistency metric acts as a safeguard for that unknown potential for modeling error due to faulty data. The measurements themselves have allowed me to focus in on the seven mismatches and three to four missing manuscript IDs to make my own decisions about tidying up the corpus I want to model.

	Just as Coleman points out, the work of data quality is not a single use standard, but rather a cyclical practice that works toward a satisfactory refinement. One inherently expects the work of data refinement to behave in this way, but seeing it in quantitative form shows where hard work pays off versus where it significantly diminishes into pointlessness. Not only that, but seeing the full range of consistency also allows a researcher to pick a point where they feel that they are able to balance consistency, the metric in question, with accuracy. A rewarding outcome of this practice, especially in terms of humanities data sets, is that the qualitative judgment of subject matter experts can be laid bare in a way that is accountable to modeling - the realm of quantification – while demonstrating the point of strong, theoretically grounded humanities interventions. And this series of decisions that eventually produces a new representation of cultural objects is one that is most fully developed by the practice of doing so. In other words, despite its quantitatively-informed foundations, computational modeling of literature involves developing a techne, a craft, or an art.

	In that light, let’s take a look at a few plausible data quality metrics under Coleman’s categories that can help us build a foundation for a model of Dickinson’s poems, namely ‘consistency’ and ‘integrity.’ (Code notes: See dickinson_consistency.py and dickinson_integrity.py) The kinds of data quality metrics that emerge for humanities studies are either self-enclosed or look to a master or source list for verification. In the case of consistency for the Emily Dickinson Archive corpus, we can think of a consistency in at least three ways, the simple counts of documents across collections or the matching of titles and manuscript IDs across collections. Integrity is a far more enclosed metric, looking for consistency in metadata across files and the accountability of the words of the corpus to a dictionary of almost 500K English words.

Notes and the Work Ahead 
To do:
 (- Find the 1 duplicate poem in the 1790-work filtered corpus)
- Discuss k-means (what it’s doing), the idea of cluster silhouette (and average silhouette), display silhouette graphs (See https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)
- Look at top N most frequent words and determine cut off based on legomena
- Consider legomena as another criterion for identifying uniqueness
- Use top N most frequent words to k-means cluster and scikit SelectFromModel() functionality to determine graphable scatter plot (for cluster graph)
    - Using top N should aid silhouette analysis
- Graph silheoutte averages by doing continued iteration on clusters increasing by one example [do X k-means computations varying from 2 clusters to 200 clusters] and then graphing to understand the shape of averages (where it levels off/stabilizes, for instance
- Some examination of the poems in the poem clusters themselves to bring out qualities through close reading of them
- This work is to be understood as getting a feel for the dataset in order to move to the next point:
- Use this basic information to choose a few simple notions from Coleman’s data quality framework(s)
- Include citations from works on Exploratory Data Analysis
	- Initial Data Analysis (IDA, a subset of EDA)
			- Common in statistics
			- Measures: https://en.wikipedia.org/wiki/Cronbach%27s_alpha
- Discussion of Dickinson dataset(s) origins
- Discussion of Dickinson dataset(s) themselves
- Postulating data quality metrics for the Dickinson dataset(s)
- Data quality assessment of the Dickinson dataset(s) based on that framework

Second Part

- Demonstrate whether or not based on the cluster analysis and scholarly research if there are any suggested poem authorship timelines
- Findings from the models
- MetaFindings that utilize the data quality metrics to better inform those original findings

Project Gutenberg section
	⁃	The problem of provenance 
	⁃	Specifically of the story of Gutenberg and how the editorial community works. Examples of trying to trace provenance
	⁃	What is a text? Does a few typos/ocr errors erase its thingness? 
	⁃	A close reading scholar would likely say no, but if we are to decompose a text into its word components in order to say something about them, then degree of exactitude does matter
	⁃	If we are to make a model of those components for further interpretation and use then that effects and implications of that degree of exactitude stretch further into the future/distance/present.
	⁃	If the criterion for making interpretations becomes this form of measurement then (reasonable and appropriate levels of) determining exactitude become incumbent and ethical acts
	⁃	This is not necessarily ‘solving’ DH, but it is an attempt to add some structure to its computational beliefs, interpretations, and conclusions.

Talking about the work on the Dickinson corpus curation
	•	integrity
	⁃	integrity by match of digital files to bibliography (manuscript book order)
	⁃	integrity by year (this might be for model quality) - matching estimated year by Franklin/Johnson to lexical year model estimation
	⁃	integrity by match of manuscript id number to year distribution list
	•	consistency
	⁃	title matching
	⁃	Known manuscripts missing from digital corpus
	⁃	known_non_franklin_ids = ["A13-6","A13-8", "excluded","A13-2"] (see dickinson_consistency.py)
	⁃	text matching
	⁃	corpus-manuscript composition

Consequences of Data Quality Curation on Modeling

Work done on November 25
- Built data quality ‘integrity’ metric (dickinson_integrity.py)
- Built generalized DickinsonPoem collection object DickinsonCollection (dickinson_collection.py)
- Some more on the consistency metric (dickinson_consistency.py)

Next steps:
- Use initial metrics to discuss Coleman’s data quality framework in terms of digital humanities projects - e.g. writing
- Call 112 Patchogue Nissan 2000 about reward, talk with manager
- Pentatonix album
- Flu shot
- Figure out which of the manuscript IDs and revisit (1742/1789 matches between digital corpus and the transcribed Franklin manuscript list)

*** Where you stop in a data quality assessment depends on how much ground-data you have to test against. It may be the case that there are no further steps after assessment – if one can make one at all. And if one can’t, then the best one can do is to offer one’s informed sense of the partiality, cumulative discrepancies or errors of the dataset.
</Text>
        </Document>
        <Document ID="CB9CF51E-D935-46AF-9B59-7E39F58F45C9">
            <Title>Draft/Ch1DQ/Gone Trhough/Prose/ What are the problems</Title>
            <Text>What are the problems with data in literary studies?

Inaccurate transcription
Overrepresented portions of a dataset (and thus underrepresented portions of it)
Overrepresented datasets
Hapax legomena ignored by CDF (statistics)
Inconsistent transcription across editions/versions of the same/similar text
Different editions - which to consider
Access to datasets - public/private
The size of a individual set/the amount of datasets available - is this enough to be modeled on its own
What is the best version of an individual text/the most suitable version of a text for modeling?
The challenge of difference in kind when it comes to evaluation: quantitative versus qualitative (and the mindsets that make this distinction as translated into modeling/anlaysis/interpretation/criticism)

</Text>
        </Document>
        <Document ID="F13BBC08-49D3-486E-9EFE-2B7112584B37">
            <Title>What I have been doing 2025</Title>
        </Document>
        <Document ID="9420870D-A95B-433F-89BA-0DAB5119FC32">
            <Title>Text mining DH criticism</Title>
            <Text>Ideas
Text mining DH criticism with the aim of showing rhetorical claims about philosophical trends
	◆	e.g. looking for window/visual metaphors for data/modeling/visualization
</Text>
        </Document>
        <Document ID="7D9B7531-DBA7-484C-B497-B8457493CF03">
            <Title>May 12, 2022</Title>
            <Text>May 12, 2022

Ideas

	▪	Taking a survey of (via DHQ and a few other major DH journals) on how Project Gutenberg is referred to and using it for discussion of data quality of access

Done

	1.	Reading Shillingsburg article, “How Literary Works Exist: Convenient Scholarly Editions” [CSE] on DHQ
	⁃	Shillingsburg refers to Project Gutenberg (PG) disparagingly
	⁃	He actually refers to an “ideal edition” - likely a foil for my point about how close is “good enough” and access data quality
	2.	Searching through DHQ for the phrase “Project Gutenberg” for understanding how PG is referred to
	⁃	Results: 6 pages of articles, so far many use PG seriously and even reference its founding and founding philosophy
	⁃	Some interesting ones so far:
	a.	Porsdam - “Digital Humanities: On Finding the Proper Balance between Qualitative and Quantitative Ways of Doing Research in the Humanities”
	b.	Chesley - “The In/Visible, In/Audible Labor of Digitizing the Public Domain”
	c.	Hoover - “The End of the Irrelevant Text: Electronic Texts, Linguistics, and Literary Theory”
	3.	Setting up Zotero folder for new works read

	Notes

Shillingsburg - both articles, “How Literary Works Exist”, CSE and IRI

	1.	The article (along with its companion piece, “How Literary Works Exist: Implied, Represented, and Interpreted“ [IRI]) “examine the nature of the ‘print things’ that textual scholarship tries to identify and analyse, in order to see how best to represent them in electronic scholarly editions and archives.” (both articles use this phrase)
	2.	IRI: “The results of existing funding policies have been projects that invariably provide only local solutions to local problems.”
	⁃	S. here believes that construction of digital materials (at this point in time) after a research project have mostly amounted to last minute ventures with little thought to what is being constructed. The deadline overrides concerns about quality (thus ‘local’ solutions to ‘local’ problems). They “hire a techie”. S. says this issue is not correctly understood as an “interface” problem
	3.	IRI: Different ideals of editing - moving from replicating or even recovering (fill in the blank) authenticity to social awareness of material conditions to the notion of the possibility of recovering/reconstructing an “urtexts” or, as he puts it: “well-wrought urns”
	⁃	Once these shifts of editorial ideology moved in this direction,
	⁃	“Gone are the romantic genius of the author, the noble hand-maidenly efforts of the editor to restore, construct, or reconstruct ideal texts, and with them the hopes for urtexts or well-wrought urns. Presumably, also gone is the subjectivity that bolstered genius, construction, and ideal texts.”
	4.	IRI: S. notes previous book where his thoughts were not yet complete on the matter he discusses in these two articles: From Gutenberg to Google (2006)
	5.	IRI: KEY STATEMENT for S.’s article and for AoLM:
	⁃	“The present paper does not trace the history of these shifts but rather takes them for granted as having taken place. It does not, however, take for granted that the shifts have been beneficial, successful, or without loss. Obviously, they have been sufficiently beneficial to be successful enough to have affected the profession of letters. But enthusiasms often come at a price and can blind one, at least at first, to the losses involved.
	⁃	So, in looking at how literary works exist, I am eager to see if these shifts have entailed losses and, in particular, if a re-examination of how literary works exist might help us to see how they should be or could be edited for electronic representation.”
	6.	How do Literary Works Exist?
	⁃	On reproduction/transcription: “if the goal of editing and of critical analysis is to examine material documents as socio-cultural and historical evidence, then the original documents themselves or high-resolution digital images of them are basic beginnings. That is so because any transcript or new edition would be a witness to the social conditions of its own production, rather than of the production of the original it was trying to (re)-present” (169)
	⁃	
	7.	His previous book: From Gutenberg to Google: Electronic Representations of Literary Texts (2006)
	⁃	“which is about the nature of literary texts and about electronic editing”
	8.	“the idea of a snapshot version of the work as a whole is at best a metaphor on the verge of collapse” (171)
	⁃	Once this is acknowledged we see that a digital edition, mastered or not by competent scholars so too becomes a snapshot and filtered through the various machinations that academic research has deemed worthy of inclusion, excision, reformatting, or even the act of leaving something untouched, commented on, etc.
	⁃	If as Shillingsburg restates here that McCarty notes that the process of reading is a machine inflected by the “sense units” of the text and our memories of them, then so to do the aspects of a scholarly edition become the machination itself.
	⁃	The thingness of the text being read, studied, modeled is a fragile metaphor. So is the idea of an accurate representation of it. This can be argued because though qualitative in its premise, the idea of accuracy denotes a boundary between “accurate” and “not accurate.” And if a text processed by amateurs into their own archive or edition is something like 95% accurate according to the standards set out by the mastered scholarly edition then surely that could not be described as something of ‘poor quality.’ (Provenance consideration as ‘quality’ not withstanding.)
	9.	On texts as fixed material objects: “even though we may not believe in ideal archetypal or intended texts, the proposition remains that literary works do not exist as fixed material objects; they are only implied variously by surviving materials” (173)
	10.	And what’s our motive? “[W]e must not forget that one reason we wish to develop some sophisticated sensitivity to the condition of literary works in their multiple selves, their implied forms, and their chronologies of dynamic existence is that we wish to create an electronic equivalence which enables and does not ignore or deny these sensitivities…an electronic archive with digital images of documents and accurate, searchable transcriptions is a good start; but it isn’t good enough.” (173)
	11.	The entirety of section 5 refuting the notion that a text is a text is a text - a monad. And references sources McKenzie and McGann.
	12.	Shillingsburg mentions “Timbuktu scholars” being foreseen able to access the electronic archives of Western institutions: “Timbuktu scholars do not have to go to great academic institutions such as Cambridge, Oxford, UCLA or TAMU — or if they still have to, we feel that in time they will not have to.”
	⁃	Of course, we know that capital (e.g. the ‘paywall’ and copyright) has dimmed this prospect making those vaunted archives of poor quality in terms of accessibility.
	13.	Potential awaiting for digital texts. But what about works?
	⁃	“So, if we think of the electronic representation of texts and documents in these terms, we might with justice feel that the electronic age has reached a milestone: that the technology and the methodology for representing texts and documents awaits the will and direction of scholars to fulfil the promise of a new but present age.” (175)
	14.	“the complexity of works exceeds the complexity of texts and requires more than imitation on a computer’s screen of hand written and machine printed texts” (176)
	⁃	Works vs texts
	15.	“Nevertheless, it is worth remembering the potential for interpretive difference deriving, on the one hand, from the bibliographical and lexical particularities of the copy being read and, on the other hand, from the contexts of origination that exercised an operant force on the construction of the copies of the work at important events in the work’s life, and on, if one but had one, the third hand, from the critical and interpretive communities influencing the present reader.” (178)
	⁃	*** These three sites of interpretation of a work present an interesting - qualitative/humanities-originated rubrik for interacting with measures of data quality
	⁃	For instance, how do (specific) differences of data quality (metrics) affect these three sites of interpretation?
	16.	“No edition of a work fully represents it. An archive of representations of a work only begins scholarship relative to the work by providing the foundation for full engagement with the work” (180)
	⁃	Key statement on my own work with Huckleberry Finn editions
	17.	Shillingsburg believes electronic archives cannot be completed by one team; it is a multi-team, multi-generational work
	⁃	Argument for collaboration (DataLad? Etc. etc.)
	⁃	“The goals of electronic archives / editions for any work of literature are too great for any one editor or one small team of editors to fulfil. But we have yet to see electronic editorial projects as partial contributions to work sites where our best collaborators may have only just now been born. Can we do our work in such a way that textual critics, instead of ignoring existing electronic editions and looking for ways to rebuild from the ground up, can build on what is already done, or could attach their own scholarly work to the existing site as developing contributions to knowledge? If that is to happen how must we undertake our work?”
	18.	What must be done
	⁃	“In order for scholarly editors in the electronic medium to make a difference in the way students and critics read and interpret texts they must do the follow- ing four things:
	⁃	1. Make access to texts in scholarly editions as convenient and inexpen- sive as paperbacks, but not oversimplified.
	⁃	2. Construct editions so that readers can not only see but touch, manipu- late, personalize, and alter their copies in whatever way they wish.
	⁃	3. Construct editions so that other scholars can participate in the fur- ther construction of the work site by adding new materials, new links, new comments, new information, new texts, new tagging, new views.”
	⁃	4. Protect the integrity of the original textual foundation and the dis- creteness of each contributor’s offering whilst accomplishing points 2 and 3. (181-2)
	⁃	“until funding agencies supporting editions and archives add the third wheel to their funding scheme — development of a Collaborative Literary Research Electronic Environment — new electronic editions and archives will con- tinue to imitate print by continuing to create closed, finished or abandoned, look-but-don’t-touch products.” (182)


Thought on accessibility:

A data set is low quality if its accessibility is low. And why would this be important? If accessibility is low, reproducibility is low.


Shillingsburg not only presents a case for how electronic editions should be constructed, but also a metaphorical humanities framework on top of which we can lay quantitative and qualitative measures of literary data quality. The reason for this is that in considering the differences between a text and a work – affecting how it it is implied and represented and who it is implied and represented to – Shillingsburg allows points of connection for considering the quality of textual data and textual data collections (archives). What is the degree of success in endeavoring to create a more rich and accurate electronic edition of a work. (And the term “work” here is distinguished from the term “text” in an important way. The former is implied by instances of the latter. A “text” is material and contingent.
</Text>
        </Document>
        <Document ID="41382D21-DDB3-47D6-9DBD-A43BDEB7D60C">
            <Title>Millicent Todd Bingham publications</Title>
            <Text>Dickinson-related publications by Millicent Todd Bingham

	1.	Bolts of Melody: New Poems by Emily Dickinson (w/ Mabel Loomis Todd, 1945)
	2.	Ancestors’ Brocades: The Literary Debut of Emily Dickinson, 1945)
	3.	Emily Dickinson: A Revelation (1954)
	4.	Emily Dickinson’s Home: Letters of Edward Dickinson and His Family (1955)
</Text>
        </Document>
        <Document ID="A863C1AD-9A7F-46F7-BF69-27DD841F0EB8">
            <Title>March 20, 2025</Title>
            <Text>Why do we consider data quality for literature?

A few opening questions: What is it we are modeling? Is it literature? Or is it the probability of literature?

In other words, what we are modeling and why is worth considering. The underlying epistemology of a researcher and their project matters because that epistemology will inevitably guide our future decisions in the modeling process, if not now at the beginning when we are looking most directly at the data we are modeling then later on during the modeling of it, during assessments of that model, and then during interpretations/analyses brought forth from it. The argument here is that we should be explicit with our epistemological premises when it comes to dealing with data for computational modeling. What we consider to be “valid” literature matters a great deal because we are temporarily transforming text loaded with implicit semantic information into a much more flat form for the sake of modeling it as language, as structured data.
 Let’s say we have dirty data, a novel scanned with OCR errors, for instance. Why would we choose to proceed with modeling it in its present condition? With or without cleanup? The answer usually follows that statistically the model is no worse for wear with some dirty data, but the degree of that dirtiness is not necessarily so clear. What would be the degree of dirtiness that data is allowed to be such that a model of it would be considered a success versus a failure? And if that question is not answerable up front then why not ensure the data is as high quality as possible? So one common activity we could engage in is attempting to clean up such OCR errors, but what if those errors exceed visibility and what if there are other errors within the data that we cannot immediately know or address? Perhaps then let’s have a well theorized and implemented test/set of tests that can measure the exact kind(s) of data quality we want for a particular dataset.
Coleman’s data quality assessment framework (DQAF) provides a suitable theoretical structure upon which humanists can hang our own measurements. The kinds of data involved in other contexts, scientific or business worlds for example, may not always exists within a humanities/arts context but there are many categories and kinds of data which Coleman’s writings address that do overlap with humanities data. In this case, this work is dedicated to applying such measurements of quality to digitized literature.

Notes:

Developing validity dq code
Downloaded COHA lexicon to use with validity check - for 19th century American English
Reassess integrity/consistency code</Text>
        </Document>
        <Document ID="12B9DF36-0BCE-423A-A1E0-EE499C694A42">
            <Title>April 16, 2025</Title>
            <Text>Table for project progress

Columns: Project (or subproject/task), Status, Next Deadline

Upenn list of publications

Citing : cnanonical, widely cited, articles from journal you submitting to
Following same organization of target journal
Intro and conclusion last!

Different chapters - instead of data and model quality, introductory with natural flow and more integrated?

This summer Mel publication workshop - from paper to journal article - you can participate!

NOTE: MTPO needs to be read correctly in via compute_and_evaluate_ur_edition</Text>
        </Document>
        <Document ID="50F5ADC1-A14B-4C2F-B3F2-2707E7EEE54E">
            <Title>April 20,2023</Title>
            <Text>Old Outline

Chapter 1 Planning

Part 1
	•	Intro on reasoning for data quality assessments for literary studies
	•	Review of data quality ideas from several fields (information science, computer science, …)
	⁃	Discussion of concepts as they exist in those fields nad how they are contingent on context
	•	Introduction to ideas on how data quality (DQ) can be used for computational literary studies
	⁃	Why DQ is important for that work
	•	First demonstration of DQ for literary studies using the metric of word frequency as it changes within a text/whole work
	⁃	Moby-Dick from “Project Gutenberg” (PG)
	•	Second demonstration of word frequency with different data cleaning heuristics (tokenization, stop word, stemming, etc.) within the same volume
	⁃	Moby-Dick, again – also includes intra-text change of metric (text time)
	•	Third demo of word frequency DQ, this time across editions of Moby-Dick from PG
 
New Outline</Text>
        </Document>
        <Document ID="D93ADE3F-2A64-4CDF-B502-4205ED5BA6C6">
            <Title>Phone notes on data quality</Title>
            <Text>What is the slope of the line over time of the corpora word vector over text time as it develops

How does document cleaning, word cleaning, and stop word lists alter the slope of that line?

What are the repercussions down the line?

Can we measure the information loss of each of those processes?

Consider different apis/methods that perform the same function and notice if there is a difference across methods

Take notes on noticings as this exploration  and measurement is done

Start with a simple document from Project Gutenberg.

Figure out its sections and begin the process</Text>
        </Document>
        <Document ID="58A506D6-3579-4491-9926-1E8844442B30">
            <Title>Dickinson Gutenberg Dataset Notes</Title>
            <Text>Dataset Notes

Metadata

Accessed: January 28, 2021
Author page: https://www.gutenberg.org/ebooks/author/996

List with # times downloaded

	1.	Poems by Emily Dickinson, Three Series, Complete 2040 downloads https://www.gutenberg.org/ebooks/12242
	2.	Poems by Emily Dickinson, Series One
	⁃	265 downloads
	⁃	https://www.gutenberg.org/ebooks/2678
	3.	Poems by Emily Dickinson, Series Two
	⁃	88 downloads
	⁃	https://www.gutenberg.org/ebooks/2679
	4.	Poems by Emily Dickinson, Third Series
	⁃	51 downloads
	⁃	https://www.gutenberg.org/ebooks/12241
</Text>
        </Document>
        <Document ID="E5A5551B-0DDC-46E1-8861-DC0E64C7B08D">
            <Title>January 17, 2025</Title>
            <Text>Why data quality matters for humanists?

Computer scientists (etc.) will be creating models of what we refer to as literature (a.k.a. Humanities data) so that chatbots/LLMs can summarize and interpret that for the public in search of information about it. What is the role of humanities in this future? There are several, but one of them will be to work on defining and sustaining high quality versions of datasets for such models. While it’s true that statistically sometimes errata in a transcription or errata from a particular edition might not matter, as humanists know small changes often make differences in things like summaries and interpretations.
Furthermore, if humanities scholars wish to produce research based on computational models of their textual subjects they too will want to have a well-defined quality standards for those subjects so that their own, more traditional research findings have sound bases</Text>
        </Document>
        <Document ID="35574BC4-E354-47B7-BEBB-EF326CFD7C83">
            <Title>April 22, 2025</Title>
            <Text>Todo
Fixing output for huckfinn_dataquality.py

Notes

A stage in data quality analysis: The precursory examination, rough modification data work before data quality assessment can be started

The idea of comparing the dynamics of certain measurements found in subsets of a text. Which part of the text is contributing the most to which force component (or partial of the scalar value) of the data quality metric for this overall dataset (text).

Ted committee alternate suggestions: Travis Wagner (cultural heritage), Emily Maemra, Ryan Cordell

</Text>
        </Document>
        <Document ID="90A512AB-4665-49B8-A5DA-5934B254A11E">
            <Title>Huckleberry Finn text editions metadata</Title>
            <Text>1885

American edition
adventureshuckle00twaiiala
Title: Adventures of Huckleberry Finn (Tom Sawyer's comrade)...
Contributor: University of California Libraries
Publisher: New York : C. L. Webster
Publication Date: 1885

British edition
adventuresofhuc00twai
Title: The Adventures of Huckleberry Finn (Tom Sawyer's Comrade)
Contributor: The Urban School of San Francisco
Publisher: Picadilly Books, Ltd.
Publication Date: 1885

Canadian edition
cihm_50160
Title: Adventures of Huckleberry Finn (Tom Sawyer's comrade) [microform] : scene; the Mississippi valley, time; forty to fifty years ago by Twain, Mark, 1835-1910
Contributor: Canadiana.org, University of Alberta Libraries
Publisher: Montreal : Dawson
Publication Date: 1885

1904

American edition
adventuresofhuck1904twai
Title: The adventures of Huckleberry Finn (Tom Sawyer's comrade)
Contributor: Harold B. Lee Library, Brigham Young University
Publisher: New York : Harper
Publication Date: 1904

1909

American edition
completeworksofm11twai
Title: The adventures of Huckleberry Finn
Contributor: Internet Archive, Alibris
Publisher: New York, Harper &amp; Bros
Publication Date: 1909

1910

British edition
dli.ernet.470159
Title: The Adventures Of Huckleberry Finn
Contributor: S.p College, Srinagar, Digital Library of India
Publisher: Chatto and Windus, London
Publication Date: 1910

1912

American edition
adventuresofhuck00twai_9
Title: The adventures of Huckleberry Finn : (Tom Sawyer's comrade)
Contributor: University Library, University of North Carolina at Chapel Hill
Publisher: New York : P.F. Collier
Publication Date: 1912

American edition
adventuresofhuck00twaiuoft
Title: The Adventures of Huckleberry Finn : (Tom Sawyer's comrade)
Contributor: Kelly - University of Toronto
Publisher: New York : Harper
Publication Date: 1912

1926

British edition
dli.ernet.14052
Title: The Adventures Of Huckleberry Finn
Contributor: Central Library, Bits-Pilani, Digital Library of India
Publisher: George G. Harrap and Company Ltd. London
Publication Date: 1926

</Text>
        </Document>
        <Document ID="1BDA0327-605B-4BB4-9146-3C1F463A18D6">
            <Title>Chapter 1. Data Quality - Outline</Title>
            <Text>Chapter 1. Data Quality

	1.	Introduction - Siting a site for modeling (the fundamental task of research)
	⁃	What is modeling? Why art (kunst)?
	⁃	Representations of the world
	⁃	Pollan’s placing a chair down
	⁃	Pollan’s looking at the ground
	2.	Ideas about data quality in the humanities
	3.	Ideas about data quality in the sciences
	4.	Hathi Trust data set (HTDS) and 19th century American Literature
	5.	Bidirectional quality
	6.	Data quality work thus far in the digital humanities
	7.	Data quality frameworks from information science
	8.	Application of Sebastian Coleman’s framework to HTDS
	⁃	Constructed to answering an area of questions about certain characteristics of HTDS
	9.	Metrics for the foundation of data quality for the humanities
	⁃	What can be done with this - building a model, next steps
	⁃	A model with HTDS built upon sound quality (for the humanities)

Research Needed

	•	Modeling
	•	Space
	•	More on data quality
	•	Recent thought/work on data-focused inquiry - See Bode

Coding Needed

	•	Abstract data quality object (interface)
	•	Implementation of such an interface for a dataset
	•	Derived classes of data quality object for the dataset/pieces of dataset

</Text>
        </Document>
        <Document ID="44A9495C-B376-4DCA-AA6E-D301EB3909DD">
            <Title>Draft/Ch1DQ/Prose/Outline Fleshing</Title>
            <Text>Chapter 1 Planning
Outline

Chapter Introduction
Part 1
	1.	Intro on reasoning for data quality assessments
	2.	Review of data quality ideas from several fields (information science, computer science, …)
	⁃	Discussion of concepts as they exist in those fields ad how they are continguent on context
	3.	Introduction to ideas on how data quality can be used for computational literary studies
	⁃	Why data quality is important for that work
Part 2
	1.	First demonstration of data quality for literary studies using the metric of word frequency as it changes within a text/whole work
	⁃	Mark Twain’s The Adventures of Huckleberry Finn via “Project Gutenberg”
	2.	Demonstration of word frequency with different data cleaning heuristics (tokenization, stop word lists ,stemming, etc.) within the same volume
	⁃	Different API versions of those cleaning methods
	3.	Third demonstration of word frequency for data quality, this time across editions of The Adventures of Huckleberry Finn from “Project Gutenberg.”

Huckleberry Finn play within the novel

Part 3
	1.	Introduction to Emily Dickinson publication history and data quality problem of editions
	⁃	Consider a few Dickinson poems
	
Chapter Introduction - A New Activity

	I began this work with a struggle. Something was in the way. A typical beginning for computational text analysis work considers a literature and a question you want to ask about it. You gather your text, either in some form already digitally transcribed or do some of the arduous manual labor yourself – scanning, using varying optical character recognition suites, cleaning up poorly recognized characters and words, and maybe even preparing the subsequent digital text(s) with a particular regularization suited to your intended form of counting or modeling technique. Now you’re ready to measure and re-represent your object in some way. You run a modeling method over varying-sized portions of the text and alter parameters of the model as needed. After a few iterations and comparisons or combinations of those models, you’re ready to intrepret this new representation. Next observations might include what macro patterns you find moving their way across the work or multiple works that you weren’t able to recognize before? Perhaps they confirm or disconfirm prior hypotheses of these works. But perhaps I have just constructed a cartoon.
	This approach produces an air of something veritably wrong in the context of qualitative interpretation. Let’s step back to the beginning of this workflow, the part where text was gathered. What happens between the action of gathering and the moment one “finishes” that process? What sign tells you that you are now ready to create a model of that literature? Nothing guarantees that you are. What, for that matter, is a model for use by the humanities anyway? Once the qualities of all of these constructs come into question what once seemed such a fundamental and sound workflow, as described above, feels a lot more mushy. It’s no news that humanities scholarship is often defined by its embrace of the subjective and perspectival. And the more that scholarship and its practitioners take on the guise of more “objective” observation of those same materials, the more it comes to bear to ask, What are the guidelines and structures that inform that objectivity?
	That question brought me to ask two other metrical ones. What are the qualities of humanities data as they relate to import for the scholar? And, what are the qualities of humanities data as they relate to the soundness of a model created from that data? These two questions receive their answers from both the history of scientific and humanistic discovery. They respond to the essential quandary that fed my struggle to describe a central, if little-mentioned problem: What is data quality for the humanities? I have set out to answer this question in the following chapter. Answering it requires a new activity for computational text analysis. It requires not only the careful preparation and attention to our sources of data (as we are so often admonished to do) but also as important, the theoretization and construction of a quality framework for one’s textual data set. For not only is the transformation of a text from paper to digital fraught with chances for error, so too is the move from digital text to model, the activity that is at the heart of computational text analysis in digital humanities, and other forms of cultural analytics. That problematic has to do with a philosophical default, the thought that when we approach a digital text – barring whatever transformations and omissions have occurred to bring into into digital form – that we are dealing with it in situ. 
	A text is in its place on your computer in memory, a digital file with a system file ID, extending from one memory block until it ends at another. That is one of its physical presences in its most basic, digital and structural forms. However, through decomposition and filtering, that same text may be transfigured. If this object is to be the foundation of all that is intended to follow then its placement and the perspective of it from within and from without must be considered. A text in its initial construction is one perspective. Subdivided, rearranged, completely reordered, it is another. And yet, it retains the building blocks of that original, and when reconstructed back into that form, those very different perspectives can be illuminating of the original. At the dawn of our new era of the popularization of digital humanities, Franco Moretti and the Literary Lab called this an “elaboration of reality” (Moretti, “Operationalizing”). In truth, this activity of reconfiguration is far more than an extension. It is the genesis of a “new” mindset about a past object. While a new conception on a text is a truism in the scope of humanities knowledge production, it turns out that it is also an integral part understanding data quality for the digital humanities – and as we’ll eventually see for model and visualization quality as well. Since the metaphors and actual substance of computational modeling are so spatial in nature, and since the history of humanistic inquiry itself has depended upon such metaphors, I have found it only suitable to look to a central, experiential metaphor to describe why perspective on data is so visceral in our attempts to understanding it. It is is the construction of and our physical relationship with a house.

Body
Part 1
	1.	Humanities data analysis has always used quantitative measurement even when it appeared in “qualitative” form. The work of comparison that analysis involves makes the use of proportionality inevitable. In truth, the difference between “qualitative” and “quantitative” analysis is not very clear, nor are the terms themselves so well-delineated. This confused basis sitting at the root of research signals the urgency of clearly understanding how our culture and its objects are described – and thus also defined in an age where information is ineffably inflected by computation. As the heuristics of data processing and information delivery scale upward, our sense of the quality of that data and the conclusions we can draw from it also fades more and more. Yet our needs for information are sometimes so pressing, be they research-, business-, or journalism-driven, that the question lingers as to the quality of the data we rely on for that information. And there are many means researchers and industry professionals have developed to assess that quality. However, the key to all of those means is the context of the data and resultant information – the context of a data source, the context of the tools used to measure and filter that data source, and the cultural context from where interpretations of those measurements and filtering occurs from. For computational text analysis, a form of humanities data analysis experienced now by people around the world via news reporting, a comprehensive means of data quality assessment has not been developed. One reason for this is that this form of data analysis, often due to its small data scale and purported lack of need for the exactitudes of science and business, has been deemed “soft” or too perspectival to be verifiable  – and thus not as worthy of attention and methodological development. This question of data quality can no longer be allowed to linger given the now-widespread consumption and influence of humanities data analysis. There are ramifications beyond the potential for inaccurate interpretations and conclusions for research that include how humanities disciplines are viewed and in turn sustained through funding and cultural support. The writing below is an attempt to provide an answer for the question of what data quality for the humanities could look like. It is an exploration where the “qualitative” interests of humanities research take precedent even if they inevitably stand on the shoulders of quantitative methods – because, of course, they always have just not so explicitly.
	2.	Among the most accessible places we can look for this answer are other fields who have undertaken a study of data quality for their own purposes. The first one we shall look to is information science.
	3.	Are metrics of data quality for literature correlative with/predictive of –
	⁃	literary qualities
Part 2
	1.	On frequency analysis

	Frequency analysis acts as a primary tool for computational analysis of literature. It’s important to be very specific about what we mean we talk about that. The notion of a “word” in a text typically gets generalized into the term, “token.” The reason for that shift is apparent once you practice this kind of analysis. It can be challenging to understand when a unique word is really being used in conjunction with another to relay a compound concept to a reader. So, for instance, if an author wrote the term “chewing gum”, a less-than-careful word-counting algorithm might look at that and record it as two words – which it is. And if we were to apply a less-than-careful part of speech tagger to the same text, it might understand “chewing” as a verb and “gum” as noun. All the meanwhile, an English speaker would interpret that correctly as a noun phrase that employs an adjective to denote a real world object: “gum” that is used for “chewing.” For an English speaker those last few sentences describe something very obvious, but for an algorithm that is built to count words, it’s not. Thus the description of terms in a text as “tokens” becomes imperative. Thus the objective of frequency analysis is to count tokens. And the counting of tokens for the sake of precision – what we also call quality – can be a very handcrafted art. 
	Literary scholars face the challenge of confronting statistical philosophy when performing this kind of analysis. Is it really necessary to attempt to make our text-turned-data as high quality as possible for subsequent modeling, analysis, and interpretation? For a techne that intends to be as precise as possible, with analyses sometimes turning on the existence of a single word, the answer to that question must be an emphatic “yes.” In fact, even if it can be proven that, statistically, a model produced from two data sets is more or less the same it remains a judgement call for the scholar if that “more or less the same” is sufficient for them to proceed. The factors of time and accuracy in research must be weighed in concert just as much as the content itself. They often dictate the efficacy of our work up to a certain degree.  How much time am I willing to spend ensuring my data is of high quality before I proceed with the modeling of it? How much quality is necessary for a particular modeling technique or for the type of analysis and interpretation I intend to provide? These questions are implicit information, but checking back in with ourselves on our current answers to them is not merely “process text.” Instead, the effort poses a subsequent philosophical challenge or question for all stripes of humanities scholars: What are my own criteria for quality when it comes to my object of research? Not much differs between leaning on the alleged soundness of a modeling technique and leaning on the theorem of central tendency. For practitioners of a field where qualitative and thus culturally-impactful understandings of its objects are the desired outcomes, both assumptions about the quantitative, methodological soundness can lead us astray. Much like a non-digital piece of literary scholarship that might turn on hapax legomena, so too do its digital counterparts often turn on small differences in measurement. Arguments of “more” and “less” are deployed far more frequently and implicitly than discussions of how and why they are deployed appear. It is up to humanities scholars to vet quantitative soundness by (im)posing our own qualitative criteria for interpretation against it.
	One good example of this is the productive line of quantitative analysis and reasoning in stylometrics that has led to thoughtful metrics such as Burrows’ Delta. &lt;Explanation of delta here&gt;
While delta has been used as a means of understanding the closeness between documents and thus likelihood of attribution for works of unclear authorship, it also can be a touchstone in this quest for producing a measure of literary data quality. Since delta 
</Text>
        </Document>
        <Document ID="193A3C81-035F-4C83-B98A-3DD961F1DFD7">
            <Title>Chapter 1 - A New Activity</Title>
            <Text>Art of Literary Modeling

Chapter 1 – A New Activity

	I began this work with a struggle. Something was in the way. A typical beginning for computational text analysis work considers a literature and a question you want to ask about it. You gather your text, either in some form already digitally transcribed or do some of the arduous manual labor yourself – scanning, using varying optical character recognition suites, cleaning up poorly recognized characters and words, and maybe even preparing the subsequent digital text(s) with a particular regularization suited to your intended form of counting or modeling technique. Now you’re ready to measure and re-represent your object in some way. You run a modeling method over varying-sized portions of the text and alter parameters of the model as needed. After a few iterations and comparisons or combinations of those models, you’re ready to intrepret this new representation. Next observations might include what macro patterns you find moving their way across the work or multiple works that you weren’t able to recognize before? Perhaps they confirm or disconfirm prior hypotheses of these works. But perhaps I have just constructed a cartoon.
	This approach produces an air of something veritably wrong in the context of qualitative interpretation. Let’s step back to the beginning of this workflow, the part where text was gathered. What happens between the action of gathering and the moment one “finishes” that process? What sign tells you that you are now ready to create a model of that literature? Nothing guarantees that you are. What, for that matter, is a model for use by the humanities anyway? Once the qualities of all of these constructs come into question what once seemed such a fundamental and sound workflow, as described above, feels a lot more mushy. It’s no news that humanities scholarship is often defined by its embrace of the subjective and perspectival. And the more that scholarship and its practitioners take on the guise of more “objective” observation of those same materials, the more it comes to bear to ask, What are the guidelines and structures that inform that objectivity?
	That question brought me to ask two other metrical ones. What are the qualities of humanities data as they relate to import for the scholar? And, what are the qualities of humanities data as they relate to the soundness of a model created from that data? These two questions receive their answers from both the history of scientific and humanistic discovery. They respond to the essential quandary that fed my struggle to describe a central, if little-mentioned problem: What is data quality for the humanities? I have set out to answer this question in the following chapter. Answering it requires a new activity for computational text analysis. It requires not only the careful preparation and attention to our sources of data (as we are so often admonished to do) but also as important, the theoretization and construction of a quality framework for one’s textual data set. For not only is the transformation of a text from paper to digital fraught with chances for error, so too is the move from digital text to model, the activity that is at the heart of computational text analysis in digital humanities, and other forms of cultural analytics. That problematic has to do with a philosophical default, the thought that when we approach a digital text – barring whatever transformations and omissions have occurred to bring into into digital form – that we are dealing with it in situ. 
	A text is in its place on your computer in memory, a digital file with a system file ID, extending from one memory block until it ends at another. That is one of its physical presences in its most basic, digital and structural forms. However, through decomposition and filtering, that same text may be transfigured. If this object is to be the foundation of all that is intended to follow then its placement and the perspective of it from within and from without must be considered. A text in its initial construction is one perspective. Subdivided, rearranged, completely reordered, it is another. And yet, it retains the building blocks of that original, and when reconstructed back into that form, those very different perspectives can be illuminating of the original. At the dawn of our new era of the popularization of digital humanities, Franco Moretti and the Literary Lab called this an “elaboration of reality” (Moretti, “Operationalizing”). In truth, this activity of reconfiguration is far more than an extension. It is the genesis of a “new” mindset about a past object. While a new conception on a text is a truism in the scope of humanities knowledge production, it turns out that it is also an integral part understanding data quality for the digital humanities – and as we’ll eventually see for model and visualization quality as well. Since the metaphors and actual substance of computational modeling are so spatial in nature, and since the history of humanistic inquiry itself has depended upon such metaphors, I have found it only suitable to look to a central, experiential metaphor to describe why perspective on data is so visceral in our attempts to understanding it. It is is the construction of and our physical relationship with a house.
	In Michael Pollan’s 199x book, A Room of My Own, 







</Text>
        </Document>
        <Document ID="D5BEADE1-BBDC-4216-9BB4-99D570340D92">
            <Title>Dickinson</Title>
        </Document>
        <Document ID="E2FF8D55-3BC1-4392-970F-9BFE12A19981">
            <Text>Traditionally, the differences between the humanities and sciences have been seen through the lens of the character of the knowledge they produce. This was put to pen by C.P. Snow in his famous “The Two Cultures” where the former appears to rely on and produce qualitative judgments while the latter relies on and produces quantitative ones. But these have increasingly been seen as false distinctions – to a point – humanistic knowledge has also increasingly been devalued in the public sphere as its cultural utility is made distant from its economic value. It is still within human judgment to distinguish between qualitative and quantitative knowledge even if the difference just seems to be the lack of fixed meanings of words being used as qualitative characterization in the mind of the beholder and across a quantifiable boundary (i.e. something that’s more in degree characteristic X than it is characteristic Y).
The point here is that qualitative judgments/the scales imputed by qualitative judgment can be reasonably applied and mixed with quantitative measurements if but because the distinction between the two seem to be more a matter of the scale of degree of assumptions being done in the measurement itself. Also, to be considered is how a subject matter expert’s expertise may lessen the degree of inaccuracy of a more plainly qualitative measurement and in fact impute a more objective consideration for the measurement.
With that in mind, we can also question the “objectivity” of quantitative measurement in that there are always a series of subjective interventions that must be made in order to make that quantitative measurement possible. Consider, for instance what it would mean to merely count words in a text, or to count unique words. Well, what is a word? Each observation made by the computing instrument must decide what a word boundary is and must confront the messiness of human language. Unsurprisingly, the researchers of well-known texts frequently produce difference word counts for them as each of their counting algorithms may have differences in them affected by their own experience and their knowledge of the language context of the text in question. Beyond that, even generalized and reused word counting algorithms still “suffer” from their own set of subjective interventions, and it would not matter if an LLM was doing the counting as their own bases for counting words are still produced by humans. Continued use of the LLM counting algorithm merely normalizes one subjective/qualitative form of counting words. So, there is no escape from qualitative assessment or seeming “inaccuracy”, but merely the degree to which you are aware of the subjectivity/biases of the measurement(s) involved.
Moving on from there to use measurements to make judgments on the data quality of a natural language text, we can see that both qualitative and quantitative measurement can be mixed as long as the character of each is noted and perhaps the degree or importance of each measurement in the overall data quality measurement is taken into consideration. For instance, it may be suitable to add weights to diminish or enhance the weight of a more subjective measurement ini the overall data quality calculation.
This action of weighting is also clearly subjective, or, perhaps better-worded, an interpretive move that frequently happens in science and statistics. Though an even weighting (i.e. within a mean calculation) can be used this activity of weighting is clearly an advantageous act within the process of calculating data quality via a customized data quality framework. There are some measurements that will simply take precedence in the mind of the text’s subject matter expert and a higher weighitng for that measurement’s value is something naturally follows from that expert’s expectation. (This, of course, could be a flawed expectation but such is the nature of making judgments.)
Let’s take a look at a study which might have benefited from a data quality assessment. &lt;study here&gt;
You can see that these calculations and assessments serve two purposes. The first is to produce a quantitative valuation of a dataset for comparison purposes – especially in a world increasingly with a heightened cultural sensibility for quantitative values. This is both a pure and impure purpose you’ll note. It seeks to draw attention to the usefulness and veracity of a dataset of natural language text, the underlying content of which is (often) far more meaningful than its reduction to a mere single number. Indeed, such a reduction is anathema to the subject matter expert and to the cultural and artistic value of the material. But none the less we do seek to convey the value of a dataset to those who seek to learn from it – both in quantitative value in existential value. Whether one sensibility is valued over the other is something we as researchers and archivists can merely promote, but it would be folly to ignore either for the concept of “utility” exists even in a qualitative or artist’s space. Such meta-judgements also betray themselves as indistinguishable when looked at closely enough. (In the contemporary context, there are clear implications towards use-value of a text as object in terms of capital – fiscal or otherwise – and while that cannot be ignored as industry absorbs large swaths of NLP datasets, utility is just as universal from the humanistic perspective in terms of social and cultural capital of a particular author, genres, or texts.)

</Text>
        </Document>
        <Document ID="4AA5AC2F-0881-47F8-BFB0-ECF072CB7FD6">
            <Title>Simsion - Data Modeling: Theory and Practice</Title>
            <Text>“Where more than one solution is possible, quality criteria are required to enable alternative designs to be evaluated and compared.  The growing body of work on data model quality demonstrates the impact of the two different characterizations of data modeling.  Approaches that reflect the descriptive characterization focus on measures of syntactic “correctness” and semantic “completeness” (e.g. Kim and March 1995).  Approaches that reflect the design characterization include relative measures instead of or as well as absolute measures, acknowledge subjectivity in assessment, and discuss the possibility of trade-offs amongst the criteria (e.g. Moody and Shanks 2003; Simsion 2005a).” 
- (Simsion, 2007, 1.3.3 - Multiple Solutions)



</Text>
        </Document>
        <Document ID="515AC365-9EF3-4CFC-8028-6FE2D60581CC">
            <Title>Prose</Title>
        </Document>
        <Document ID="404E0683-EB61-407A-BE70-4FB38D5DD880">
            <Title>Masters Thesis Dickinson Background</Title>
            <Text>Introduction – Emily Dickinson Publication History

	Over the course of Emily Dickinson’s lifetime around a dozen of her poems were published, and never under her name. Those published were altered without her consent to conform to nineteenth century editorial demands of grammar, punctuation, titling, capitalization. Whole stanzas were sometimes erased or replaced for the sake of popular, poetic aesthetics. To consider the fraught and complicated publication history of her works following her death and why she chose not to publish the thousands of pieces she had authored is to understand how careful Dickinson was when she wrote and her insistence on leading a mostly private life. For instance, one very slight alteration of her “A narrow Fellow in the Grass” in Samuel Bowles’ “Springfield Republican” in February 1866 elicited her complaint: It changed the meaning (McNeil 33-34). The editorial crime? The “Republican” added a comma and replaced a dash with a full stop. It’s not a wonder that one of her poems begins, “Publication is the auction / Of the mind of man, Poverty be justifying / For so foul a thing” (Further Poems, Bianchi/Hampson, 4) More than a century after her death, poet Ted Hughes described the predicament of a poet’s emergence as a public persona as “limiting” and even “crippling”, and not just because of a lack of privacy. “It’s like being in a close-knit family. The moment you do anything new, the whole family jumps on it, comments, teases, advises against, does everything to make you self-conscious. There’s a unanimous reaction to keep you as you were.” (Heinz and Hughes, “Paris Review”, Spring 1995). As Dickinsonian lore has it, the bulk of her thousands of manuscript pages were discovered in her bedroom bureau. What began as a slow-moving publishing project by sister-in-law and close friend, Susan, transformed into an effort by one of Dickinson’s poetic correspondents and her brother’s mistress, and then not long after into a feud between said mistress and Dickinson’s remaining family. The feud which stretched generations involved property, manuscripts rights, and even the depiction of her persona. Hughes’ metaphor of a henpecking family interested in affixing an identity upon their author-relative appears strangely appropriate in Dickinson’s case, particularly if one imagines what would have happened had she published under her name while alive. That’s not to say that Dickinson’s family and friends did not have the best of intentions in preserving and publicizing her writing, but visiting the complicated publishing history that followed her death at times depicts more about the people involved in it than the poet herself. And the editorial changes to the poems, particularly in their nascent forms in print, make the replacement of merely one of her famous dashes in the “Republican” look quaint.
	This kind of information, the detailed and rich provenance depicting a publication history is not always available for a literary scholar attempting to make sense of the various forms of a writer’s works in print. And even when that information is available, disentangling the thorniness of that history can prove difficult. One of the advertised promises of computational analysis of literature is to uncover latent information that can aid in difficult research questions, to help detect patterns, trends, and influences especially when there are gaps in the historical record. The production of mathematical models of literature – that is quantified representations of the pieces of language in sequence that compose literary works – are made to pull off a paradoxical magic trick: to reduce our knowledge temporarily in an effort to expand it. Polly Longsworth, writing on attempts by Dickinson’s publishers and subsequent biographers to reverse engineer a persona of the poet through her image and the traces of her life – letters, notes, furnishings, etc. – imparts a telling if disturbing description of the nature of that task and Dickinson’s own part in making it a difficult one. “By straining to keep all traces of what she called "personality'" out of her poems and letters, she made herself vulnerable to the
imposition of multiple personalities” (Longsworth 39). The multiple roles that Dickinson has taken on post-mortem reflects that computational magic trick. For the notion of a model of Dickinson’s poetry contains the idea that we are reassembling an Emily Dickinson and her ouevre by considering the multiple versions of her poems put to print, altered by her editors and publishers. We are reconstructing a Dickinson that is based on quantifiable evidence, but also, of course, the perspective of the modeler themselves. This is a feature for literary scholarship and not a bug: to produce a fact-based and experientially-inflected interpretation of reality. Moretti’s “elaboration” is not so new after all, merely an enhanced component of a fact-finding toolkit that extends our ability to gain new perspectives into a partially more objective realm.


“It’s certainly limiting to confine your writing to one public persona, because the moment you publish your own name you lose freedom. It’s like being in a close-knit family. The moment you do anything new, the whole family jumps on it, comments, teases, advises against, does everything to make you self-conscious. There’s a unanimous reaction to keep you as you were.

“In poetry, living as a public persona in your writing is … crippling.”

In fact, there seems to be only one case where she willingly changed, and that was at the behest of her close friend and sister-in-law, Susan. 

After her death, a feud over the publication of her manuscripts arose between the families of her brother Austin’s wife, Susan, and his mistress Mabel Loomis Todd. The rights to the manuscripts were divided. 

They were subsequently published in varying collections, at times edited in similar ways that would have likely evoked Dickinson’s reprobation, notably the initial three volume Poems series, 1891-96. 

Susan’s collection of Emily’s manuscripts was given to Harvard University in 1950, and those works to which the Todds claimed ownership to Amherst College. 

It would not be until 1955 when scholar Thomas Johnson released his Complete Poems of Emily Dickinson that much of the completed poems would all be published together. 

Johnson’s edition arranged the poems by approximate date of writing and kept Dickinson’s original punctuation, capitalization, and phrasing. 

This was followed by the 1958 publication of Dickinson’s letters, themselves containing poems that were written to friends and family. 

The majority of works existed in Susan’s collection: over 1,000 poems, 300 letters, Emily’s herbarium, and personal effects like her bible and other books from her library. 

Thus Harvard’s Houghton Library and University Press have been responsible for publishing most of the standard Dickinson collections. 

However, Amherst College maintains over 850 pieces of Dickinson’s writing including several fascicles as well as many rough drafts and fragments of paper on which Dickinson would often scribble poetic lines or ideas.

	When R.W. Franklin put out the two-volume compendium, The Manuscript Books of Emily Dickinson in 1981, it was clear that there was more to Dickinson’s original manuscripts than even Johnson’s edition had attempted to restore via his addition of a chronology for them. 

That many of her poems were found in a little, wooden box (or chest) in the form of little booklets, or fascicules as Mabel Loomis Todd first named them, was the stuff of both popular and academic Dickinson lore: separate sheets folded (not nested), stacked, and hand sewn together. 

Franklin went to the painstaking trouble of re-assembling the original manuscript ordering of the poems by examining the alignments of smudges and needle holes on the page. Franklin’s volumes included 1,148 loose manuscript and fascicle poems. 

This work has continued, the latest of which is Franklin’s 1998 The Poems of Emily Dickinson: Variorum Edition which expands beyond the poems of The Manuscript Books to include the full 1,789 known, complete Dickinson poems. 

Just as many modern collections have done, Dickinson’s editors had been arranging her poems by their apparent thematics: death, flowers, God, etc. But with the ability to assess the fascicle poems in their original order, more scholars began to question the insight that had been offered when looking to singular poems and comparing them against other, thematically-close if not chronologically-close, Dickinson poems.

	Ruth Miller’s 1968 The Poems of Emily Dickinson, was the first major work to treat the fascicles seriously as a potential structuring element for Dickinson’s poetry. Though she did not have the precise fascicle ordering, she argued that each book was arranged as a quest narrative. This was followed by Martha O’Keefe’s brief article “Primal Thought” in 1979, where she shows how Dickinson placed opposing “primal” words on opposite fascicle pages. William Shurr’s 1983 The Marriage of Emily Dickinson: A Study of the Fascicles was the next attempt to take a comprehensive, look at fascicle ordering, arguing that, when considered in fascicle order, the poems appear to address the phases of a “marriage.” Shurr’s work is the first, comprehensive post-Franklin accounting of the poems that considers Johnson’s chronology and Franklin’s fascicle ordering. In addition to the biographically based, critical narrative of Shurr’s monograph, its indices and appendices offer a concise summary of Dickinson’s project.

	The critical work that Franklin enabled can be divided between those that have attempted to treat the entirety of the fascicles or singular ones. The inherent structuralism in this criticism is woven together by evidence biographical, psychological, sociological, and by the way the fascicles themselves are examined and compared. Without going into too much detail here, I will offer a listing of both types of critical works as examples of past approaches to the fascicles in order to situate my examination of them. I will be addressing some of these in greater detail in my own fascicle analysis to follow. Those who, in varying ways, have addressed the entirety of the fascicles (in chronological order) include M.L Rosenthal and Sally Gall’s The Modern Poetic Sequence (1983), Paul Gallipeo’s The Amherst Fascicles of Emily Dickinson (1984), Martha O’Keefe’s This Edifice (1986), Robyn Bell’s Emily Dickinson’s Bookmaking (1988), Sharon Cameron’s Choosing Not Choosing (1992), Dorothy Oberhaus’ Emily Dickinson’s Fascicles: Method and Meaning (1995), and finally Eleanor Heginbotham’s Dwelling in Possibilities (2003), a monograph to which my work owes much credit. 

	Those works listed above that have attempted to examine the entire collection often relent to addressing just a few of the fascicles. Even if the fascicles did not contain some 800 poems with which to compare, the difficulty of Dickinson’s poems are already a high enough barrier. For instance, Heginbotham begins her own observations of paired poems by focusing on fascicles 8 and 21. Much like the other oeuvre-wide Dickinson criticism, Dwelling in Possibilities explicitly hopes to act as a sort of primer or guide for looking at the fascicles further. This synecdochic hope can be seen in the works that address single fascicles. Without being exhaustive, William Doreski writes on fascicle 27 in his “Exchange of Territory” (1986), John Gerlach on fascicle 39 in his “Bolts, Hounds, the Variorum”(1994), Daneen Wardrop on fascicle 16 in her “Emily Dickinson and the Gothic in Fascicle 16” (2002), and so forth. Though I will not list the contents here, the recent Dickinson’s Fascicles: A Spectrum of Possibilities (2014), edited by Heginbotham and Paul Crumbly, also features a collection of essays of both types of criticism. 

	After more than two years of work between institutions, the “Emily Dickinson Archive” at edickinsonarchive.org was launched in the fall of 2013. It contains a fully indexed and searchable database of Emily Dickinson’s poem manuscripts, the majority of which are from Harvard University’s Houghton Library and Amherst College.  These are text-searchable and encoded in TEI.  Images of each of the manuscripts including any extant variants or drafts of poems are available in high resolution for close examination. All of the previously established indexing schemes, (e.g. Johnson’s and Franklin’s) are referenced in the metadata in addition to established dates of authorship). Directly reflecting the original publishing feud between the Dickinson and the Todd descendants, work on the archive was at times contentious as previous conflicting assertions of final authority on the definitiveness, refinement, and boundaries of the collection were brought up anew. However, a deal was struck where digital copies of all materials would be shared amongst the partners. The archive also notably leaves aside poetic scraps and fragments from Dickinson’s late life that exist outside of Franklin’s archival work, as featured in Marta Werner’s online archive “Radical Scatters.” At the time of release it was argued by Werner and Michael Kelly of Amherst College (both on the archive’s advisory board) that the initial collection of materials largely reinforce the authority and predominance of Franklin’s work, which, like Thomas Johnson’s initial volumes, had been done with the support of Harvard University Press.
=========================================</Text>
        </Document>
        <Document ID="10F4354D-3A7C-4644-89D4-E78F142817EF">
            <Title>Chapter 1: Data Quality</Title>
        </Document>
        <Document ID="8C92C5C8-1E9A-4535-85C0-A4FE979B2EF0">
            <Title>December 31, 2024</Title>
            <Text>Continuation of implementation of data quality metric for dataset completeness - record counts for the body text of Huck Finn 

Experiment results for dataset completeness - metadata sufficiency for the metadata of Project Gutenberg editions of Huck Finn:

Cleaned up huckfinn_dataquality.py
Added second experiment function for dataset completeness - record counts
Backed down to Python 3.12 in order to install spaCy for sentence counts

nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence. This is another sentence.")
assert doc.has_annotation("SENT_START")
for sent in doc.sents:
    print(sent.text)

SW 2361
Arrive 2:15
At curb 2:45</Text>
        </Document>
        <Document ID="98A2B9B3-ADEE-4087-A764-F889AF95F4A2">
            <Title>May 23, 2019</Title>
            <Text>Notes
	1.	Rotunda UVA press access granted to Emily Dickinson’s correspondences for a period of 2 weeks beginning May 22, 2019
	⁃	credentials (u: T-JA-9355, p: NS9hRziZnc, expiry: Wed, 05 Jun 2019, 14:25)
	⁃	url: http://www.upress.virginia.edu/rotunda/
	2.	Thoughts
	⁃	The considered qualities of a cultural data set – like a data set composed of literary content – and metadata on that content must be composed of more than just the physical attributes and aspects of the underlying content and the particularities of its physical gathering. It must be acknowledged that the gathering of that data set was derived in a context and is perspectival. This is okay. It does not make the material “fettered” by subjective aims. It is simply acknowledging the unavoidable interventions that researchers, librarians, and curators make when they perform the activities of archiving and curating that work. This is not just okay. This is a very good thing. When humans thoughtfully intervene with materials they become an author of them in more than just an aspirational sense. They become guardians and editors. So when we are modeling, we are not just modeling the materials, we are modeling the perspectives of those individuals, enriching and extending the cultural fabric of those materials. Their window frames.
Done
	1.	Gathered all tei files for the Emily Dickinson correspondences via two webscraping scripts
	2.	Data and scripts are located in Users/PeregrinePickle/Documents/School/New York University/Dissertation/datasets/dickinson_correspondences

</Text>
        </Document>
        <Document ID="AB1D9219-5D5B-4AFF-97A2-4FCC197E480C">
            <Text>	1.	From A to Z — the surprising history of alphabetical order - Judith Flanders
	2.	The Quantifying Spirit in the 18th Century - The Broken Circle” - Gunnar Broberg
	3.	A dissertation on the science of method - Samuel Taylor Coleridge - https://archive.org/details/dissertationonsc00cole - Accessed January 4, 2022
</Text>
        </Document>
        <Document ID="847061CB-5E45-4CD9-ACE2-68E70966DDBB">
            <Title>Submission - Week of December 19, 2021 (Conflicts)</Title>
        </Document>
        <Document ID="6439AB6B-94D2-4E60-BF0C-54A22E26F537">
            <Title>Ideas</Title>
        </Document>
        <Document ID="5193D3C8-8B9F-43BC-B9BB-EE2281B9CDA1">
            <Title>March 19, 2025</Title>
            <Text>Christof Schoch - Big? Smart? Clean? Messy? Data in the Humanities

Mentions to look at Johanna Drucker (capta), Trevor Owens (treat data as an artifact, something always created by people)

</Text>
        </Document>
        <Document ID="36129BE8-D9E5-4A90-89CE-647A80EE3225">
            <Title>August 14, 2025</Title>
            <Text>What is “good enough”? Can we figure out what “good enough” means for a particular data set? For a particular kind of model? For a particular kind of analysis?

Good Enough

Section by section comparison (in this case, chapter by chapter) makes it possible to look at a text for a granular quality assessment. This can serve the purpose of understanding where and why a digital edition fails the quality standards set out by a researcher’s DQAF, aid the construction of an amalgamated edition, adjust a model produced from that digital text, and beyond. Even if a digital edition cannot be reasonably amended due to practical or philosophical constraints on editing and correction, having the ability to accurately point to a digital edition’s failings serves the scholarly goal of objectively documenting the state of that edition.</Text>
        </Document>
        <Document ID="00EB8C15-D0D7-4E30-AF3E-9DCAD5BDBD3F">
            <Title>January 18, 2025</Title>
            <Text>Finished altering ia_huckfinn_writer.py and the three incomplete editions of Huck Finn from the internet archive. Processed them into json readable for the HuckFinn dq metric reading in huckfinn_dataquality.py

</Text>
        </Document>
        <Document ID="920F0195-6D66-455B-B7EA-CEC38465B98F">
            <Title>Dickinson Data Quality Metric Prose - Consistency leading to Integrity</Title>
            <Text>Dickinson Data Quality Metrics (December 3 2019 Notes.txt)

Not all of Coleman's categories for data quality readily fit with the criteria of literary modeling. New categories can always be theorized and implemented based on a project, but 'consistency' and 'integrity' adapt easily to the practice.

What might 'consistency' look like in a project considering Emily Dickinson's works? Let's consider a regular problem in modeling literature: the composition of corpora. What's in and what's out? Lists of works of a corpus can vary from source to source let alone the actual textual contents of each of those works. One simple first stab for Dickinson would be to compare a less certain, but communal record of her oeuvre with that of a master list. Why would we want to do so? The stability and accountability of digital data sets for literary research as of this writing still has much room for growth. For instance, the notion of a data quality check itself also can consist of checking the accuracy of metadata. The status of Dickinson metadata is a common scenario. It is scattered around the internet or available in book form if one is able to access and transcribe it. Wikipedia's own list is widely available and somewhat comprehensive. (It doesn't delve into the kinds of detailed and critical distinctions made by Martha Nell Smith, for example.) It's also easily comparable to R.W. Franklin's and Thomas Johnson's compendiums of Dickinson's manuscripts – comprehensive, if criticized 'master' source lists.

A phenomenon emerges from a test that compares work titles between the lists. The metric itself, something I will call 'title consistency', is simply a minimum of matching sequential words between titles across two lists. The closest matching titles are determined and the percentage of discrepancy calculated. After shifting the minimum matching word threshold by ten percent intervals (10% of sequential words must match, 20% must match, etc.), the number of matches converges to a spot like a limit as it approaches zero. Likely a generalizable phenomenon, the curve produced has its own signature rate of match success for the corpus and metadata. Decreasing the threshold below ten percent does not move the needle towards a higher match. With the exception of missing manuscript IDs (Franklin IDs not present in the Wikipedia list), the behavior of the remaining three data quality measurements under the general metric of 'title consistency' (matching titles, mismatching titles, and percentage of titles discrepant from the master source list) can be described as almost asymptotic.

The Franklin version of Dickinson's corpus is generally regarded as consisting of 1,789 manuscripts (depending on one's definition of a complete Dickinson work). However the full digital collection available for download from the Emily Dickinson Archive consists of 4,825 transcriptions across several publications of poems and letters, 2,459 of which are attributed to Franklin's variorum, 'The Poems of Emily Dickinson'. Decreasing from 100% required word match we see the number of possible poem title matches increase, until they level off at 10% match at 1,782 poems – about a 1% discrepancy between the measurement and Franklin's master list. Ironically, or perhaps expectedly, relaxing the requirements allows more titles to be included as works identifiable and thus accountable to a bibliographic source list.

This is an early juncture in the modeling process where dataset curation can propagate errored models. Knowing the amount of discrepancy at least tempers and makes visible that potential error. For the case of the Dickinson corpus, it's small enough to understand the discrepancy. The missing seven poems turn out to be a case of Johnson (Franklin's compendium predecessor) and the Wikipedia authors choosing alternate titles for poems or choosing titles based on alternate manuscripts of the same poem. Their choices are interpretations of which of Dickinson's revisions seem like her final ones, and none the less valid ones. Alternates they may be, but Wikipedia's curated poem list displays a quality of research familiar to literary scholars: the ultimate subjectivity that is the composition of a data set. Johnson's prior choices and Wikipedia's list also provide a valuable service for the research wary of diving into tomes like Franklin and Johnson: a viable, alternative authority for making choices between variant poems. Depending on the level of one's work, these may not be decisions one wants to make.

Here I have chosen not to include a consistency measurement of comparing each work word for word as this is often involves an unreasonable amount of work. Even for the relatively small Dickinson corpus, transcribing each work from Franklin's or Johnson's or Smith's to compare to digital editions is not only an arduous endeavor, but also potentially redundant given the knowledge that expert scholars and graduate students were involved in transcribing Dickinson's manuscripts for the Emily Dickinson Archive. In this case, the title consistency metric acts as a safeguard for that unknown potential for modeling error due to faulty data. The measurements themselves have allowed me to focus in on the seven mismatches and three to four missing manuscript IDs to make my own decisions about tidying up the corpus I want to model.

Just as Coleman points out, the work of data quality is not a single use standard, but rather a cyclical practice that works toward a satisfactory refinement. One inherently expects the work of data refinement to behave in this way, but seeing it in quantitative form shows where hard work pays off versus where it significantly diminishes into pointlessness. Not only that, but seeing the full range of consistency also allows a researcher to pick a point where they feel that they are able to balance consistency, the metric in question, with accuracy. A rewarding outcome of this practice, especially in terms of humanities data sets, is that the qualitative judgment of subject matter experts can be laid bare in a way that is accountable to modeling - the realm of quantification – while demonstrating the point of strong, theoretically grounded humanities interventions. And this series of decisions that eventually produces a new representation of cultural objects is one that is most fully developed by the practice of doing so. In other words, despite its quantitatively-informed foundations, computational modeling of literature involves developing a techne, a craft, or an art.


Consistency

Measurement title consistency between communal bibliographic record (Wikipedia) and Franklin's manuscript list - a 'source' list.

Reasoning: For the sake of argument, lists of works of a corpus can vary from place to place. Comparing the Emily Dickinson archive's transcription against Franklin would be next to pointless, though it could be done. We would find match near 100% and the next step would have to be comparing the digital copy to a transcription (or worse, an OCR) of the manuscript books – a step far too laborious and unreasonable, and likely redundant, for a very minimal payoff. In this instance, the digital transcriptions are known to be made by scholars working off of the manuscripts. (See Emily Dickinson Archive about section)

Running a test by varying a sequential, minimal word-matching threshhold for title comparisons, we see a phenonemon emerge. In varying the threshold by ten percent intervals the matching converges to a spot like a limit as it approaches zero. Decrementing the threshold step by less than ten percent does not move the needle towards a higher match. With the exception of missing titles (titles that are not present in the Franklin digital corpus regardless of match percentage), the behavior of the remaining three data quality measurements under the general metric of 'consistency' can be described as almost asymptotic.

The Franklin version of Dickinson's corpus is generally regarded as consisting of 1,789 manuscripts (depending on one's definition of a complete Dickinson work). However the full digital collection available for download from the Emily Dickinson Archive consists of 4,825 transcriptions across several publications of poems and letters, 2,459 of which are attributed to Franklin's variorum, 'The Poems of Emily Dickinson'. Beginning with 100% word match we see the number of possible poem title matches increase, until they level off at 10% match at 1,782 poems – about a 1% discrepancy between the measurement and Franklin's list. Ironically, or perhaps expectedly, relaxing the requirements allows more titles to be included as works identifiable and thus accountable to a bibliographic source list. This is a point where dataset curation can introduce errored models, but knowing the amount of discrepancy at least tempers and makes visible that potential error. For the case of the Dickinson corpus, it's small enough to understand the discrepancy. The missing seven poems turn out to be a case of Johnson (Franklin's variorum predecessor) and the Wikipedia authors choosing alternate titles for poems or titles based on alternate manuscripts of the same poem. (Their choices are interpretations of which of Dickinson's revisions seem like her final ones.)

Here I have chosen not to include a consistency measurement of comparing each work word for word as this is often involves an unreasonable amount of work. Even for the relatively small Dickinson corpus, transcribing each work from Franklin's to compare to digital editions is not only an arduous endeavor, but also rather redundant given the knowledge that expert scholars and graduate students were involved in transcribing Dickinson's manuscripts for the Emily Dickinson Archive.

Just as Coleman points out, the work of data quality is not a single use standard, but rather a cyclical practice that works toward a satisfactory refinement. One inherently expects the work of data refinement to behave in this way, but seeing it in quantitative form shows where hard work pays off versus where it significantly diminishes into pointlessness. Not only that, but seeing the full range of consistency also allows a researcher to pick a point where they feel that they are able to balance consistency, the metric in question, with accuracy. A rewarding outcome of this practice, especially in terms of humanities data sets, is that the qualitative judgment of subject matter experts can be laid bare in a way that is accountable to modeling - the realm of quantification – while demonstrating the point of strong, theoretically grounded humanities interventions. And this series of decisions that eventually produces a new representation of cultural objects is one that is most fully developed by the practice of doing so. In other words, despite its quantitatively-informed foundations, computational models of literature involves developing a techne, a craft, or an art.

Next step - Using low and high ranges of the dq metrics to produce alternate versions of a chronological model of the Dickinson corpus. The end-goal here is to demonstrate the effect of discrepant and missing works on model outputs. Since a model like a word embedding model that involves contextual word information, these differences – (important) varied across dq metric values – show how a model (a Dickinson chronological model in this case) can be altered by understanding the quality of the underlying data.

====================================================================================================

Integrity</Text>
        </Document>
        <Document ID="615A81C9-65D0-4559-B006-15059F6CC026">
            <Title>April 11, 2025</Title>
            <Text>April 11, 2025

Some notes from Coleman

“While we began our project with the hope of establising repeatable processes for in-line measurements, ultimately we produced a set of measurement types that can be applied at different points in the general process of data assessment: initial assessments that establish the baseline condition of the data, the data model, and the metadata; process controls that assure the integrity of data as it moves between or within systems; in-line measurmenets taken as part of data processing; periodic measurments that test the integrity of data relationships within a database; and periodic assessmentsthat analyze the sufficiency of database content to meet the ongoing needs of the organization.”

This implies that measuring data quality for literary modeling and analysis can gtake place at several stages for a project: namely the initial in-take of data using (hopefully) generalized measurement scripts, and then continually after each round of textual clean up (if the researcher chooses to do so). In turn, measuring iteratively can convey the degrees of efficacy of each separate clean up process. Since the overall dq is meted out across several metrics this gives a more detailed look at the efficacy of those cleanup processes. This may then for future researchers indicate which process is more worth of their time. Then of course measuring dq at the final iteration, after all cleanup is done, can indicate the dq of the dataset at time of usage for modeling and for subsequent researchers who may want to reuse the now cleaned up dataset.

Spatialization and Data Quality Assessment (Voice memo from April 10)

Since data quality assessment is done over several metrics, we can easily translate these measurements into a vector space. In turn, we can compare datasets across the various dimensions of that vector space to see where the strengths and weaknesses of each lay. This, of course, can be represented visually. Additionally, weighting schemes for the individual metrics may still be used, but those weightings would have to be treated – in linear algebra terms – as a subspace for proper comparison if it were desired for datasets with different weighting schemes to be compared in a similar vector space. That translation aside the weighting of metrics remains useful and meaningful when we consider how a subject matter expert weighs the relative importance of particular qualities of a dataset. [See the Piper/Algee-Hewitt Goethe paper for unique spatial representation of vector-based text models, entitled, “The Werther Effect 1: Goethe, Objecthood, and the Handling of Knowledge”]

Transcription from yesterday’s (April 10) writing session at Black Bean Deli

Art of Literary Modeling - The Epistemological Argument

Traditionally, the differences between the humanities and sciences has been seen through the lens of the character of the knowledge they produce. This was put to pen by C.P. Snow in his famous “The Two Cultures” where the former appears to rely on and produce qualitative judgments while the latter relies on and produces quantitative ones. But these have increasingly been seen as false distinctions – to a point – humanistic knowledge has also increasingly been devalued in the public sphere as its cultural utility is made distant from its economic value. It is still within human judgment to distinguish between qualitative and quantitative knowledge even if the difference just seems to be the lack of fixed meanings of words being used as qualitative characterization in the mind of the beholder and across a quantifiable boundary (i.e. something that’s more in degree characteristic X than it is characteristic Y).
The point here is that qualitative judgments/the scales imputed by qualitative judgment can be reasonably applied and mixed with quantitative measurements if but because the distinction between the two seem to be more a matter of the scale of degree of assumptions being done in the measurement itself. Also, to be considered is how a subject matter expert’s expertise may lessen the degree of inaccuracy of a more plainly qualitative measurement and in fact impute a more objective consideration for the measurement.
With that in mind, we can also question the “objectivity” of quantitative measurement in that there are always a series of subjectie interventions that must be made in order to make that quantitative measurement possible. Consider, for instance what it woudl mean to merely count words in a text, or to count unique words. Well, what is a word? Each observation made by the computing instrument must decide what a word boundary is and must confront the messines of  human language. Unsurprisingly, the researchers of well known texts frequently produce difference word counts for them as each of their counting algorithms may have differences in them affected by their own experience and their knowledge of the language context of the text in question. Beyond that, even generalized and reused word counting algorithms still “suffer” from their own set of subjective interventions, and it would not matter if an LLM was doing the counting as their own bases for counting words are still produced by humans. Continued use of the LLM counting algorithm merely normalizes one subjective/qualitative form of counting words. So there is no escape from qualitative assessment or seeming “inaccuracy”, but merely the degree to which you are aware of the subjectivity/biases of the measurement(s) involved.
Moving on from there to use measurements to make judgments on data quality of a natural language text, we can see that both qualitative and quantitative measurement can be mixed as long as the character of each is noted and perhaps the degree or importance of each measurement in the overall data quality measurement is taken into consideration. For instance, it may be suitable to add weights to diminish or enhance the weight of a more subjective measurement ini the overall data quality calculation.
This action of weighting is also clearly subjective, or, perhaps better-worded, an interpretive move that frequently happens in science and statistics. Though an even weighting (i.e. within a mean calculation) can be used this activity of weighting is clearly an advantageous act within the process of calculating data quality via a customized data quality framework. There are some measurements that will simply take precedence in the mind of the text’s subject matter expert and a higher weighitng for that measurement’s value is something naturally follows from that expert’s expectation. (This, of course, could be a flawed expectation but such is the nature of making judgments.)
Let’s take a look at a study which might have benefited from a data quality assessment. &lt;study here&gt;
You can see that these calculations and assessments serve two purposes. The first is to produce a quantitative valuation of a dataset for comparison purposes – especially in a world increasingly with a heightened cultural sensibility for quantitative values. This is both a pure and impure purpose you’ll note. It seeks to draw attention to the usefulness and veracity of a dataset of natural language text, the underlying content of which is (often) far more meaningful than its reduction to a mere single number. Indeed, such a reduction is anathema to the subject matter expert and to the cultural and artistic value of the material. But none the less we do seek to convey the value of a dataset to those who seek to learn from it – both in quantitative value in existential value. Whether one sensibility is valued over the other is something we as researchers and archivists can merely promote, but it would be folly to ignore either for the concept of “utility” exists even in a qualitative or artist’s space. Such meta-judgements also betray themselves as indistinguishable when looked at closely enough. (In the contemporary context, there are clear implications towards use-value of a text as object in terms of capital – fiscal or otherwise – and while that cannot be ignored as industry absorbs large swatchs of NLP datasets, utility is just as universal from the humanistic perspective in terms of social and cultural capital of a particular author, genres, or texts.)

</Text>
        </Document>
        <Document ID="3BFC4C37-2F5B-481D-A1A1-2E1D3190CE31">
            <Title>February 14, 2019</Title>
            <Text>Done
	1.	Implementing expectations into data profiling to move toward programmatic assessment
	2.	Look over notes from Coleman chapter to build in those expectations according to her framework
	3.	Criticality assessment guidelines (see criticality_assessment.txt)
	⁃	Notion that there is a separation between data type and expections - some have to do with the data, and others have to do with the modeling/questions desired to perform/answer
Denoting what a DQ assessment looks like
	1.	Date type enumeration
	2.	Expectation enumeration
	⁃	Criticality of fields assessment
Data Quality Testing Plan
	1.	Beyond DQ testing of my own data sets, get data sets from other scholars - particularly famous DH papers
	2.	Source example == Ted, Cultural Analytics, Jockers, Schmidt, Hoover - anyone big who has published their data sets</Text>
        </Document>
        <Document ID="18759E76-2F9F-4304-A2EA-EB1D4D2EC1D2">
            <Title>Meeting Notes with Tom (3/17/22) and Subsequent Prose</Title>
            <Text>	1.	Next step is to flesh out Project Gutenberg (PG) story more
	2.	Writing on the question of the scholarly edition industry (SEI) vs. the e-edition publication environment
	⁃	PG does not publish in-copyright works
	⁃	Consider the data quality (DQ) dimension of access
	⁃	An idealized text is a highly capitalized text
	⁃	Scholarly appartatus is manufactured by business
	⁃	Why would we want to put up with that instead of looking to “lower quality” e-editions
	⁃	This discussion of access can open up discussion of literary quality in terms of DQ and our relationship to data
	⁃	Think more about infrastructure of e-editions and in context of the publication of Twain/Twain’s Huckleberry Finn
	⁃	What is PG’s role in literary studies?
	⁃	Is it key for digital humanities (DH) within our ecology of literary studies?
	⁃	Let this be a chance to talk about the question of necesssity of scholarly and “authentic” editions
	⁃	The converse question of, “How authentic or authoritative or close to the original must an edition be to be considered valid and worthy to be read, studied, and computed?”
	⁃	How close is good enough?
	⁃	The apparatus of the definitive edition is ultimately commercial and part of cultural capital
	⁃	If, for example, a free edition is 99% accurate, isn’t that worthy?
	⁃	What and how do/have these e-editions exist within the ecology of literary studies?
	⁃	Is the question of, “What is literary data?” answered in part by what it is that these transcribers value?
	⁃	Why did they make the choices/selections that they made?
	⁃	Is that a new “literary”? i.e. Widger’s choices
	⁃	Is this a more decentralized version of “literary”?
	⁃	The subject of this ecology re: literary quality and the question of accessibility as part of a data quality assessment is the prose context for the discussion of quantitative DQ metrics and ultimate assessment

Literary Quality

What are the qualities that compose the idea of “literary” works?
Is it strictly prestige via publication? Review? Purchase? Reading popularity in terms of sales?
Is it what is taught?
Is it what is popularly read on the internet? In other words, if in prior times when the main means of literary capture was through libraries, bookshops, classrooms, word of mouth, etc. what does the widespreadness from internet reading/download mean in terms of “literary quality”?
Are internet reviews to be considered equivalent to reviews in notable publications if they in turn produce similar levels of influence?
Certainly, internet reviews provide capturable metrics for literariness in that light since they can display the amount of times a review has been read and the amount of times a review has been found useful, even sometimes the rating a user might give a review itself.


Discussion

On Literary Data Quality and Prestige

	One consequence of looking at data based on literature through the lens of its quality for computational modeling is that it forces us to confront values like prestige, accuracy, and authority – values that are also associated with scholarly editions. Whereas in prior scholarship, sourcing/citing an edition that is lesser known, and, in turn potentially of lower quality, when a known, “high quality” edition was available would be a puzzling choice. Here however, the prestige falls away. Accuracy within small percentage becomes less important and valuable than the degree of access to the material itself. A free edition vetted by volunteers can then be seen as more valuable for reference than one put out by a well-known publisher and set of scholars. The consequences of that devaluation are multifold and should be considered – one of them, of course, is argumentation in support of looser copyright and open access to materials at least in tandem with the more “authoritative” editions. To do otherwise puts into question the eponymous namesake of the scholarly edition? For who and for what is the goal of publication of scholarly editions? Juxtaposing that question with, “For who and for what is the goal of publication of free e-editions?” inherently surfaces the role capital plays in scholarship.
</Text>
        </Document>
        <Document ID="BEA8FBB3-8887-437F-800F-5E384A51682F">
            <Title>Summer Fellowship</Title>
        </Document>
        <Document ID="28A91101-275A-4DF9-9CB3-70C4D835DFA9">
            <Title>AOLM as literary data primer</Title>
            <Text>View the chapter/diss as a primer on talking about how to treat and understand literature as data

	The study of literature in the West has gone through varying paradigm shifts over the last century. 
Within the last twenty years, texts rapidly have become more available in digital form. In tandem, the machines we read them on have become capable of running algorithms that process an increasing amount of them for the purposes of study. These changes have altered the physical, cognitive, and cultural processes used in their interpretation. Despite a change of medium for its objects of study, this transition is yet again an instance of the once new school becoming the old, and then meeting the new. This liminal moment begets a push and pull between means of defining meaning and quality for that literature. What is literature when it becomes digital data? Does our reading of it change when it does? Where do we look for the answer to and how do we understand the old question, “What is ‘literary’?” Is it even a relevant question anymore?
It is an idea created, stratified, and embellished across socioeconomic class of the author, publisher, and reading audience. Business and various interested institutions, academic and otherwise have constructed the notion – a function of both economy and social prestige. 

	One of those paradigms in the early twentieth century that had doubt cast upon it is formalism and its various iterations. At its heart, formalism promises that meaningful structure and aesthetic can be found objectively in the writing on the page. The doubt came from the situatedness of the author’s prose and the person reading and interpreting it. These are both reasonable doubts for formalism’s hermeneutic tenets. However, the structural and aesthetic analysis that can be done often denote irrefutable qualities that do clearly exist in the writing, regardless of that situatedness. In fact, looking back at the doubt cast upon those formalist methods, we find that the doubt itself is just as culturally situated as its target.

(Produce formalist, structuralist, new criticism, etc. Example of analysis - perhaps of a Dickinson poem). 

We must therefore conclude that our analysis and interpretations of literature irrevocably denote qualities imbued by our own times and experiences, that meaningfulness itself is situated. How we choose to engage in analytical and interpretive method is a matter style but also a function of its intended outputs. With that understanding, we can incorporate seemingly alien methods like algorithms and artificial intelligence to look at the literatures that we value, and from there what can we do to determine perhaps new meaningful structural and aesthetic qualities in the paradigm of reading literature as data.

	A new layer of qualitative comprehension is produced via our treatment of literature as data. Depending on the context of study, there are numerous ways to understand our expectations of a dataset. 
Consider a business that runs web services for their clients that relies on a a daily stream of new data. While there are similarities in the relationship between the business/client and author/reader, the factor of time becomes much more important to the business. Time exists in a literary data set in multiple forms – be it publication history or in the more abstract notion of narrative time – but the action of prediction has a very distinct utility for the business’ data whereas for the literary data set it may bear a less crucial one. Still, as the past years of research in digital humanities and cultural analytics have used prediction to great effect ine exploratory data analysis, uncovering macro trends in a multitude of characteristics of literature and the people involved in its creation. One could ask what purpose would it serve to predict something that is already concrete and whole other than one to hone the action of prediction itself? In addition to uncovering macrotrends, cultural analytics at that scale serve that methodological purpose as well. Aside from newfound materials for a deceased author, there is no daily data coming in to refresh our understanding of their works. In that way of thinking, it becomes to look inward at the work itself and just what it is that depicts meaningful quality for the study of text as data. Those qualities might differ from work to work, from author to author, from researcher to researcher. There may be, however, some generally useful, a posteriori qualities to attend to that could help us bolster that understanding of “literary.”

	Let’s begin with two algorithms to see if we can produce meaningful, qualitative assessments of Emily Dickinson’s poems.

Frequency analysis
Topic analysis</Text>
        </Document>
        <Document ID="8D3BC7B2-AE86-4A56-B0A4-E1A5A3F60D65">
            <Title>Bio</Title>
            <Text>	Jonathan Armoza is an English PhD student at New York University where he works on the development and theorization of new text analysis and data visualization methods for digital humanities. His work focuses on nineteenth century American literature, most notably the poems of Emily Dickinson. His academic background includes degrees in computer science and literature from University of Maryland and University of Washington. He has worked as a software engineer, game programmer on titles such as 'Civilization' and 'Scribblenauts,' and as a technical consultant for Google where he began his digital humanities work volunteering on Google Books’ Ngram Viewer. Jonathan has been involved with the international digital humanities community over the last decade and has been invited to give talks on his research throughout the United States, Canada, and Europe. He is the recipient of the Canadian Society for Digital Humanities’ 2017 Ian Lancashire award for Outstanding Graduate Research for his introduction and application of matrix factorization modeling to literature. His visualization, "Topic Words in Context" (TWiC) is used by researchers and in classrooms. It allows users to spatially explore topic models of collections of texts in a web browser, showing how the model operates at the level of the collection and its individual texts. Its design and application in exploring Dickinson's fascicle poems were the subjects of his Master's thesis at McGill University. 
	Jonathan’s recent projects include a collaboration at University of Edinburgh called “Working with Scraps”, a interactive and browsable reimagining of Scottish poet laureate Edwin Morgan’s scrapbooks. For the past several years, he has worked as part-time neuroscience researcher and developer at SimExp Lab in Montréal, designing and implementing AI language experiments for test subjects reading in MRI scanners. His ongoing dissertation, ‘The Art of Literary Modeling,’ demonstrates new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. It borrows and adapts ideas from information science, statistics, and design to give humanities-centric understandings of quality for datasets, models, and visualizations.</Text>
        </Document>
        <Document ID="1F334A32-A877-410D-9EE5-E995879929A9">
            <Title>Notes</Title>
        </Document>
        <Document ID="6A835642-069A-49E4-AFE5-F97F2213F50B">
            <Title>April 15, 2025</Title>
            <Text>TODO:

PENDING - Finishing conversion of Huck Finn experiment output from csv to json
- Finish analyzing if earlier consistency and integrity metrics are salvagable or if they were simple merged conceptually with the new completeness and validity metrics
- Put recent prose into context of the Chapter 1 outline

Current Issues:

- One of the DNI HuckFinn’s is not computing edition chapter validity for the lexical validity metric.

============================</Text>
        </Document>
        <Document ID="2CEF044D-4177-4F1F-B71E-720B068C2C07">
            <Title>Chapter 2: Model Quality</Title>
        </Document>
        <Document ID="6EB6BE00-C0E2-4DAF-93C5-2F398CEFD0B9">
            <Text>
In Search of the Canonical Digital Edition

I want to pose a question about the practice of literary modeling by paraphrasing and extending a question posed by John Guillory in his exploration of the cultural capital of literature. What is the relationship between the formal characteristics of a piece of writing and the institutional conditions that enable its composition and select it as the perfect text for use in research and pedagogy? Guillory identifies three “matrices” involved in canon building and literary criticism: “compositional”, “generic”, and “linguistic.” If we transpose these to the tasks of computational modeling and subsequent literary criticism, what gets laid bare is the site and foundation of those tasks: the digital edition. But what is a digital edition let alone a canonical one?
So very often we are treated to familiar notions of canonicity considering a work and the nature of its representativeness - of an era, of a thematic, of an authorship, of its characters. Guillory notes that some of these senses can become conflated and confused, and often for political reasons – political meaning in reference to the norms of literary culture and as much as it relates to external social politics. Guillory hopes to instead insist on “the interrelation between representation and distribution” (vii-viii) and to tease apart the sense of representation as in “representative of a constituency” and representation as in the “relation between an image and what the image represents” (vii-viii). The reason he wants to do so is because of the “unfortunate” effect where those those fighting the “symbolic struggle” for the former sense of canonical representation overestimate its political effectiveness and distract from the latter sense, namely, the “social and institutional conditions of symbolic struggles” (vii-viii). Guillory feels this imbalance actually undercuts canonical representation because it only considers the surface - who works seem to be representing and not the reason why such representation comes to be in the first place.
How this relates to the notion of digital edition is that we are often considering precisely the former sense of representation - that of representative of a constituency - when we include digital works for modeling. But we rarely we consider the who/what/how/when/where/why of the edition itself being used other than an often vague reference to the authority of its original creators. The closest we come to this is the quantitative representation that basic modeling can provide us: counting words in such a way that gets us to more of an “exact” or “good enough” measure between a digital edition and its analog counterpart. This uncertainty of data quality however creates a silent vacuum for the act of computational literary modeling and subsequent analysis and interprertation from it. For if we are not certain about the character of the bases of our models, we cannot be certain of the character of our models, analyses, or interpretations! We have to processes to consider: digital edition formation and digital canon formation – which digital works are selected for use: reading, modeling, and analysis.
Canon formation includes the work involved in creating an edition, but it is an antecedent process and rarely viewed as transfigurative toward canonization of the work. For digital editions, this is not the case. This could be said of both analog and digital editions of a work, but given the mode and often less tangential transformations and sites a digital work visits, especially the latter. 
We also have no formalized systems for preserving this metadata - just ones piecemeal across sites of work and archive. There are however means of measuring </Text>
        </Document>
        <Document ID="AA091AC3-8F10-47AE-9E09-7C3ED675ED7B">
            <Title>May 13, 2019</Title>
            <Text>Source Notes
Sebastian Coleman on Data and Architecture

	⁃	“In architectural drawings, the house will be depicted in a set of views showing the size, shape, and details of its structure. The purpose of an architectural drawing is to enable people to see what the house would look like and to make decisions about how it will be built. The accompanying floor plan, another model of the house, contributes to the process of understanding the size and shape of the house and is also necessary input to building the house. It contains details such as room size, the number of windows and doors, and the like that will influence the construction of the house. None of these models is the house itself, but all of them depict the house. Each represents a subset of characteristics important to the purpose of the representation. The same idea applies to data models.
	⁃	“When working with data models, it is important to recognize that there is not one-and-only-one way to model any given dataset. Put this way, models present a kind of chicken-and-egg problem: Do data define models, or do models define data? The answer is both. To be understandable at all, data require context and structure. Data models provide a means of understanding this context. In doing so, they also create context. If data stakeholders find that models are understandable representations of data, then they can become a primary means of defining data.
	⁃	“For most databases, especially data warehouses, models are critical to data management. The conceptual and logical models allow data managers to know what data resides in the data asset. The physical model has a direct bearing on how data is moved within the database, as well as how it is accessed.”
	⁃	from just before “Physical Characteristics of Data”</Text>
        </Document>
        <Document ID="4F4BBC1C-CDE0-473D-884B-7F6DAD13C1B8">
            <Title>Source List from Readings</Title>
            <Text>from Schöch - Big? Smart? Clean? Messy? Data in the Humanities
	◆	Borgman: Scholarship in the Digital Age: Information, Infrastructure, and the Internet. Cambridge &amp; London: MIT Press, 2010. 
	◆	Borgman: “The Digital Future Is Now: A Call to Action for the Humanities.” Digital Humanities Quarterly 3.4, 2009, §1-82.
	◆	Floridi: Information. A Very Short Introduction. Oxford: Oxford Univ. Press, 2010, 22-25.
	◆	Mehta and Sahni (eds.): Handbook of Data Structures and Applications. Boca Raton: Chapman &amp; Hall/CRC, 2005, section “Basic Structures.”
</Text>
        </Document>
        <Document ID="A518B9B9-FF68-474A-A263-E1765DC0C758">
            <Title>Draft/Ch1DQ/Outlining/Chapter 1. Data Quality</Title>
            <Text>Chapter 1. Data Quality

	1.	Introduction - Siting a site for modeling (the fundamental task of research)
	⁃	What is modeling? Why art (kunst)?
	⁃	Representations of the world
	⁃	Pollan’s placing a chair down
	⁃	Pollan’s looking at the ground
	2.	Ideas about data quality in the humanities
	3.	Ideas about data quality in the sciences
	4.	Hathi Trust data set (HTDS) and 19th century American Literature
	5.	Bidirectional quality
	6.	Data quality work thus far in the digital humanities
	7.	Data quality frameworks from information science
	8.	Application of Sebastian Coleman’s framework to HTDS
	⁃	Constructed to answering an area of questions about certain characteristics of HTDS
	9.	Metrics for the foundation of data quality for the humanities
	⁃	What can be done with this - building a model, next steps
	⁃	A model with HTDS built upon sound quality (for the humanities)

Research Needed

	•	Modeling
	•	Space
	•	More on data quality
	•	Recent thought/work on data-focused inquiry - See Bode

Coding Needed

	•	Abstract data quality object (interface)
	•	Implementation of such an interface for a dataset
	•	Derived classes of data quality object for the dataset/pieces of dataset

</Text>
        </Document>
        <Document ID="E54A0184-4ED4-4A9E-95B2-5B0E4D3B0BDF">
            <Title>March 5, 2021</Title>
            <Text>Model quality

Two different directions/considerations for model selection:
1) Inference
2) Prediction

https://en.wikipedia.org/wiki/Model_selection

List of information criteria metrics:
https://en.wikipedia.org/wiki/Model_selection#Criteria

For example 

	•	Akaike information criterion (AIC), a measure of the goodness fit of an estimated statistical model
	•	Bayes factor
	•	Bayesian information criterion (BIC), also known as the Schwarz information criterion, a statistical criterion for model selection
	•	Bridge criterion (BC), a statistical criterion that can attain the better performance of AIC and BIC despite the appropriateness of model specification.[3]
	•	Cross-validation
	•	Deviance information criterion (DIC), another Bayesian oriented model selection criterion
	•	False discovery rate
	•	Focused information criterion (FIC), a selection criterion sorting statistical models by their effectiveness for a given focus parameter
	•	Hannan–Quinn information criterion, an alternative to the Akaike and Bayesian criteria
	•	Kashyap information criterion (KIC) is a powerful alternative to AIC and BIC, because KIC uses Fisher information matrix
	•	Likelihood-ratio test
	•	Mallows's Cp
	•	Minimum description length
	•	Minimum message length (MML)
	•	PRESS statistic, also known as the PRESS criterion
	•	Structural risk minimization
	•	Stepwise regression
	•	Watanabe–Akaike information criterion (WAIC), also called the widely applicable information criterion
	•	Extended Bayesian Information Criterion (EBIC) is an extension of ordinary Bayesian information criterion (BIC) for models with high parameter spaces.
	•	Extended Fisher Information Criterion (EFIC) is a model selection criterion for linear regression models.
</Text>
        </Document>
        <Document ID="910FA819-842F-457F-8255-050A41E87DB1">
            <Title>Knowledge Organization and Data Modeling in the Humanities - White Paper</Title>
            <Text>Knowledge Organization and Data Modeling in the Humanities - White Paper

	◆	“In computer science,  among both theorists working in the academy and those working in industry doing practical data modeling, most regard data modeling as a description of a real  and objective world (which includes the possibility of assessing the correctness of data models) while only a minority views it as a design process. However, in digital humanities there seems to be a general  understanding that a data model, like all models, is an interpretation of an object, either in real life or in  the digital realm.” (14)

The notion of posing statistics vs qualitative understanding is flawed, and so is responding in the same mode. The
problem with coming to terms with knowledge produced by statistical (or “mechanical”) insight isn’t the method, its 
parameters, or even the underlying data. It’s a mature understanding of representation and the processes of 
representing for a particular project/effort/endeavor.
</Text>
        </Document>
        <Document ID="D0E9C586-D0D4-4266-833B-62F4FA85CCE9">
            <Title>Body - Working Draft</Title>
            <Text>

	In fits and starts over the course of his life, Mark Twain attempted to write his autobiography. Toward the end, he dictated much of it to a stenographer. He also came to a peculiar agreement with his publisher and estate in which the work would not be published until 100 years after his death. He reasoned that this would grant him freedom to talk openly about contemporary people and subjects. By the time that anniversary came, the Mark Twain Project at University of California, Berkeley had been preparing the autobiography for years for its eventual publication. As a celebrated American author, it was met with commercial success. However, much to the initial confusion of readers, the entire autobiography would not be published fully until five years later, a volume meted out every two years. The final product was roughly 2,300 pages long. In effect, it was a book popularly bought but not as widely read.

	My dissertation, ‘The Art of Literary Modeling’, describes and demonstrates new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. Its first chapter answers the question, "What does it mean to read literature as data?" That chapter's discussion of "literary" data quality looks to Emily Dickinson's poetry and its complex publication history. My proposed project for the digital humanities summer graduate student fellowship is the programming, modeling experiments, and draft prose for a followup chapter on new means of qualitative and quantitative assessment of model quality for humanities researchers and application of those methods towards uncovering new insights about Twain’s sizable autobiography. The digital version of this material, in TEI form, has been supplied to me via the Mark Twain Project Online at University of California. 
	
	Approaching an unwieldy work for reading and interpretation is not a new problem for the humanities, and certainly not for the digital humanities. One might look to previous scholarship, lightly read the whole work while concentrating on a few sections, or, via computation, attempt to represent the work(s) with a model that can describe the entirety of the collection/object while giving insight on mathematically notable sections. For the computational approach there are a few significant decision points. There is the choice of models, sometimes driven by state of the field or by state of the art. And then there is the type of model analysis. How do we comprehend the model and how do we account for how well it represents the underlying textual data? These questions of model selection and estimation of quality are well-theorized in the field of statistics. The question of which model might be most appropriate to understand or discover new insights about Twain’s autobiography and works like it, is not though. 

	That first chapter of my dissertation adapts a series of data quality metrics from information science scholar and career practitioner, Laura Sebastian Coleman, that are typically employed in environments where data sets are dynamic and have external concerns – legal, technical, financial, and otherwise. Newly derived metrics from Coleman’s examples concerning the textual consistency and lexical and thematic qualities across Dickinson's multiple, posthumous publications are brought about by analyzing and modeling those publications, ultimately filling out an overall quality assessment of those digital works. When taken into account alongside historical and bibliographic knowledge of the books, this produces a new, amalgam poetry collection from which to conduct future readings, modeling, and analysis.

	Moving forward from that sensibility for data quality as foundation for modeling literature, this project attempts to answer the followup question: "What does it mean to model literature as data?" Whereas the previous work adapted data quality concepts from information science, this one looks to standards for model selection and quality analysis from statistics known as "information criterion." For instance, a set of information criteria measurements based on and related to one initially developed by Japanese statistician Hirotugu Akaike in the early 1970s, can provide a sense of the relative quality of models by reflecting the amount of information loss between a model and the original data set. While a mathematical description of data generation – in the case of literature, authorship – remains out of reach, Akaike’s information criterion measure (and other criteria produced by successive statisticians) allows us to at least understand the difference between the statistical distributions of a work’s words (i.e. their likelihood of being used together) according to various kinds of modeling techniques such as frequency analysis, topic models, regression analyses, matrix factorization methods, etc. This comparison made by information criteria presents a new qualitative decision point for the use of literary studies.

	Of course, in the case of humanities scholars the quantitative outputs of such measurement can provide a befuddling endpoint. What to do with these numbers? The process of adaptation of such a measurement requires the incorporation of qualitative assessment of sections of a work as deemed notable (or perhaps very unnotable, e.g. hapax legomena) according to the various models being compared. Scholarly knowledge of Twain’s life and writings would be posed against the highlights of Twain’s autiobiography suggested by high quality models of it. Such comparisons are essentially close readings supported by qualified computation modeling and by the epistemological concerns of more traditional humanities approaches to textual scholarship. These readings/measures of model quality can be also thought of as a new form of annotated knowledge for readers and scholars of Twain’s autobiography. As such, they are also generalizable examples of how to engage in a thoughtful discourse between reading and the modeling of literature as data.



</Text>
        </Document>
        <Document ID="E9E8BF9D-B04D-4F48-9511-38DFD57346F3">
            <Title>Call Notes</Title>
        </Document>
        <Document ID="7947BA04-45C3-41A1-9477-AAF977D646D3">
            <Title>ToDos</Title>
            <Text>Find texts on scholarly editions for citeable writing on the topic

Notes

MLA committee on scholarly editions states that it currently it seeks, “to review editions that will help rectify the historical underrepresentation of racial, ethnic, gender, and linguistic diversity in the list of publications that receive the seal of the MLA Committee on Scholarly Edition” given their past legacy of featuring works that restrict the literary canon. Missing from this list is the idea of class-based literature. Without going into a critique of the field itself, regarding the “literary” this speaks specifically to low-pedigree works birthed digitally or by otherwise independent, publisher-less authors.
Why not create a scholarly edition of a fanzine, regardless and/or in concert with the “racial, ethnic, gender, and linguistic” diversity that it brings? Often times when a text is birthed digitally and not featured by a major publisher, the only category that remains on that list is “linguistic” diversity. However, we can say without a doubt that such a text features a lack of traditional “literary” pedigree because of the traditional gatekeepers.  This is the new question of “literary quality” – and it is one distinctly based on class politics that the 21st century publication industry has carried on and placed on top of the seemingly democratic realm of the internet</Text>
        </Document>
        <Document ID="1F20A0E3-59EA-4F64-B06C-9FD49FCF318D">
            <Title>May 28, 2019</Title>
            <Text>Notes
The kinds of qualities humanities scholars are interested in, Part 2

	1.	The question for a quality framework of a humanities data set is what qualities are to be measured and presented? Is it as many qualities as you can feasibly mine? Is it just those one feels are relevant to the research question? The preference should be to lean on the former, because, frankly, our bias in the humanities towards are research questions can narrow the field of evidence for an eventual argument so as to make it a virtually inescapable conclusion. Part of the raison-d’être of digital humanities study is to push for more gray areas as to conclusions (e.g. confidence intervals, etc.) – and not a gray area that come from what those in the sciences have sometimes called weasel words* [source needed] but rather via the latitude that our data set and observations can afford. All of this must be balanced, of course, with a reasonable narrowness for the argument of the research to proceed as unhindered as possible by the unfettered horizons of ‘could’ and ‘might be’.
	2.	If possible, consider the original criteria of the creators of the data set. The aim of this is not to merely to ascertain a full accounting of the searchability of an original digital edition/database, but also to understand the contexts and motives of the dataset’s creation. What was chosen to be included and what to be excluded? The overall goal is not to reproduce the work of the creators (an unreasonable request) but rather to summarize that work for the sake of the strength of a project.
	⁃	NOTE: Provide examples of what’s included and what was excluded for the Dickinson dataset
	3.	For instance, in the case of the Dickinson correspondence.
	⁃	Their notes on searching: “Notes on Searching
	4.	Create a search using any combination of criteria in the form to the left. Click on any one of the Run Search buttons; the three sections of the form are linked, and each "Run search" button has the same effect.
	5.	To search on text only, use the top box in the form
	6.	To add limiting criteria, use the second and third boxes
	7.	To search on particular features only, leave the text search empty”
	⁃	What’s searchable? Correspondences by the following criteria:
	⁃	Text of the letters (AND, OR boolean search plus phrase option)
	⁃	Also “search in editorial notes” option
	⁃	Names - every person listed in a name tag in the tei files - thus authority of original authors, subjects of writing, and editors are considered
	⁃	Repositories [Harvard University, Amherst College, Boston Public Library]
	⁃	All places mentioned in files
	⁃	Genre (as described by editors) [Letter, Verse, Verse-Letter, Letter w/ embedded verse, Letter w/ enclosed verse]
	⁃	Years [1850-1886 – but broken out into decade and the last truncated decade (by Emily’s death): 1850-1859, 1860-1869, 1870-1879, 1880-1886]
	⁃	Medium [Pencil, Ink, Pencil and Ink]
	⁃	Condition (physical): [binding holes, damage, mutilation, paper clip impressions, paste marks, pin impressions, pin holes, stains]
	⁃	Search options (checkboxes): Draft letter, Extralinear Material, Contains Enclosure
	⁃	Marks in Other Hands [Cataloguer, Susan Huntington Gilbert Dickinson, Emily Elizabeth Dickinson, Harriet Graves, Mabel Loomis Todd, Martha Dickinson Bianchi, Millicent Todd Bingham, Thomas Niles]
	⁃	Closing Signature [Dickinson, E, E - Dickinson, Emilie, Emily, Emily., Lola S. Scott, Sue]
	⁃	Physical Container [bound manuscript book, unbound]
	9.	Another fair criticism of the Dickinson example would be the existence of the TEI editions at all. While it’s true that this bit of work that exists is both advantage and filter, it’s also true that a “digital” edition that is merely plain text has already gone through a series of transformations, some of which are unnoted when used. The notion that a plain text unmarked is also unfiltered is a notable blind spot in the production of a computational model of literature in that the history of the text’s transformation. This includes any available and unobservable flaws in that transformation – the OCR error and the washing away of authorial and editorial purposeful structuring of that text. These errors have both quantitative and qualitative repercussions for a model of that text and any conclusions drawn from it, depending on the purpose of the model and the argument being made (i.e. translated into more scientific language: percentage error for token identification and ‘bucketing’). [At least 2-3 notable studies of OCR error and textual dataset error repercussions, already on hand in my bibliography.]
	10.	Rather, the TEI editions in the Dickinson case are the ideal case (or a “gold standard” – Hill and Hengchen 2) in which the transformation and perspective of the transformers are known almost to the nth qualified degree. These conditions rarely exist, but as they do here, they present that which towards users of textual datasets (in literary studies) are aspiring to grasp.
</Text>
        </Document>
        <Document ID="853F8F82-967F-49CB-9EBC-D7F3C93F4F10">
            <Title>Chapter 1. Data Quality - Introduction Notes</Title>
            <Text>I’ll begin with an experiment, and tell you in advance that it’s going to fail. How do I know? you ask. Well, I already tried it. How I got to failure: now that’s what matters. Over the course of a literary experiment, just like any other, tried and true notions will fail.

What are the characteristics that you wish to consider when siting a model?

“Charlie had said that key elements of my building’s design…would be determined by this first fact.” [italics mine] (Pollan 31)
“[T]here were views to consider (from the building, of the building), the fall of light across its floor, the movement of air around it, the ambient sounds, the angle at which it met the late-day sun.” (31)
“Dwell too long on so many soon-to-be-set-in-stone characteristics and the decisions is liable to paralyze you.” (31)
“I felt that by choosing its site – a single place out of all possible places in which to build – I was setting this great big contingency in motion, rolling it down the steep, one-way hill of personal and local history.” (31)

Views of the Building/Views from the Building [of a Literary Model]

What are the links between a data subset and its wider context/full dataset?
	•	These links will be key transitions between a model built and other possible models built
	⁃	One possibility for these is shared fields
	⁃	But the more salient possibility are fields that share or impart together (in combination) dynamics in the data set
	⁃	So first step would be to detect those dynamics - large and small trends
	⁃	Think basic statistical values (max, min, mode, median, mean)

</Text>
        </Document>
        <Document ID="D216B014-70C6-4F93-827E-D0E08149D894">
            <Title>May 30, 2019</Title>
            <Text>Done
	1.	Gathered and read “Rationale” for EDC alongside introduction (downloaded PDF). 
	2.	Read a bit of Martha Nell Smith’s first chapter to Rowing in Eden.
	3.	Downloaded and read a bit of Jerome McGann’s Radiant Textuality
	⁃	relevant in concepts and considering the time period of the production of the EDC
	4.	Reread Sebastian Coleman notes on Data Frameworks and thought about the principles and study involved in creating a data quality framework for a humanities data set – in particular the EDC
	⁃	Considering for metrics and metrics of quality - what the expectations of the producers of the EDC were and what information they wanted to draw out and what questions they wanted to provoke in the readership/users of the EDC
Notes
The questions they're interested in

"What was Emily Dickinson writing? Is this verse or prose or something else? What kinds of practices did she enact in her letter writing? How are poems integral to, and integrated into, her letters? How might poems in letters, and letter-poems, differ from poems in bound manuscript books?" -- from "Rationale"
My thoughts
	1.	What about showing the "level" of embeddedness of her poems in letters (or vice-versa)
	2.	What could this tell us more about the questions proposed by the authors of the EDC? What else could it tell us?
Their broader thoughts (re: outside of Dickinson's writing and pertaining to Dickinson's writing):

"With this edition, we hope to open up deeper, broader questions about editorial practices across our culture and society. Editing is not only central to literary productions and to film and television, but is constitutive of art, music, drama, and even politics. By revealing the work of editors, including our own, in shaping and making Emily Dickinson’s poems and letters, we aim to encourage readers to cultivate reading strategies unbound, strategies that refuse to accept the patterns given to them unquestioningly. These new reading strategies will offer, we are confident, far more than fresh lenses in our reading glasses. Such active, participatory reading was recommended by Dickinson herself, who wrote that the Poet dwells “in Possibility” and spreads “wide” her “narrow Hands / to gather Paradise." -- from "Rationale"

Mix and match, differing order suggestion, also possible place for cross-validation DH studies

"In addition to exploiting search capabilities more flexible (and so more powerful) than a book’s index, readers of this edition can reorder and rearrange the presentation of these materials in ways a book’s static page orderings will not allow. Such dynamic engagement with editorial materials likewise makes possible critical comparisons of original documents and transcriptions and comparison of documents previously available to only a very few scholars. We hope that these new interactions with Emily Dickinson’s writings will be transferable to other forums for reading so that readers will see the power and scope of editorial work in an array of cultural arenas."
-- from "Rationale"

Guiding Principle for Data Framework

An understanding of the rationale and intentions of a datasets creators helps create the highest quality models based on it. Metrics and measures that incorporate those subjectivities, create a richer and more substantive footing from which to make claims. This is how to begin to form a data quality framework for your data set.

These expectations of the producers of the dataset may and will likely differ from those using it. A resolution those differences is helpful, but not always possible. See Initial Assessment and Profiling notes on Laura Sebastian Coleman.

Emily Dickinson Correspondence “Rationale” (Accessed May 30, 2019)

https://rotunda.upress.virginia.edu/edc/preface.xqy

Emily Dickinson’s Correspondences: A Born-Digital Textual Inquiry, a new critical edition of selected poems, letters, and letter-poems, focuses on the diversities of Emily Dickinson’s surviving written record and encourages innovation in interpretation, in reading, and in scholarly editorial practices. Published translations of Emily Dickinson’s manuscripts homogenize her various writings so that all letters appear to be of the same ilk, as do all poems. Editions that divide letters from poems and poems from letters elide important aspects of the positions of Dickinson’s poems within epistolary contexts and vice versa. Emily Dickinson blended or embedded poems into the prose in letters; wrote them on separate sheets and enclosed them with letters; wrote poems and sent them as letters, with salutation and signature. In other words, Dickinson’s poems are often part of letters, and poems are often letters or are contextualized by their mutual enclosure in an envelope with a letter. But these poems, letters, and letter-poems are difficult to comprehend when they are stripped from their original context. Emily Dickinson’s Correspondences: A Born-Digital Textual Inquiry is designed to expose readers to the different kinds of the poet’s writings and features views of every sort of variation the editors have located in the Dickinson corpus of surviving manuscript writings. By doing so, we hope to make it possible for more readers to engage in deeper, broader inquiries into the writing practices of this revered American poet, asking questions such as, “What was Emily Dickinson writing? Is this verse or prose or something else? What kinds of practices did she enact in her letter writing? How are poems integral to, and integrated into, her letters? How might poems in letters, and letter-poems, differ from poems in bound manuscript books?”

Because all editing is interpretation, the editors of this critical edition welcome your feedback and suggestions about editing Emily Dickinson’s writings. With this edition, we hope to open up deeper, broader questions about editorial practices across our culture and society. Editing is not only central to literary productions and to film and television, but is constitutive of art, music, drama, and even politics. By revealing the work of editors, including our own, in shaping and making Emily Dickinson’s poems and letters, we aim to encourage readers to cultivate reading strategies unbound, strategies that refuse to accept the patterns given to them unquestioningly. These new reading strategies will offer, we are confident, far more than fresh lenses in our reading glasses. Such active, participatory reading was recommended by Dickinson herself, who wrote that the Poet dwells “in Possibility” and spreads “wide” her “narrow Hands / to gather Paradise.” The fact that this edition is “born-digital” makes it possible for us to show images of the materials on which we are reporting and to expose our editorial processes in ways that have proved too cumbersome for book editions. In addition to exploiting search capabilities more flexible (and so more powerful) than a book’s index, readers of this edition can reorder and rearrange the presentation of these materials in ways a book’s static page orderings will not allow. Such dynamic engagement with editorial materials likewise makes possible critical comparisons of original documents and transcriptions and comparison of documents previously available to only a very few scholars. We hope that these new interactions with Emily Dickinson’s writings will be transferable to other forums for reading so that readers will see the power and scope of editorial work in an array of cultural arenas. This edition is designed, then, not only to show how Emily Dickinson’s literary works have been made for public consumption, but to enable readers to become producers, not simply consumers, of literature and scholarship.

—Martha Nell Smith, Lara Vetter, Ellen Louise Hart

See Emily Dickinson Correspondence - Introduction in “/Dissertation/sources/ for a fuller discussion of the EDC materials

</Text>
        </Document>
        <Document ID="2C0450AC-2F1C-443E-B49A-E2CB40CBEBFB">
            <Title>February 26 2025</Title>
            <Text>DCRG Writing Day

- Finishing debugging dq evaluation output for the purposes of plotting
- Consider the thought that maybe writing should come first before code for the simple reason that writing may end up driving further code
- Let’s finish up this bit of code, continue writing and see where it leads for the first chapter draft
</Text>
        </Document>
        <Document ID="99E5F012-3010-4B60-A6D4-171F55BA004D">
            <Title>August 1, 2019</Title>
            <Text>Notes
	1.	Consideration of Bode’s new object for computational literary analysis she calls a “scholarly edition of a literary system”
	2.	Quote from Schoch’s Big and Smart data article to push off from: “capturing data is not passively accepting what is given, but actively constructing what one is interested in”




</Text>
        </Document>
        <Document ID="5DA18AC2-94B5-4B67-98A4-56AA87FAC1EF">
            <Title>Dissertation Journal</Title>
        </Document>
        <Document ID="9B871F45-BE94-41E8-A912-A1091707CF29">
            <Title>January 6 2025</Title>
            <Text>Found error in MTPO Huck Finn xml

Usually chapter number is identified by locating all div1 tags with type attribute = “chapter”
That tag’s head.string value should match the string Chapter &lt;chapter roman numeral number&gt;.

Chapter 18 had no dot suffix, causing an error</Text>
        </Document>
        <Document ID="FA375BED-E51F-41C2-BB63-84D9CB50AD8C">
            <Title>Part 1</Title>
            <Text>Chapter 1 - Part 1 - Data quality and assessment framework explanation
Github issue: https://github.com/jarmoza/aolm_full/issues/14

Topics to Discuss

	•	Data Quality and Laura Sebastian Coleman's 'Data Quality Assessment Framework'
	•	Inaccurate transcription
	•	Overrepresented portions of a dataset (and thus underrepresented portions of it)
	•	Overrepresented datasets
	•	Hapax legomena ignored by CDF (statistics)
	•	Inconsistent transcription across editions/versions of the same/similar text
	•	Different editions - which to consider
	•	Access to datasets - public/private
	•	The size of a individual set/the amount of datasets available - is this enough to be modeled on its own
	•	What is the best version of an individual text/the most suitable version of a text for modeling?
	•	The challenge of difference in kind when it comes to evaluation: quantitative versus qualitative (and the mindsets that make this distinction as translated into modeling/anlaysis/interpretation/criticism)
	•	The need for DataLad-like documenting of digital text datasets

Body Text

Data Quality and Laura Sebastian Coleman's 'Data Quality Assessment Framework'



The measuring tools - Discussion of the tools we make and use to measure data quality

Of course, we can look to the accuracy of our measuring tools when we think of data quality. A measurement is, after all, only as good as its tool. (Cite Rockwell’s source on tools in his DH making book.)

Inaccurate Transcription

When we think about specific issues with “quality” in literary studies more typical concerns are for the fidelity of an edition to the source material – if that source material exists at all. That idea of moving closer and closer to authorial intention has been tested over the years, but the idea of accuracy has mostly been left to authorial experts and the artisans involved in the practices of publishing. The digital edition has troubled this concept of accuracy in that the means by which a text (i.e. a novel, poem, letter, etc.) is transformed from a physical book into a digital format are typically far more opaque. Like book publishing practices before it, the less opaque we want our digital transcription processes to be the more expensive they become to produce and, in turn, receive. The early promises of free information on the “information superhighway” loom large over our conceptions about access to digital texts. In the past few decades, we have had to reapproach our relationships between digital information and the costs of its production. Paywalls, old and new, spring up before us barring access. In reaction, demands for open access to data from both the public and from a private industry hungry for more and more digital information have also arisen. This level of access too therefore must be part of our considerations when we think about data quality in the humanities. Getting back to the idea of accuracy in a digital edition though we see that errors arise from at least two disaparate sources: human transcription and machine reading. Sometimes the best way to transcribe an edition into digital form is to sit with a physical book and simply copy. If you picture a medieval scribe looking from one copy of an illuminated tome to another one as they copy word by word, this is not far off from the modern version. And like the scribe, modern humans are subject to the same foibles of copying: omission and misperception. This sits in the high cost end of digital transcription. After all, unless a person is volunteering (more on that later) someone must pay a person to sit and transcribe a book from its physical form. And machine reading, also known as ‘optical character recognition’, may consistently produce the same error over and over again since its recognition mechanisms are interested in pattern recognition whereas a human may only make that mistake a handful of times. All that is to say, that if inaccuracies in transcribing have changed slightly they have only been compounded. The difference in error rate of manual transcription versus machine reading is not consistent and is dependent upon the human, the algorithms/machine learning, and the texts themselves.

Overrepresented portions of a dataset (and thus underrepresented portions of it)

	Repeatedly throughout literary studies and beyond into other knowledge domains we see researchers repeat citations of famous sources. This mimics the behavior and phenomenon of oft cited passages of literature. (Cite Columbia Jonathan heatmap study). Of course, there are obvious pitfalls of repeatedly citing one source or one section of one source over another source or another section of even the same source. No sociologist would be surprised that a nice statistical curve could fit well the numbers of these citations by areas in a book. (A small study? A citation?) This uneven citation phenomenon however also doubles in when it comes to errors and error correction for digital editions of texts. The question has always been how much this kind of disparate error correction affects the data (digital text) and any model made from it. The assumption and and overlying corrective (not quite the right word) has been that the central tendency principle involved with statistical modeling will render such small sets of errors as long-tail statistical noise and will affect the greater statistical shapes at play within the modeled text. But this assumption/corrective is directly opposed to the tendency of literary studies to find meaning and even great significance in small words and phrases within a text. For literary studies these small errors do matter a great deal. While efforts oftened likened as close reading (etc.) can easily focus on small pieces of the text, it is important to retain this ability and view of the text even as we model its large statistical tendencies. Therefore for many literary scholars what would otherwise be characterized as long-tail noise is, if missing from the model, a significant loss of data quality – and in turn, model quality. Our perspectives on what texts are matters. If the person or program doing natural language processing considers as digital text to be a series of natural language phrases to be measured and counted then 3% error in that measurement and counting may be neglible and deemed reasonable. If a text is something that is full of meaningful words and phrases that are invaluable to the overall work then 3% error could be the difference between a classic novel and a flawed one. Therefore when we model texts for the purpose of literary studies it could be an important concern to ensure that that 3% error is eliminated. And while these concerns could ultimately be regarded as epistemological and meaningless to a statistical model, with meaningful error in place it renders a computational model of a text for literary studies significantly lower quality by the standards of literary studies.
	Taking the same idea of the heatmap of citation and reference, we can apply it to data quality. Depending on the metric and its relevance, we can quickly see how data quality when measured is not just a summary measure but something that stretches over the entire body of a work. We can also take this notion and extend it further if we have multiple digital editions that are under consideration for further use down the research chain –  either reading or modeling, for example. Ultimately, how data quality measures depends on the aims of the research project, not just in which measures are selected as quality criteria but also how those measures are used. By exploding the points of data quality measurement within a work across logical divisions (whatever ‘logical’ may mean for a particular researcher), we can do more than compare data quality of editions, but we can do what would otherwise seem unthinkable: compose the highest quality digital edition of a work from several works. Again, such an action would depend on the aims and ethics of the researcher. Do we require the model of a digital edition of a work to be as true to its physical source as possible? If not, then what would be the difference between composing a high quality Frankenstein edition and going through an scanned work and correcting flawed results of optical character recognition? What if, to provide the converse of overrepresentation, or, at least a different textual form, we only provided the sections of a text that met the highest of quality standards under our assessment framework?

Overrepresented datasets

Data quality measures can be taken beyond mere counting of words to do something more complex and elegant. Take for example the kind of work that has been done for decades with authorship attribution studies &lt;citation&gt; that compares word usage frequency among their writings. One could take the linguistic signs of an author’s writing and measure how well there is a match between a particular work and the authorial signature – or, as previously mentioned, sections of a digital work that match more than others. If authorship of works are in question, once a data quality measurement for that authorial signature is in place, there is a well-defined reason that can be pointed to as to why one work might be more suited for modeling and study than another work. And by using a data quality assessment framework, researchers can also ensure that that signature-as-metric does not become an overriding principle by which to make such an assessment of the suitability for assigning the probability of authorship, modeling, or study of one work over another. Taking a step back from the perspective of a prospective study, one can imagine this approach to understanding the data quality of literary datasets to be adapted in retrospect. If previous research has looked to digital works, perhaps in a seeming overrepresented fashion (see Columbia Jonathan’s heatmap) what can new understandings of the data qualities of the textual datasets used in those studies tell us about their findings? Perhaps some digital work of poorer data quality shaped the project’s overall model so as to skew its results. Or vice-versa, perhaps there was some work of higher data quality considered by the project but was overlooked in part because there was no data quality framework at hand to make those decisions. Without defined measurements of the qualities of digital works such understandings can be left undiscovered.

#

Implementing a basic authorial signature above for the novels of Herman Melville taken via Project Gutenberg
And this measurement coud be tailored to the needs of the project and the subject matter expertise of the researcher.
Depending on the purpose of the model you are building after your data quality assessment, one can imagine a number of questions such a data quality assessment would answer. What is the most Melvillian novel? What is the least? Which novels are most like a particular novel? Combining such typical digital humanities findings with an overall data quality assessment we can see that not only can such an assessment be used to report on the current quality of the digital editions present within a literary dataset, but also how the assessment can be used to reconfigure the dataset itself presuming a researcher might want editions within the dataset to skew towards a certain characteristic. Using the above example of all of Melville’s novels, the metric easily answers those questions. What is the most Melvillian novel? Perhaps unsurprisingly it’s Moby-Dick – a tragic sea yarn mixed with metaphor-dense prose. What is the least Melvillian novel? Also, in what would not be a surprise for those versed in his novels, the least Melvillian novel is The Confidence Man – an otherworldy voyage down the Mississippi by boat with a large cast of esoteric American characters. There all sorts of investigative vectors one could take and further decision points from there regarding the refinement and curation of the dataset. For example, later novels, Pierre and The Confidence-Man are close in vocabulary and word usage while much of his sea tales are closer to Moby-Dick with White Jacket and Mardi, Volume 1 being closest. Curiously, Mardi, Volume 2 is notably further from the first volume and certainly from Moby-Dick. Why is that? Perhaps there is something amiss with its digital edition? Or perhaps the reason is more thematic in nature? In any case, this kind of work can be extended to multi-author datasets looking to isolate particular categories of works, genres, etc. By including both general and specific data quality metrics within the assessment framework, it can become a frequented tool for both stating the quality of a dataset to be modeled and a means of tailoring the model itself. Though it produces summary figures, one specific aim of a data quality assessment framework for digital texts is to provide an array of information about them for practitioners and researchers.</Text>
        </Document>
        <Document ID="5B38F7ED-7CF8-4ECE-B0DF-A2FDC66B4DE6">
            <Title>Why 'Literary'?</Title>
            <Text>Introductions

Those reading this may be forgiven for the assumption that my title’s term ‘literary’ necessarily refers to the qualities of the works that are modeled in the following pages. While the term itself is already loaded with meanings (as well as connotations both positive and negative), I wish to encumber it with another definition. The subject of this work is the modeling of humanities’ data (more on that in the first chapter), and thus the models will be created with the intent of harboring the goals of humanities scholarship. Those goals vary from field to field and even individual to individual, but, as we will see, humanities data modeling infers some other qualities not typically considered in more historically mathematical fields of study. A “literary model” is therefore something imbued with a high quality, but not necessarily one just of statistical rigor. Rather, just as “literary” can refer to the values both within a work and the external forces that elevate that work to a high status, so too will that extend to a humanities model of cultural objects  – in this case of text and images. A “literary model” is still a model that quantifies and reduces in representation of its underlying objects, but it is more aesthetically rich, and one that is more sound because of that richness.</Text>
        </Document>
        <Document ID="D2595764-DDC7-4286-AB49-A542123A151A">
            <Title>Body (w/ outline)</Title>
            <Text>1.	 Mark Twain autobiography description - popular response

In fits and starts over the course of his life, Mark Twain attempted to write an autobiography of his adventure-filled life. Toward the end, he dictated much of it to a stenographer. He also came to a peculiar agreement with his publisher and estate in which the work would not be published until 100 years after his death. He reasoned that this would grant him freedom to talk openly about the people in his life as well as public figures and subjects of the time. By the time that anniversary came, the Mark Twain Papers &amp; Project at University of California, Berekely had been preparing the autobiography for years for its eventual publication. As a celebrated and legendary American author, it was met with great commercial success. However, much to the initial confusion of readers, the entire autobiography would not be fully published until five years later, meted out every two years. The final product was three volumes in length, each of which, including editorial apparatuses, contained almost 800 pages. In effect, it was a book popularly bought but not as widely read.

2. This sets up the problem of a large, opaque work

The presentation of an unwieldy and thus opaque work for reading and interpretation is not a new kind of problem for humanities scholars, and certainly not for digital humanities scholars. On either end of the analytical spectrum, it is approached with similar divide and conquer heuristics. One might look to previous scholarship, lightly read the whole work while concentrating on a few sections, or, via computation, attempt to model the work(s) with an algorithm that can describe the entirety of the collection/object while giving insight on mathematically notable sections. These are but a few possible, general approaches. 

3. The problem of model selection for computational modeling of literature

For the computational approach there are a few significant decision points. There is the choice of models, sometimes driven by state of the field and sometimes driven by state of the art. And then there is the type of model output analysis. How do we comprehend the model and how do we account for how well it represents the underlying textual data? These questions of model selection and estimation of quality are not under-theorized in the field of statistics. The question of which model might be most appropriate to understand or discover new insights about Mark Twain’s autobiography and works like it, is though. 

4.	Project proposition on model selection via model quality assessment for project/humanities
5. Description of previous chapter on data quality and overall dissertation

My dissertation, ‘The Art of Literary Modeling’ , describes and demonstrates several new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. Its first chapter attempts to answer the question, "What does it mean to read literature as data?" That chapter's discussion of "literary" data quality looked to Emily Dickinson's poetry and its complex publication history. My proposed project for the summer fellowship is the followup chapter on new means of qualitative and quantitative assessment of model quality specifically for humanities researchers and their application towards uncovering new insights about Twain’s sizable autobiography. The digital version of this work, in TEI form, has been supplied to me via the Mark Twain Project Online at University of California, Berkeley. 

6. Discussion of adaptation of information science methods for humanities purposes (dq metrics, dq assessment, new “literary” data quality valuation)

That first chapter of my dissertation adapts a series of data quality metrics from information science scholar and career practitioner, Laura Sebastian Coleman, that are more typically employed in environments where data sets are dynamic and have external concerns – legal, technical, financial, and otherwise. Newly derived metrics from Coleman’s examples concerning the textual consistency and lexical and thematic qualities across Dickinson's multiple, posthumous publications are brought about by analyzing and modeling those publications, ultimately filling out an overall quality assessment of those (digital) works.  When taken into account alongside historical and bibliographic knowledge of the books, this produces a new, amalgam Dickinson poetry collection from which to conduct future readings, modeling, and analysis.

7. “Information criteria” for selected model as means of quality estimation of model output

Moving forward from that sensibility for data quality as foundation for modeling literature, this chapter attempts to answer the followup question: "What does it mean to model literature as data?" Whereas the previous work adapted data quality concepts from information science, this one looks to standards for model selection and quality analysis from statistics known as "information criterion." For instance, a set of information criteria measurements based on and related to one initially developed by Japanese statistician Hirotugu Akaike in the early 1970s, can provide a sense of the relative quality of models by reflecting the amount of information loss between a model and the original data set. While a mathematical description of data generation – in the case of literature, authorship – remains out of reach, Akaike’s information criterion measure (and other criteria produced by successive statisticians) allows us to at least understand the difference between the statistical distributions of a work’s words (e.g. their likelihood of being used together) according to various kinds of modeling techniques such as frequency analysis, topic models, regression analyses, matrix factorization methods, etc. In turn, the comparison made by information criteria presents a new qualitative decision point for the use of literary study.

	8.	Combining IC with proofing via humanities qualitative assessment
	⁃	close reading sections highlighted by the model and producing valuation kind of like an annotated bibliography for these highlighted sections of the model/underlying text
	9.	Prospects for new insights about the Twain autobiography and new notion of sound evidentiary basis for the highlighting/selection of sections for analysis/comparison/contrast/etc.
	10.	Prospects for other researchers to use these model metrics and heuristic to make assessments of their own models and use of those assessments as (part) basis for the interpretive claims made

Of course, in the case of literary scholars the quantitative outputs of such measurement can provide a befuddling endpoint. What to do with these numbers? The process of adaptation of such a measurement requires the incorporation of qualitative assessment of sections of a work as deemed notable (or perhaps very unnotable, e.g. hapax legomena) according to the various models being compared. A direct comparison must be made between the highest quality model’s highlights of Twain’s autobiography and scholarly knowledge of Twain’s life and writings. Such comparisons are essentially close readings supported by qualified computation modeling as well by the epistemological and subject matter concerns of more traditional humanities approaches to textual scholarship. 

These readings/measures of model quality can be also thought of as a new form of annotated scholarship for readers and scholars of Twain’s autobiography. As such, they are also generalizable examples of how to engage in a thoughtful discourse between reading and the modeling of literature as data.



</Text>
        </Document>
        <Document ID="C96AF56F-82EE-4920-9D6A-DE2DFE9B79CB">
            <Title>January 14, 2021</Title>
            <Text>Introduction prose
Borderline

	One summer I drove up the pine-lined coast of Vancouver Island in British Columbia. I didn’t have any particular destination in mind. The prospect of Tofino had occurred to me, but that would be several hours further along the coast. The region, what Americans refer to as the Pacific Northwest, is somewhat lesser known in the American imaginary. To describe the landscape briefly, it is a place where mountains and large coniferous forests directly meet the Pacific ocean. The beaches of this place are also comparatively alien, particularly to someone who has spent their life in the east, where fine sand beaches are more common. In the northwest, one finds rocky beaches composed of small to medium sized stones. The colors are extraordinary. Amid the giant sea stacks and escarpments, the foamy surf of the water washes up and recedes upon these stones that glisten in pinks, greens, oranges, blues, and purples – products of volcanism, water, wind, and time. Even the more common basalt grey ones shine under a typical cloudy day. The summer time however brings long spells of sunshine. I stopped my car at a small, nameless beach park where only a few travelers or area residents were sitting and begin to hike toward one of the bordering escarpments where a giant pine was perched. The erosion had exposed some of its finer roots the saltwater and air. I clambered over the boulders at the base of this outcropping, as best as a non-native could. These types of giant rocks, immediately adjacent to the place where water greets the land, are notorious for their danger, covered in slick seaweed, anemones and razor-sharp, black mussels. Further along and closer to the escarpment, I entered a small cave – really more of an arbor considering the tree’s roots jutted down from overhead. This was a calm place, sequestered away from even the scant amount of people a few dozen yards away. The cave cut underneath the escarpment and continued around away from the beach. To my surprise, I found the path led upward through the soil and to the small, grass and moss covered area around the tree. From this vantage, I saw the boulders below, the stone beach, the people, but now even more: the coastline leading northward, oceangoing vessels, more pine-covered islands and mountains in the far distance.
	These kinds of perspectives – on the beach, over top the boulders, inside the cave, above on the tree’s perch – all inform an experience of what would be considered the world outside our interior spaces – our homes, office buildings, and the rooms and spaces inside them. As we venture into more metaphorical (and metaphysical) spaces provided by computers, the contrast with the exterior space has been extended, one step further removed from such a wild, if serene scene. The computer itself acts a technological extension of our mind’s interiority. The subsequent virtual spaces on it reflect as such (desktops, windows, bins, folders, file systems, and on).  But the metaphors we use to move in and comprehend any of these spaces turn out to be universal. Grasping to understand a concept within a virtual, interior, or exterior space follows a similar heuristic:

	1.	We situate ourselves from a particular position in that space.
	2.	We observe objects from that vantage.
	3.	We measure them.
	4.	We produce determinations, comparisons, and inferences.

	From the tree’s perch, I observe the pine-topped island across the water, a gigantic ferry slowly moving away from shore, the miniscule person reading on the beach. The relationships between these objects produce a sense of space and perhaps quietude. They are less tangible in the moment, but I know that with some effort I could proceed towards them, albeit with some considerable effort. Yet, during and by journey’s end, my perspective on them will have changed. In my office, the scale of the space and my conception of it circumscribes that kind of physical possibility. Yet, as it does so, the possibilities for heightened attention and exploration of the objects in the room become more possible. These choices of space and perspective then become generative of the perceptions and conceptions that are produced from them.

Further thoughts

	1.	Spaces and objects with relationships forming the theoretically possible
	2.	Why do we build big windows looking out onto that exteriority - the fear of the loss of perspective when moving indoors. The desire to be able to have large perspective while being interior, occuping interior physical, psychological, virtual spaces
</Text>
        </Document>
        <Document ID="6089098D-8FE4-4295-8757-03A9D86492A7">
            <Title>July 16, 2019</Title>
            <Text>Notes
In Search of Quality; And not the one you’re thinking of, either.

What we seek when we look for the qualities of a piece of literature began to confuse me. What is close reading? What was distant reading? When did those terms make meaning? And when did they change? What struck me as I considered these kinds of questions that have phased in and out of fashion with digital humanities scholarship is that underlying all was one that gave every one of them the shakiest of foundations. And here it is:

		What is the difference between a quality and a quantity?

Digital humanities (or whatever what one may want to call it) has risen on the crest of the fear of quantification. It insists that counting things may be revelatory of new meaning. I’ll illustrate this with a hypothetical conversation, and then provide an example afterward.

Person 1: Well, what is it that is being counted? 
Person 2: A quality or qualities of that thing. 
Person 1: Okay. How would you describe a quality?
Person 2: An array of attributes that prompt that thing being labeled.
Person 1: Okay. So there are one or more attributes that cause something to be labeled as having a quality. How would you describe that attribute?
Person 2: Well that thing either does or doesn’t exhibit that attribute. Okay.
Person 1: So you could say that the exhibition of that attribute exists in a range? At some point that thing assumes that attribute, either like a switch on-off, or gradually so.
Person 2: I suppose you could say that.
Person 1: Okay. So a quality is quantifiable.
Person 2: Now, I don’t know about that!
Person 1: In fact, you might say that the notion of quality rests on implicit, quantified measurements.
Person 2: Now you’ve just gone too far.

That shaky question, as it turns out, is not just whether or not there is a difference between a quality and quantity, it’s if the notion quality arises from cursory attention to detail.

Why is this important? It goes to the heart of the current questions before many different types of researchers considering how to use computational tools to accurately extract meaning from passages of text, and for literary scholars, whether or not they’re usable at all for the questions that have traditionally been asked by their field(s). And if quality is more indicative of inattention than not, or at least of the comforting “good enough” soup in which we navigate our everyday lives, then quality must be revisited, redefined, and reconfigured for the purposes of digital literary study.</Text>
        </Document>
        <Document ID="D7846FA2-8777-43FA-8CDB-06320642D819">
            <Title>January 9, 2025</Title>
            <Text>Once results are computed for each overarching metric (including its submetrics), an evaluation function is called to combine the results in a logical way in order to produce one, unidimensional number for the metric

Each submetric is weighted evenly. And each sub-submetric may calculate its own evaluation to a number in a way that is logical for that sub-submetric. (Weighted average)

Submetrics are folded into a metric via averaging

Each subsubmetric is similarly weighted evenly in creating the final submetric number and then averaged for the final submetric number

Metadata sufficiency is an explicitly internal metric

While textrecord count is one that relies on external comparison with the ur text

Now on to visualizing and writing. The evaluations are now complete on the Project Gutenberg Huck Finn editions.

Overall, they average a 72% quality rating based on metadata sufficiency and text record data quality metrics

These can be separated by edition and also extended now to the Internet Archive editions for further testing</Text>
        </Document>
        <Document ID="B6A849EF-096D-4EB4-937D-75BAE7684759">
            <Title>Discussion of digital editions of Huckleberry Finn</Title>
            <Text>Discussion of original publication

Twain wrote The Adventures of Huckleberry Finn between 1876 and 1883, with at least three separate sprints of writing. “Mark Twain Project” notes that it draws from two distinct manuscripts, with its publication in the U.K. by Chatto &amp; Windus in 1884 by Charles L. Webster &amp; co. in 1885. Project staff have been so meticulous in its bibliographic detail that they note the paper types and different types and time periods of different the writing implements Twain used. Depending on what material you count the novel consists 366 pages meted across forty-three chapters. The original edition is also replete with illustrations of the scenes of the novel, making accurate scanning of the book for optical character recognition a more challenging task with captions being caught up in the body of chapter text and the beginning letters of chapter beginnings baked into the illustrations. The book is one of the most famous and most reprinted of American novels, itself being outside of modern copyright protection. This also makes the novel – as an abstract entity for analysis – a very fruitful site to study how data quality can affect literary data and in turn the production of literary models.

Discussion of digital editions in general (find sources?)

Towards those ends, we will be looking at the best source of a “ground truth” for the novel: the highly-curated edition features on the “Mark Twain Project” site. And in comparison, we will be looking at digital editions curated over time by volunteers at “Project Gutenberg” as well as more scanned and digitized editions on “Internet Archive” that have more dirty character recognition but known bibliographic provenance. This mix of editions will be used to visit our data quality metrics for literary data - metrics specifically designed around our interests in Twain and Huckleberry Finn.

Discussion of Project Gutenberg and Internet Archive

Detail on each edition being used in the subsequent analysis and of the qualities that are noticeable from visually inspecting them

List of digital editions

	1.	
</Text>
        </Document>
        <Document ID="892C1D01-220A-4205-8ED1-A6FF67758A7E">
            <Title>May 27, 2019</Title>
            <Text>Notes
The kinds of qualities humanities scholars are interested in, Part 1
	1.	An investigation of the Emily Dickinson correspondences data set
	⁃	Authority as quality?
	⁃	Who made this? Appeals to ethos can be seen as a relatively weak piece of evidence to stand upon when constructing an argument. Never the less, maintaining an understanding of the sources and (therefore philosophies of the construction of) our sources has powerful rhetorical weight.
	⁃	Aspects of authority to consider:
	1.	Institution of content
	2.	Institution of annotation
	3.	Experience
	4.	Guiding principles
	⁃	How about a percentage of the work actually “gone over” (transcribed, encoded by the primary researcher)
	⁃	Percentages of where a data set’s actual content came from
	⁃	In this case the breakout for the Dickinson correspondence data set is (out of 74 sources):
	▪	Harvard University (49) —&gt; 66.2%
	▪	Amherst College (21) —&gt; 28.4%
	▪	Boston Public Library (4) —&gt; 5.4%
</Text>
        </Document>
        <Document ID="CF1E91FB-2EF7-46C7-9AF8-D6B79FA61BA5">
            <Title>June 8, 2023</Title>
            <Text>On model quality - the purpose behind it

Now that we have seen how data quality standards can be established for humanities research practices and projects, the turn to undertaking a better understanding of model quality is only natural. The aim here is take all of our interests, aims, presumptions and carry them through (or defeat them after reflection) the computer-aided interpretations of our objects of interest. And of course, a model is an interpretation under the framework of a methodology. The ideas that led to the use of a model type and the creation of an instance of that model type with our data get compressed and shaped by the limitations of that interpretive, mathematical system. I don’t want to shy away from speaking of these interpretations as mathematical systems either. Though an obvious characterization, like the pre-established rules of a poetic form, a mathematical system portrays reality in such a way that conforms to the limits of the system and the parameters that are fed to inform the creation of that system. Thus any extended interpretation from that system faces the same limitations. This phenomenon is directly analogous to any research that has utilized distinct tools to aid/inform its outputs. The ideas radiate outward through comparison.
How has statistics handled this need to compare the outputs of its methods? Information criterion. That is to ask, “What possible criteria can we use to determine whether our information – our model of our data – is lossy in terms of its structure, e.g.  faulty?” And following that question, “How does this model’s amount of information loss differ from another model’s information loss over the same dataset?” It’s not so difficult to see the implications of this understanding once model quality comes into play – and comes into play on top of datasets graded by our previously established data quality frameworks.
Let’s imagine an example text analysis/DH project. You have a research question and a potential volume of data that you think can be modeled to answer that question. Let’s say you’d like to understand how Shakespeare’s language in his comedies changed over time. Now the definitive versions of Shakespeare’s plays are sometime in contention, but let’s say through your data quality framework you were able to establish a qualitative and quantitative metrics that measure each version and then grade them in a DQ framework. The framework suggests a particular version of each play – guided by its scoring. Now what? You could select a set of models you’d like to use to attempt to answer your research question – in this case, characterizing Shakespeare’s language change in comedies over time. You could select a TF-IDF model, a topic model, a Hidden markov model. And then perform it over the set of versions suggested by the data quality framework. You could even additionally model the versions deemed lesser quality by your DQ framework. Try a variety of values for the respective model parameters as well! And all of this modeling to what end?
By selecting an information criteria for model quality assessment we now have a means of comparing all of the above models and characterize their comparative information loss and therefore quality. Now, information loss bears a bit explaining still. Information loss from what? Let’s speak reductively for a moment and imagine that our dataset represents the real world object it models to 100% accuracy. (Of course, it does not.) Our models above are further simplification of the dataset! So the question becomes how much information from the dataset is lost when the model “fits” its dimension reduction on top of the dataset as a means of gaining simpler relationships – and thus understandings – in that dataset.

NOTE from the other day: Notion of qualitative check-in points, sampling for verifying data quality with a qualitative metric score</Text>
        </Document>
        <Document ID="10E4100C-D7F8-4BD6-BE12-E2DD2D861721">
            <Title>Todo and Sources</Title>
            <Text>Todo:

	1.	Do readings on contemporary information criterion
	⁃	Read preface to Burnham and Anderson’s book Model Selection and Multimodel Inference
	⁃	“Information theory includes the celebrated Kullback–Leibler “distance” between two models (actually, probability distributions), and this represents a fundamental quantity in science.” (vii-viii)
	⁃	“In 1973, Hirotugu Akaike derived an estimator of the (relative) expectation of Kullback–Leibler distance based on Fisher’s maximized log-likelihood. His measure, now called Akaike’s information criterion (AIC)” (viii)
	⁃	We focus on Akaike’s information criterion (and various extensions) for selection of a parsimonious model as a basis for statistical inference. (viii)
	2.	- Do readings on Twain’s autobiography (popular reception, academic reception, academic critique) 
	3.	- Find elements of both data and metadata in autobiography tei files for analysis
	4.	- Model those data and metadata elements
	5.	- Examine code already done for chapter/fellowship in Twain and information criterion
	6.	- Consider possible visualizations that can be drawn out from modeling results to demonstrate the differences between models of low and high quality


Sources:

Introduction to AIC — Akaike Information Criterion: https://towardsdatascience.com/introduction-to-aic-akaike-information-criterion-9c9ba1c96ced
Zajic, Alexandre; December 27, 2019

AIC works by evaluating the model’s fit on the training data, and adding a penalty term for the complexity of the model (similar fundamentals to regularization). The desired result is to find the lowest possible AIC, which indicates the best balance of model fit with generalizability. This serves the eventual goal of maximizing fit on out-of-sample data.</Text>
        </Document>
        <Document ID="A3DD3A1E-F4CE-49EC-AE29-16B50E6857A3">
            <Title>Preparatory</Title>
        </Document>
        <Document ID="955A2813-A608-4216-AF0A-89F2BD230FD3">
            <Text>Could I use open linked data on text modeling? In other words, annotate and create a graph model of texts and make intra- and inter-text connections

“federated” data analysis of texts using open linked data</Text>
        </Document>
        <Document ID="20AA57A6-47DA-40B3-A83C-A2DFE772C323">
            <Title>Epilogue</Title>
            <Text>A group of digital humanists, much like any concentration of specialized geeks at one table, proves a most entertaining gathering. At a conference dinner I sat next to a senior professor who specializes in text mining. As the evening wore on with food and wine, talk of life, field gossip, and war stories of projects past and present, I turned to them and pondered aloud something that I had been considering after years of working on such projects. At this point you can guess what I asked them.

“Sometimes I think that maybe we are doing a disservice to our successors by not devoting significant effort to developing rich datasets.”

“Yes, I see your point,” they replied. “But I would like to work on research problems too. It’s a bit more exciting.”

Mind you, I am paraphrasing, but that was the gist of the exchange. I don’t fault them for their reply in the least, but it is telling of the state of things for the field, and perhaps even larger, our information age. This mindset – that I too have shared – formalizes and energizes our smallish subfield of study. But it must be said that dataset creation and curation is undoubtedly also a research problem. It is not merely a step along the way to uncovering hidden patterns of culture- and meaning-making. 

The balance between defining well the bases of literary research and the modeling solutions we wish to use to look for answers will never be a solved equation. Nor should it ever be. For researchers, performing what may seem like the drudgery of thoughtfully crafting a data set will never hold quite the same amount of excitement as asking questions about those materials and looking for those answers. The tension of those two activities hold the key to successful research. We do need well-considered data that draw in useful and interesting descriptive information about source materials while also historically and culturally contextualizing those materials. But we do need exciting work as well. We need to produce questions that actively drive interest in literature and work on statistical/computational methods. Both open up possibilities for future research, and future avenues of research. This is the tension between the carpenter and architect. One cannot exist without the other. Each activity has a meaningful and complementary contribution to make to the practice of literary modeling. The architect – as we know the more prominent activity of digital humanities – is text mining and modeling, making decisions about how to shape and position their data for analysis. But the carpenter, the person creating and shaping the data set that underlies those models (the data really being a model itself) also makes decisions about the possible models that can be derived from that data.</Text>
        </Document>
        <Document ID="97C70077-B576-4F74-A10F-74354DB5179B">
            <Title>Chapter 1 - Full Draft (2025)</Title>
            <Text>Introduction - Siting a site for modeling (the fundamental task of research)

Ideas about data quality in the humanities

Ideas about data quality in the sciences

Hathi Trust data set (HTDS) and 19th century American Literature

Bidirectional quality

Data quality work thus far in the digital humanities

Data quality frameworks from information science

Application of Sebastian Coleman’s framework to HTDS

Metrics for the foundation of data quality for the humanities</Text>
        </Document>
        <Document ID="2436EE7F-799E-4ACB-84AE-375C28CB90C2">
            <Title>PhD 2022 Annual Review</Title>
            <Text>1. Please consult pp. 18-20 of the PhD Handbook and write a few sentences comparing your progress in the past year to the model schedule described.

I would say I am behind the listed model, but have been making steady progress in the past year on data curation, code, and prose for the first two chapters of the dissertation. I have also taken on some work to make up for the lack of fellowship support. Due to my digital humanities summer fellowship during the summer of 2021 however, I was able to make significant progress on the core project and methodology for my second chapter. And as a result of that work, gained momentum towards my work on the first chapter. (Since my dissertation entails a discussion of both data quality and computational model quality for literary studies, the topics are codependent across chapters.) Since then, I have fully curated over a dozen digital copies of the novel I am now using as an exemplar for the first chapter (Twain’s Huckleberry Finn swapped with Dickinson’s varied poetry publications), composed and computed data quality metrics specifically for them, and put together/revised prose I have had on hand for the first chapter.

2. If you are at the dissertation stage, please indicate your current working title and an approximate date when you last met via Zoom with your director and/or committee. Feel free to describe why/when in a few words. *

The Art of Literary Modeling; February 24, 2022, Check in on progress and some prose review.

3. Specifically in reference to the last year, what academic, professional or other activities and accomplishments (if any) would you like the department to know about? *

1) Winner of NYU’s Digital Humanities Summer Fellowship in 2021. This project involved the creation of a model for detecting aphorisms via Mark Twain’s three-volume autobiography. (Code: https://github.com/jarmoza/twain_autobio_modelquality) A short talk on this work was given at the NYU Digital Humanities showcase on October 7 (https://nyuhumanities.org/event/2021-digital-humanities-showcase/)
2) A cowritten paper, "The T3 - an open, large, human-annotated benchmark for biological and artificial semantic representations", featuring my past work on language models and fMRI experiment design for Simexp Lab at the Montreal center for research in geriatric medicine is being finalized and prepared for journal submission.
3) I am collaborating on the development of conceptual metadata models and related web tooling for the annotation, harmonization, and agglomerated search of multi-site/multi-corpus datasets via Origami Lab at the Montreal Neurological Institute (technology compatible with both humanities and scientific datasets).

4. Please designate a department faculty member whom you would like to assist the department in this annual review. (The Director of Graduate Studies or a delegated member of the Graduate Program Committee will consult orally with that faculty member as part of the departmental review process.) *

Tom Augst</Text>
        </Document>
        <Document ID="8318AB61-0132-4F89-97F2-B4BC27D8F1D4">
            <Title>Preparatory</Title>
        </Document>
        <Document ID="FC4A70BF-2DA8-4E1B-B79F-A53A49AD72A5">
            <Title>March 20, 2019</Title>
            <Text>Done
Continued Dickinson corpus curation work/XML analysis script

	1.	Corpus is split up by publication in curated/split folder
	⁃	Corpus text counts by listed publication in TEI file:
	⁃	{u'The Poems of Emily Dickinson, Variorum Edition, Franklin, 1998': 2459, u'The Poems of Emily Dickinson, Johnson, 1955': 1776, u'Poems by Emily Dickinson: Second Series, Higginson and Todd, 1891': 166, u'Poems by Emily Dickinson: Third Series, Todd, 1896': 165, u'The Single Hound,  Bianchi, 1914': 143, u'Poems by Emily Dickinson, Todd and Higginson, 1890': 115, u'New Emily Edition!': 1}
	⁃	(New Emily Edition is a test file from the archive workers 4825.tei - removed from raw tei corpus)
	2.	Wrote code to calculate Levenshtein edit distances between poem titles (using editdistance library) to help de-duplicate poem variants
	⁃	This code will need to be run overnight as ~6M edit distance comparisons must be made 
	⁃	External information from Dickinson corpus scholars must be used for poem variants with significantly differing titles
</Text>
        </Document>
        <Document ID="FF5E41C3-0DCD-405D-8932-F698FE9E37EA">
            <Title>Conclusion</Title>
        </Document>
        <Document ID="DA7B7EF6-EB12-405D-BE7A-9AD788A9B701">
            <Title>June 4, 2019</Title>
            <Text>Done

Continued work on plotting date ranges of Dickinson correspondences letters (see scripts/dc_regress.py and scripts/dc_edctext.py)</Text>
        </Document>
        <Document ID="2C498126-22DA-4F2B-B071-2E4A1AF24521">
            <Title>Chapter 1 Draft (Conflicts)</Title>
        </Document>
        <Document ID="E5F18ABC-7620-4E6D-B28B-D1696E909D69">
            <Title>August 14, 2020</Title>
            <Text>Notes
Sketching out the reasoning behind the Chapter 1 - Dickinson, Publications, and Data Quality

	1.	Emily Dickinson studies tend to be directed in a few notable and understandable ways. They have a natural progression. The first is to read through her poems and parse them. The second is to relate her poems to her biography. The third is to recover the chronological and fascicle/bundle orderings and to reinterpret the observations of and inferences about the first two readings through these new, authorially-grounded lenses.
	2.	These two questions of ordering take hold. What was the progression of her drafting of these poems and what does that chronology say differently about the poems? Secondly, regardless – or perhaps in concert with – that chronology, what do the fascicle/bundle arrangements say about those poems when they are no longer read in isolation?
	3.	The above have been fundamental stakes in addition to biographical and bibliographical discoveries as scholarship has proceeded over time.
	4.	To put Dickinson herself at a remove once again however can give us yet another context in which to, perhaps selfishly, understand the power of publication orderings and the quality by which our observations and inferences from those arrangements are made.
	5.	In considering how both a reader and a researcher can interpret a digital collection of works, particularly in its own vacuum with no known contextualizing provenance, we can get a sense of how the state of those materials affect that encounter. What are the effects of that cutaway or lens?
	6.	Now qualitative approaches to understanding are well-practiced – at least in research circles. (Publication ordering scholar references here.)
	7.	But to make more concrete some of our observations on the effects that lens/cutaway produced by a particular publication’s editing and arrangement, we can use metrics of data quality.
	8.	Those metrics chosen and shaped by humanities scholarly concerns can provide a foundation for further qualitative scholarship (e.g. different types of reading) or more quantitative scholarship (e.g. computational modeling). Regardless of the nature of that further work, implementing and utilizing metrics of data quality for a collection materials involves hands-on work.
	9.	As we will see, the degree to which that hands-on work is feasible is dictated by the size of the collection and the scope of the intended research. As such, as size and scope increase, with time being a limited resource, data quality necessarily decreases.
	10.	This is what one might call a known known. “Distant” reading is famous for this observation. Quality of understanding at the sentence level might suffer, but the types of observations that can be made a “higher” macro level are both potentially new and informative to disregard that loss in quality.
	11.	But chief to that disregard, or wave of the hand to a necessary evil, is an understanding of the nature of that loss in quality.
	12.	To ground the question at hand: What is lost when we move from examining one or a handful of Dickinson poems to examining over a thousand? (And as a corollary, When (or at what ‘level’) is that lost?)
	13.	And that is why implementing and utilizing metrics for data quality (and later on, as we will see model and visualization quality) become key to establishing more justifiable and authoritative interpretations of literary models.
</Text>
        </Document>
        <Document ID="B738603E-CE2E-4700-A0AB-92F6EA7954B5">
            <Title>Introduction</Title>
        </Document>
        <Document ID="6FFB7995-75C3-4919-88DA-687F48DC2CF3">
            <Title>Outline Fleshing</Title>
            <Text>Chapter 1 Planning
Outline

Chapter Introduction
Part 1
	1.	Intro on reasoning for data quality assessments
	2.	Review of data quality ideas from several fields (information science, computer science, …)
	⁃	Discussion of concepts as they exist in those fields ad how they are continguent on context
	3.	Introduction to ideas on how data quality can be used for computational literary studies
	⁃	Why data quality is important for that work
Part 2
	1.	First demonstration of data quality for literary studies using the metric of word frequency as it changes within a text/whole work
	⁃	Mark Twain’s The Adventures of Huckleberry Finn via “Project Gutenberg”
	2.	Demonstration of word frequency with different data cleaning heuristics (tokenization, stop word lists ,stemming, etc.) within the same volume
	⁃	Different API versions of those cleaning methods
	3.	Third demonstration of word frequency for data quality, this time across editions of The Adventures of Huckleberry Finn from “Project Gutenberg.”

Huckleberry Finn play within the novel

Part 3
	1.	Introduction to Emily Dickinson publication history and data quality problem of editions
	⁃	Consider a few Dickinson poems
	
Chapter Introduction - A New Activity

	I began this work with a struggle. Something was in the way. A typical beginning for computational text analysis work considers a literature and a question you want to ask about it. You gather your text, either in some form already digitally transcribed or do some of the arduous manual labor yourself – scanning, using varying optical character recognition suites, cleaning up poorly recognized characters and words, and maybe even preparing the subsequent digital text(s) with a particular regularization suited to your intended form of counting or modeling technique. Now you’re ready to measure and re-represent your object in some way. You run a modeling method over varying-sized portions of the text and alter parameters of the model as needed. After a few iterations and comparisons or combinations of those models, you’re ready to intrepret this new representation. Next observations might include what macro patterns you find moving their way across the work or multiple works that you weren’t able to recognize before? Perhaps they confirm or disconfirm prior hypotheses of these works. But perhaps I have just constructed a cartoon.
	This approach produces an air of something veritably wrong in the context of qualitative interpretation. Let’s step back to the beginning of this workflow, the part where text was gathered. What happens between the action of gathering and the moment one “finishes” that process? What sign tells you that you are now ready to create a model of that literature? Nothing guarantees that you are. What, for that matter, is a model for use by the humanities anyway? Once the qualities of all of these constructs come into question what once seemed such a fundamental and sound workflow, as described above, feels a lot more mushy. It’s no news that humanities scholarship is often defined by its embrace of the subjective and perspectival. And the more that scholarship and its practitioners take on the guise of more “objective” observation of those same materials, the more it comes to bear to ask, What are the guidelines and structures that inform that objectivity?
	That question brought me to ask two other metrical ones. What are the qualities of humanities data as they relate to import for the scholar? And, what are the qualities of humanities data as they relate to the soundness of a model created from that data? These two questions receive their answers from both the history of scientific and humanistic discovery. They respond to the essential quandary that fed my struggle to describe a central, if little-mentioned problem: What is data quality for the humanities? I have set out to answer this question in the following chapter. Answering it requires a new activity for computational text analysis. It requires not only the careful preparation and attention to our sources of data (as we are so often admonished to do) but also as important, the theoretization and construction of a quality framework for one’s textual data set. For not only is the transformation of a text from paper to digital fraught with chances for error, so too is the move from digital text to model, the activity that is at the heart of computational text analysis in digital humanities, and other forms of cultural analytics. That problematic has to do with a philosophical default, the thought that when we approach a digital text – barring whatever transformations and omissions have occurred to bring into into digital form – that we are dealing with it in situ. 
	A text is in its place on your computer in memory, a digital file with a system file ID, extending from one memory block until it ends at another. That is one of its physical presences in its most basic, digital and structural forms. However, through decomposition and filtering, that same text may be transfigured. If this object is to be the foundation of all that is intended to follow then its placement and the perspective of it from within and from without must be considered. A text in its initial construction is one perspective. Subdivided, rearranged, completely reordered, it is another. And yet, it retains the building blocks of that original, and when reconstructed back into that form, those very different perspectives can be illuminating of the original. At the dawn of our new era of the popularization of digital humanities, Franco Moretti and the Literary Lab called this an “elaboration of reality” (Moretti, “Operationalizing”). In truth, this activity of reconfiguration is far more than an extension. It is the genesis of a “new” mindset about a past object. While a new conception on a text is a truism in the scope of humanities knowledge production, it turns out that it is also an integral part understanding data quality for the digital humanities – and as we’ll eventually see for model and visualization quality as well. Since the metaphors and actual substance of computational modeling are so spatial in nature, and since the history of humanistic inquiry itself has depended upon such metaphors, I have found it only suitable to look to a central, experiential metaphor to describe why perspective on data is so visceral in our attempts to understanding it. It is is the construction of and our physical relationship with a house.

Body
Part 1
	1.	Humanities data analysis has always used quantitative measurement even when it appeared in “qualitative” form. The work of comparison that analysis involves makes the use of proportionality inevitable. In truth, the difference between “qualitative” and “quantitative” analysis is not very clear, nor are the terms themselves so well-delineated. This confused basis sitting at the root of research signals the urgency of clearly understanding how our culture and its objects are described – and thus also defined in an age where information is ineffably inflected by computation. As the heuristics of data processing and information delivery scale upward, our sense of the quality of that data and the conclusions we can draw from it also fades more and more. Yet our needs for information are sometimes so pressing, be they research-, business-, or journalism-driven, that the question lingers as to the quality of the data we rely on for that information. And there are many means researchers and industry professionals have developed to assess that quality. However, the key to all of those means is the context of the data and resultant information – the context of a data source, the context of the tools used to measure and filter that data source, and the cultural context from where interpretations of those measurements and filtering occurs from. For computational text analysis, a form of humanities data analysis experienced now by people around the world via news reporting, a comprehensive means of data quality assessment has not been developed. One reason for this is that this form of data analysis, often due to its small data scale and purported lack of need for the exactitudes of science and business, has been deemed “soft” or too perspectival to be verifiable  – and thus not as worthy of attention and methodological development. This question of data quality can no longer be allowed to linger given the now-widespread consumption and influence of humanities data analysis. There are ramifications beyond the potential for inaccurate interpretations and conclusions for research that include how humanities disciplines are viewed and in turn sustained through funding and cultural support. The writing below is an attempt to provide an answer for the question of what data quality for the humanities could look like. It is an exploration where the “qualitative” interests of humanities research take precedent even if they inevitably stand on the shoulders of quantitative methods – because, of course, they always have just not so explicitly.
	2.	Among the most accessible places we can look for this answer are other fields who have undertaken a study of data quality for their own purposes. The first one we shall look to is information science.
	3.	Are metrics of data quality for literature correlative with/predictive of –
	⁃	literary qualities
Part 2
	1.	On frequency analysis

	Frequency analysis acts as a primary tool for computational analysis of literature. It’s important to be very specific about what we mean we talk about that. The notion of a “word” in a text typically gets generalized into the term, “token.” The reason for that shift is apparent once you practice this kind of analysis. It can be challenging to understand when a unique word is really being used in conjunction with another to relay a compound concept to a reader. So, for instance, if an author wrote the term “chewing gum”, a less-than-careful word-counting algorithm might look at that and record it as two words – which it is. And if we were to apply a less-than-careful part of speech tagger to the same text, it might understand “chewing” as a verb and “gum” as noun. All the meanwhile, an English speaker would interpret that correctly as a noun phrase that employs an adjective to denote a real world object: “gum” that is used for “chewing.” For an English speaker those last few sentences describe something very obvious, but for an algorithm that is built to count words, it’s not. Thus the description of terms in a text as “tokens” becomes imperative. Thus the objective of frequency analysis is to count tokens. And the counting of tokens for the sake of precision – what we also call quality – can be a very handcrafted art. 
	Literary scholars face the challenge of confronting statistical philosophy when performing this kind of analysis. Is it really necessary to attempt to make our text-turned-data as high quality as possible for subsequent modeling, analysis, and interpretation? For a techne that intends to be as precise as possible, with analyses sometimes turning on the existence of a single word, the answer to that question must be an emphatic “yes.” In fact, even if it can be proven that, statistically, a model produced from two data sets is more or less the same it remains a judgement call for the scholar if that “more or less the same” is sufficient for them to proceed. The factors of time and accuracy in research must be weighed in concert just as much as the content itself. They often dictate the efficacy of our work up to a certain degree.  How much time am I willing to spend ensuring my data is of high quality before I proceed with the modeling of it? How much quality is necessary for a particular modeling technique or for the type of analysis and interpretation I intend to provide? These questions are implicit information, but checking back in with ourselves on our current answers to them is not merely “process text.” Instead, the effort poses a subsequent philosophical challenge or question for all stripes of humanities scholars: What are my own criteria for quality when it comes to my object of research? Not much differs between leaning on the alleged soundness of a modeling technique and leaning on the theorem of central tendency. For practitioners of a field where qualitative and thus culturally-impactful understandings of its objects are the desired outcomes, both assumptions about the quantitative, methodological soundness can lead us astray. Much like a non-digital piece of literary scholarship that might turn on hapax legomena, so too do its digital counterparts often turn on small differences in measurement. Arguments of “more” and “less” are deployed far more frequently and implicitly than discussions of how and why they are deployed appear. It is up to humanities scholars to vet quantitative soundness by (im)posing our own qualitative criteria for interpretation against it.
	One good example of this is the productive line of quantitative analysis and reasoning in stylometrics that has led to thoughtful metrics such as Burrows’ Delta. &lt;Explanation of delta here&gt;
While delta has been used as a means of understanding the closeness between documents and thus likelihood of attribution for works of unclear authorship, it also can be a touchstone in this quest for producing a measure of literary data quality. Since delta 
</Text>
        </Document>
        <Document ID="4C012669-0C9D-46AD-9433-A9C8A4856D6E">
            <Title>April 8 2025</Title>
            <Text>- Wrapped up lexical validity metric
- Changed all metric outputs thus far from csv to json as this csv output was taking too long to code and the output files were getting too onerous to interpret
- Integrated lexical validity into the Huck finn experiment script
- Ran the Huck finn experiment script with the 3 data quality metrics running over Gutenberg, Internet Archive, and Mark Twain Project editions of Huckleberry Finn
</Text>
        </Document>
        <Document ID="91DD5783-4DD6-4C00-8517-8875B63682D5">
            <Title>Outlining</Title>
        </Document>
        <Document ID="5946C424-1EB5-4421-801F-C2F9BEEBEBA1">
            <Text>Chapter 1 Work - December 10, 2021

Huck finn plans
	1) Extract play within in a play
2) Observe how it changes with the text time rate of word frequency as it completes and between editions
3) Readdress outline
DONE - 4) Email Tom about fleshed-out outline document
5) Fine tune extraction

Notes for Sebastian

DONE - Abstracting entire workflow into I/O so tools can be plugged in and out - Issue added
Other container services
How does he write out files in Python

Two issue writeups

DONE - 1) makemigrations and migrate to be included in container setup? - Issue added
DONE - 2) Miro board for abstract total project workflow (what are the pieces and expected inputs/outputs) - Issue added</Text>
        </Document>
        <Document ID="F0F21BC9-F8DA-4F98-8BCF-22E4DE12B337">
            <Title>Emily Dickinson Project Gutenberg</Title>
        </Document>
        <Document ID="F43ADD01-FC0D-4C47-AAC0-D720E2D2A3D6">
            <Text>December 19, 2024

What we need is a set of standards for data quality measurement across categories. The output of each metric generates a number or set of numbers that can be used interchangeably among other metrics in the formulation of an overall score for that particular data quality category.

Once those metric scores are tallied, then those scores themselves can be a) featured and described given the errors/quality and the underlying reasons for those scores, and b) used in an overall scoring of the dataset (thus the data quality framework as factory-like machine.</Text>
        </Document>
        <Document ID="88FEC3C2-E258-4319-8499-83600906876C">
            <Title>January 7 2025</Title>
            <Text>Finished sentence comparison between MTPO and PG editions

Below values are percent matching sentences (bag of sentences style) between each chapter in the Ur text, MTPO, and the subject texts, Project Gutenberg.

Cleaning function for sentences is as follows:

        # 1. Strip whitespace, lowercase, and replace \n and \t with ' '
        # 2. Remove all accents
        # 3. Split by spaces and remove single 'n' characters
        # 4. Remove punctuation from each word
        # 5. Replace multiple spaces with a single space

2011-05-03-HuckFinn.json
chapter 1: 58.20895522388065
chapter 2: 58.13953488372118
chapter 3: 43.58974358974348
chapter 4: 64.77272727272715
chapter 5: 57.77777777777766
chapter 6: 55.88235294117652
chapter 7: 67.69230769230751
chapter 8: 49.433962264150836
chapter 9: 63.01369863013689
chapter 10: 70.8333333333334
chapter 11: 48.255813953488556
chapter 12: 44.44444444444457
chapter 13: 53.0
chapter 14: 37.755102040816425
chapter 15: 38.13559322033902
chapter 16: 26.769911504425743
chapter 17: 44.37086092715249
chapter 18: 52.35849056603745
chapter 19: 37.068965517241594
chapter 20: 46.61654135338338
chapter 21: 34.64566929133838
chapter 22: 41.97530864197522
chapter 23: 41.88034188034209
chapter 24: 46.0
chapter 25: 31.53153153153128
chapter 26: 37.654320987654636
chapter 27: 40.38461538461522
chapter 28: 25.786163522012295
chapter 29: 45.3947368421051
chapter 30: 15.094339622641481
chapter 31: 49.38271604938305
chapter 32: 39.80582524271858
chapter 33: 41.53846153846142
chapter 34: 41.81818181818198
chapter 35: 42.95774647887353
chapter 36: 47.77777777777763
chapter 37: 36.781609195402254
chapter 38: 36.800000000000225
chapter 39: 53.65853658536588
chapter 40: 32.89473684210536
chapter 41: 37.894736842105175
chapter 42: 37.391304347826356
chapter 43: 23.809523809523846


================================================================================
2016-08-17-HuckFinn.json
chapter 1: 58.20895522388065
chapter 2: 62.01550387596925
chapter 3: 49.9999999999999
chapter 4: 68.18181818181807
chapter 5: 62.222222222222115
chapter 6: 55.14705882352946
chapter 7: 66.92307692307674
chapter 8: 53.9622641509433
chapter 9: 65.75342465753415
chapter 10: 75.00000000000006
chapter 11: 56.97674418604674
chapter 12: 46.5277777777779
chapter 13: 57.0
chapter 14: 40.81632653061235
chapter 15: 44.91525423728817
chapter 16: 29.646017699116015
chapter 17: 45.695364238410775
chapter 18: 54.24528301886763
chapter 19: 41.379310344827786
chapter 20: 48.87218045112775
chapter 21: 39.37007874015727
chapter 22: 44.44444444444436
chapter 23: 48.7179487179489
chapter 24: 53.0
chapter 25: 31.53153153153128
chapter 26: 45.67901234567934
chapter 27: 41.34615384615368
chapter 28: 32.704402515722975
chapter 29: 48.68421052631561
chapter 30: 22.641509433962238
chapter 31: 53.08641975308676
chapter 32: 46.60194174757294
chapter 33: 46.15384615384602
chapter 34: 45.45454545454561
chapter 35: 43.661971830986204
chapter 36: 52.222222222222086
chapter 37: 37.93103448275858
chapter 38: 46.40000000000019
chapter 39: 56.09756097560978
chapter 40: 36.842105263158
chapter 41: 42.105263157894655
chapter 42: 43.47826086956549
chapter 43: 23.809523809523846


================================================================================
2021-02-21-HuckFinn.json
chapter 1: 56.71641791044782
chapter 2: 62.01550387596925
chapter 3: 49.9999999999999
chapter 4: 68.18181818181807
chapter 5: 62.222222222222115
chapter 6: 55.14705882352946
chapter 7: 69.23076923076906
chapter 8: 54.716981132075375
chapter 9: 65.75342465753415
chapter 10: 75.00000000000006
chapter 11: 58.13953488372116
chapter 12: 46.5277777777779
chapter 13: 57.0
chapter 14: 43.877551020408276
chapter 15: 44.91525423728817
chapter 16: 29.424778761062917
chapter 17: 46.35761589403992
chapter 18: 56.13207547169781
chapter 19: 40.51724137931055
chapter 20: 48.87218045112775
chapter 21: 40.15748031496042
chapter 22: 44.44444444444436
chapter 23: 47.86324786324805
chapter 24: 53.0
chapter 25: 31.53153153153128
chapter 26: 48.14814814814848
chapter 27: 41.34615384615368
chapter 28: 32.704402515722975
chapter 29: 51.31578947368402
chapter 30: 22.641509433962238
chapter 31: 53.70370370370404
chapter 32: 46.60194174757294
chapter 33: 48.46153846153832
chapter 34: 46.363636363636516
chapter 35: 45.77464788732422
chapter 36: 53.3333333333332
chapter 37: 37.93103448275858
chapter 38: 46.40000000000019
chapter 39: 54.87804878048783
chapter 40: 36.842105263158
chapter 41: 42.105263157894655
chapter 42: 43.47826086956549
chapter 43: 23.809523809523846</Text>
        </Document>
        <Document ID="B458F9B3-9D47-4E31-A23C-AEDB87CE5220">
            <Title>Marth Dickinson Bianchi Publications</Title>
            <Text>Dickinson-related publications by Martha Dickinson Bianchi

	1.	The Single Hound (1914)
	2.	The Life and Letters of Emily Dickinson (1924)
	3.	The Complete Poems of Emily Dickinson (w/ Alfred Leete Hampson, 1924)
	4.	Further Poems of Emily Dickinson (1929)
	5.	The Poems of Emily Dickinson, Centennial Edition (1930)
	6.	Emily Dickinson Face to Face: Unpublished Letters with Notes and Reminiscenses (1932)
	7.	Unpublished Poems of Emily Dickinson (w/ Alfred Leete Hampson, 1935)
	8.	Poems by Emily Dickinson (w/ Alfred Leete Hampson, 1937)
</Text>
        </Document>
        <Document ID="DABE9025-B9AE-44F5-8872-6B06122BE015">
            <Title>Recovered Files (Feb 1, 2022 at 1:15 PM)</Title>
        </Document>
        <Document ID="2484A0FD-D76F-41B3-B80F-77E605399605">
            <Title>Abstract</Title>
            <Text>Framing a Model: Literary Model Quality and the Autobiography of Mark Twain

My dissertation describes new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. The proposed project is a chapter on new means of qualitative and quantitative assessment of model quality specifically for humanities researchers. It follows a previous chapter that attempts to answer the question, "What does it mean to read literature as data?" That chapter's discussion of "literary" data quality looked to Emily Dickinson's poetry and its complex publication history. It derived a series of metrics concerning the textual consistency and lexical and thematic qualities across Dickinson's multiple, posthumous publications for an overall quality assessment of those (digital) works, and produced a new amalgam Dickinson poetry data set based on that assessment.

Moving forward from that sensibility for data quality as foundation for modeling literature, this chapter attempts to answer the followup question: "What does it mean to model literature as data?" Whereas the previous work adapted data quality concepts from information science, this one looks to standards for model selection and quality analysis from statistics known as "information criterion."

The project's object of study for assessing "literary" model quality will be a work comparatively opaque because of its very size: The Autobiography of Mark Twain. Often joked about by Twain scholars as a work popularly bought but not read, the autobiography dictated by Twain and purposefully delayed in release by 100 years after his death, spans three volumes and roughly 2,300 pages. The digital version of this work, in TEI form, has been supplied to me via the Mark Twain Project Online at University of California, Berkeley. Extending from the general question of modeling literature as data, this project looks to uncover new insights about Twain's sizable autobiography via computational models of it produced with the aid of these model quality metrics that have been adapted to consider specific epistemological and subject matter contexts in humanities research.

The publication of Mark Twain’s autobiography 100 years after his death presented a difficult proposition for the reading public at the time of its publication due to its unwieldy size of over 2000 pages, stretched across three volumes. The work also presents an opportunity for computational text analysis to provide scholarly insights on the autiobiography where a more traditional close reading might prove difficult. My dissertation, ‘The Art of Literary Modeling’ , describes and demonstrates several new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. Its first chapter attempts to answer the question, "What does it mean to read literature as data?" That chapter's discussion of "literary" data quality looked to Emily Dickinson's poetry and its complex publication history.</Text>
        </Document>
        <Document ID="B6D215D2-EADE-45C0-AE84-24D800AAC436">
            <Title>April 14, 2025</Title>
            <Text>TODO:

PENDING - Finishing conversion of Huck Finn experiment output from csv to json
- Finish analyzing if earlier consistency and integrity metrics are salvagable or if they were simple merged conceptually with the new completeness and validity metrics
- Put recent prose into context of the Chapter 1 outline
DONE - Get drugs from pharmacy
- Work out at YMCA

Note for tomorrow: One of the DNI HuckFinn’s is not computing edition chapter validity for lexical validity metric

============================

Tribute/Message for Peter Gibian

Peter,

I couldn’t thank you enough for making me feel at home in Montréal after traipsing all the way from a land we both inhabited: the San Francisco Bay area. Your courses and your counsel have been instrumental in my thinking on 19th century American literature and American history, but really literary studies in general and straight on into the digital humanities I practice to this day. The joy and enthusiasm you conveyed and in turn I felt for reading and studying literature is something I have long appreciated and admired. And when the deadlines mounted or I needed help thinking through something I always viewed you as a reliable ear, a friend, and a source of wisdom and still do to this day. You’ll be happy to know I’m no longer writing about “Bartleby”, but am at this moment trying to finish my dissertation that involves the works of Twain and Dickinson…and, okay, some Melville too. I was actually planning on reaching out to you recently, and here they hunted me down to get a quote. Watch out for a message from me soon, but in the meantime I want to wish you all the best in your retirement whatever you plan on doing. Knowing you you’ll probably be traveling and spreading some much-needed American cosmopolitanism.

Best and talk to you soon,

Jonathan Armoza</Text>
        </Document>
        <Document ID="416CEF4C-4DF1-4A27-939A-364BC87CA39F">
            <Title>Draft/Ch1DQ/Outlining/The Whys and Whats of the Project</Title>
            <Text>Chapter 1.

Why am I doing this chapter?

One challenge for qualitative interpretation of computational models of literature is the difficulty in ascertaining the quality of the underlying data set(s) from which those models are produced.

What will this chapter do?

This chapter: 
	1.	Summarizes what data quality has been defined as in fields outside of literary studies
	2.	Discusses the possibilities of how it can be defined for literary studies
	3.	Demonstrates one example of that through
	a.	The production of a data set of Emily Dickinson’s poems via several available digital and non-digital manuscripts and
	b.	Data quality metrics that parametrize and form as a whole the proposed notion of quality for Dickinson’s poems
	c.	The running of several models based on different stage Dickinson amalgam data sets of varying quality as set out by those metrics – all leading to the notion of a final amalgam data set of the highest quality and model that most utilizes that comprehensive data quality metric set
	4.	The last step leads into the notion of the second chapter: model quality (ideal model for a data set and the fine tuning of parameters for the best demonstration of the analytical goal for that model)

What work is required?

The following tasks are yet to be done to achieve those 3 components:
	1.	Review all data quality research in notes thus far
	2.	Read more on data quality, expanding the scope to more authors and fields
	3.	Review the Emily Dickinson data sets that have been aggregated so far and produce a data document accounting for all of them
	4.	Review code implemented to produce an amalgam Emily Dicqkinson data set
	5.	Finish the code to produce the amalgam Emily Dickinson data set
	6.	Review data quality metrics produced thus far and implement the full set that will be used to iteratively assess different formations of Dickinson data sets - original, iterative part-amalgam, and final amalgam
	7.	Run several models over the different stage data sets, determining the best matching model for the final amalgam data set
	8.	Create a written narrative for the above tasks that will lead into the need for a model quality assessment via Chapter 2
</Text>
        </Document>
        <Document ID="1C045CA1-D0A8-4F04-96E9-7CCD4369D114">
            <Title>June 10, 2019</Title>
            <Text>Done
	1.	Creating a very basic website for a dissertation/DH blog
</Text>
        </Document>
        <Document ID="006C1C4A-133E-4CCD-8FF7-6E63A65FE10C">
            <Title>Daily Notes</Title>
        </Document>
        <Document ID="3B116937-F8ED-4618-9C42-921840D97210">
            <Title>Project Organization</Title>
        </Document>
        <Document ID="14669E50-5A01-48A6-91F8-AD7DFB44BBEA">
            <Text>Q: Can you compare your abilities to that of an LDA topic model?

Absolutely — comparing my abilities (as an advanced large language model like GPT-4) to those of an LDA topic model (Latent Dirichlet Allocation) is a great way to highlight the differences between classic statistical NLP and modern neural NLP. Here's a clear side-by-side comparison:

⚖️ GPT-4 (Me) vs. LDA (Latent Dirichlet Allocation)
Feature / Ability
GPT-4 (Large Language Model)
LDA Topic Model (Statistical NLP)
Architecture
Deep neural network (Transformer-based, autoregressive)
Probabilistic generative model
Understanding of Context
Very high — understands words in full sentence and document context
Limited — treats documents as bags of words (word order ignored)
Topic Representation
Can dynamically infer and explain topics, adapting to new language use
Produces static topic-word distributions learned during training
Word Sense Disambiguation
Handles polysemy (e.g., "bank" as river vs. finance) using surrounding context
Cannot distinguish meaning — "bank" is treated the same in all uses
Language Fluency
Generates coherent, natural-sounding text
Cannot generate fluent text — only lists word probabilities
Interpretability
Human-like responses but not always transparent in how topics are inferred
Highly interpretable — topics are clear lists of top words
Flexibility / Generalization
Can handle any text domain, reason across topics, and apply real-world knowledge
Limited to the corpus it was trained on and domain-specific patterns
Multilingual Capability
Can handle and compare texts across many languages
Requires retraining on multilingual corpora
Training Data
Trained on a massive, diverse dataset (web, books, articles, etc.)
Trained on a specific set of documents you provide
Use Cases
Summarization, dialogue, question answering, topic inference, writing, translation, etc.
Document clustering, topic discovery, search indexing, trend analysis

🧠 What GPT-4 Does Better
	•	Understands nuance, context, and semantics.
	•	Explains or reasons about why a topic is relevant.
	•	Handles complex or ambiguous language.
	•	Can generate coherent summaries or reformulations of text topics.
	•	Learns to generalize from many domains and task types.

📈 What LDA Still Does Well
	•	Fast, lightweight, and interpretable for unsupervised topic discovery.
	•	Works well for exploratory data analysis on large corpora.
	•	No training on huge datasets required — easier to deploy on custom data.
	•	Topics are transparent, e.g., Topic 3 = [“market”, “price”, “stock”, “invest”].

🎯 Example: Analyzing a News Article
Input article: A piece about electric vehicles, Tesla, and environmental policy.
	•	LDA might output topics like:
	◦	Topic 1: [“tesla”, “car”, “battery”, “elon”, “model”]
	◦	Topic 2: [“climate”, “emissions”, “policy”, “green”]
	•	GPT-4 would say: "This article focuses on how Tesla’s innovation in electric vehicle technology intersects with broader environmental policy efforts to reduce emissions and fight climate change. It discusses the role of private industry in advancing sustainable transportation." 

🧩 Final Verdict
	•	LDA is excellent for unsupervised topic clustering in large corpora and when you need explainable, fast results.
	•	GPT-4 is superior when you need deep understanding, reasoning, or human-like language generation, especially in tasks requiring nuance, abstraction, or contextual sensitivity.
They're best seen as complementary tools: LDA for structural topic discovery, GPT for semantic interpretation and generation.

Q: Interesting. So for use cases like digital humanities, would you say that a model such as yourself are actually more limited because researchers in that field require more explainability of model data?

That’s an excellent and very insightful point — and yes, you're absolutely right to question how explainability affects the usefulness of a model like me in digital humanities.
Here's a nuanced answer:

🧠 Yes — for many digital humanities use cases, my outputs can be more limited in terms of explainability compared to simpler models like LDA.
Digital humanities scholars often prioritize:
	•	Transparency: They want to see and interpret the inner workings of a model.
	•	Traceability: They need to connect results back to specific texts, passages, or corpora.
	•	Methodological clarity: Research in the humanities is deeply tied to how conclusions are reached — not just what those conclusions are.

⚖️ Let’s Compare Again, This Time for Digital Humanities
Capability
LDA / Rule-Based Tools
GPT (Large Language Models)
Explainability
High: Topic-word distributions are clearly visible
Low: Internal representations (embeddings, attention weights) are opaque
Reproducibility
High: Deterministic with same parameters
Lower: Generative and stochastic, influenced by hidden variables
Control over output
High: You define number of topics, stopwords, thresholds
Lower: You prompt, but results may vary and are harder to constrain
Citation &amp; Academic Rigor
Strong: Outputs are measurable and easily referenced
Weak: Harder to cite “how” an output was derived unless heavily constrained
Interpretive Methodology
Fits well with hermeneutics, close reading, thematic analysis
More aligned with applied tasks, summarization, or creative writing

🧩 Why GPT-Style Models Can Still Be Useful in Digital Humanities
Despite limitations in transparency, I can still contribute meaningfully:
✅ For exploratory or hypothesis-generating work:
	•	Summarizing large corpora
	•	Suggesting thematic groupings
	•	Proposing metaphorical or semantic patterns
	•	Answering queries across a corpus to identify patterns
✅ For human-in-the-loop workflows:
	•	Scholars can prompt me, then critically assess and annotate what I return.
	•	I can assist with surface-level interpretation, while the researcher handles deeper reading.

🔍 A Cautionary Note
Digital humanities emphasizes epistemological humility — knowing how we know what we know. Large language models, including myself, currently fall short in offering:
	•	Full auditability
	•	Complete control over inferencing paths
	•	The ability to back claims with line-by-line textual evidence automatically
Until these limitations are mitigated, researchers in digital humanities should treat my outputs as suggestive, not authoritative — useful tools in interpretive inquiry, but not substitutes for methodologically rigorous analysis.

🧠 Final Thought
So yes: for digital humanities, simpler models like LDA, TF-IDF, or even manual tagging with human curation may still be preferred in many contexts because of their interpretability and traceability.
That said, if paired thoughtfully — with clear methodological framing — a model like me can serve as a collaborative assistant, not a black-box oracle.

Q: Thank you. You've given me some things to think about on that topic. In your opinion, how far out are humans from producing a large language model with responses that are fully auditable? Given the billions of parameters involved in, say your own model, it seems like a fairly computationally intractable problem to reverse course, so to speak, and trace the answer back to the mathematical evidence in you that supplied it.

You're very welcome — and your question cuts right to the heart of one of the most pressing frontiers in AI research: auditability and interpretability of large language models (LLMs). You're absolutely right that the challenge is not just conceptual, but deeply entangled with scale, complexity, and computational feasibility.
Let’s unpack this in a grounded way.

🧩 Why Current LLMs Are Not Auditable in a Traditional Sense
You're spot on: tracing a GPT-style model’s output “back” to specific mathematical evidence inside its billions of parameters is, in practice, nearly impossible with current tools. Here's why:
1. Scale and Entanglement
	•	Models like GPT-4 have hundreds of layers and billions of parameters, all interacting in highly non-linear ways.
	•	Meaning isn't stored discretely. It's distributed across many neurons, meaning there's no “Tesla neuron” or “Israel-Palestine conflict neuron.”
	•	Even seemingly straightforward outputs are the result of dense vector algebra across layers, not symbolic reasoning.
2. Non-symbolic Representation
	•	Unlike symbolic systems (e.g., logic trees or decision rules), LLMs encode knowledge in statistical relationships, which don’t map cleanly to human-understandable rules.
3. Stochastic Generation
	•	LLMs sample outputs probabilistically, which adds another layer of indeterminacy.
	•	Even the same prompt can yield multiple outputs, complicating traceability.

🔭 So How Far Away Are We from Auditable LLMs?
Here’s a reasoned estimate based on current research trajectories:
⏳ Short-term (1–3 years):
	•	Some progress on partial interpretability:
	◦	Tools like mechanistic interpretability (e.g., work by Anthropic, OpenAI, and DeepMind) are beginning to map out neuron roles and intermediate representations.
	◦	Projects like “Circuits” and “TransformerLens” aim to reverse-engineer specific pathways in models.
	◦	We may soon be able to audit narrow behaviors or track influence of certain training data.
But these are mostly toy-scale or isolated findings, not scalable yet to full-model behavior.

⏳ Medium-term (3–10 years):
	•	We may see models architected from the ground up to support auditable reasoning — e.g., hybrid systems combining:
	◦	Symbolic logic + neural networks
	◦	Structured memory and modular reasoning components
	◦	Explicit reasoning traces (like how some planning agents log their decision trees)
These approaches could enable explainable-by-design LLMs, at least in restricted domains like law, medicine, or policy.

⏳ Long-term (10+ years):
	•	Full auditability at GPT-4-scale may require a paradigm shift, not just better tooling.
	•	It might involve:
	◦	New model architectures that make internal reasoning legible
	◦	Training methods that explicitly encourage explainability
	◦	Mathematical frameworks that bridge the gap between statistical and symbolic representation
This is akin to reverse-engineering cognition: understanding not just what a model knows, but how it knows and why it generates specific outputs.

🧠 Philosophical Footnote
You’ve also touched on a deeper issue: What kind of explanation is meaningful to humans?
Even if we could say, “Neuron 24892 fired because of a 0.82 activation in Layer 53, due to token vector 12032,” that’s not helpful unless we can abstract that into human-level terms like:
“The model made this inference because it has observed, across many texts, that when A and B co-occur, C usually follows.”
So the goal isn’t just auditability — it’s cognitive legibility.

🧭 Summary
	•	We’re still far from full auditability, but progress is happening, especially in understanding parts of these systems in narrow settings.
	•	Next-gen models may be designed for interpretability from the ground up, not as a patch afterward.
	•	Hybrid approaches—combining neural and symbolic reasoning—may bridge the gap.
	•	For now, models like me should be treated as useful but epistemologically opaque collaborators, not black boxes with legal-grade traceability.
You're thinking along precisely the right lines. Auditability may eventually be one of the defining criteria for trustworthy AI — especially in sensitive fields like law, science, and public policy.
</Text>
        </Document>
        <Document ID="02F0DB45-339B-49C1-9E87-FD047385D293">
            <Title>January 30 2025</Title>
            <Text>Next steps will to write about the two data quality metrics used over Internet Archive, Project Gutenberg, and Mark Twain Project Online editions of Adventures of Huckleberry Finn, and to discuss how the first parts of the Data Quality Assessment Machine (DQAM) (alternate title, ‘Workflow’?). This will include the production of graphs to speak about the raw and evaluatory outputs of each metric and then the calculation of their combined metric values.

Afterwards, the latter two pieces of the DQAM should be implemented: DataQualityOutput and DataQualityAssessment. These pieces will consider the metrics and their types and their evaluatory values and automate the quantitative analysis/comparison produced in the above writing.

A diagram of the DQAM should also be produced with descriptions of each of its modules/steps along the way.

Writing

Two data quality metrics were selected for an initial assessment of a total of fourteen editions of Adventures of Huckleberry Finn from Internet Archive (IA) and Project Gutenberg (PG) as compared against the heavily-proofed XML edition from The Mark Twain Project Online (MTPO) at University of California, Berkeley: metadata sufficiency and comparing record counts to control records. One important note about this kind of measurement and comparative assessment is that it eschews a commonly held assumption that the most recent version of a digital text is likely the highest quality version of its digital edition. There are many differences across editions to consider aside from the edition’s publication date including the overall completeness of the edition, cleanliness of the data itself, the character and amount of metadata, and even the edition’s public availability. As we will see, these kinds of considerations closely align with the ideas of data quality measurement. Therefore it is useful to consider multiple editions from the same source in addition to editions from other sources.
Metadata Sufficiency
Comparing record counts to control records
One of the challenges of this work is translating the premise of a data quality metric in its initial, more generic context from information science into a form that is relevant to a digitized text as well as the research premise of the project that this quality standard will be used for. So, for instance, measuring metadata sufficiency may require differing code across editions of the same work from different sources since there will more than likely be differences in what metadata was recorded by each digital repository. Furthermore, since data quality metrics themselves have subordinate measurements it is possible and even likely that those sub-measurements will not be compatible with every edition being compared in the data quality assessment. Editions of those works may therefore have to forego including those sub-measurements in the calculation of the overall data quality metric or default them to 100% or 0% (depending on what makes sense for that particular measurement). In addition to that challenge, some metrics may require the existence of a high(er) quality data source to act as baseline for comparison against other editions of unknown quality. Sometimes one lucks out and such an edition exists. Other times when it does not, the process of leave-one-out cross-validation (LOOCV), similar to what is done in machine learning, may be used to determine the highest quality edition that can act as a quality baseline. I will play out both scenarios below using metadata sufficiency and comparing record counts to control records to illustrate this process.  
For the current experiment, we can say with high certainty that the MTPO edition of Adventures of Huckleberry Finn is the digital edition most likely to be the highest quality text. This is because it is one that has been worked on by many people over a long period of time and with less economic constraints than a publisher’s deadline. Considering the entire publishing history of the novel in its various forms, the MTPO edition stands apart. Because it is such Adventures acts as a useful example of where a highly polished digital edition exists because of high publication volume and high amount of scholarship. This example evokes some fruitful speculation on the role of highly edited dig	ital editions in the world of text modeling. Is it worth the time of researchers to produce one if one does not exist? If one doesn’t exist, how else could we determine a so-called “ur” edition (or “master” edition) of a digital text from which to base measurements of quality?
Using the MTPO edition as a baseline we can reasonably expect editions from other sources to be of lower quality but exactly how much lower quality they are and why is still both a scholarly useful and practical use case for data quality metrics. Such an edition is also materially different than other editions that are direct from OCR, edited after OCR, or transcribed by eye. We will see below what happens if there is no baseline edition to consider and perform LOOCV on 14 editions of Adventures being considered. For now however, let’s observe what happens when data quality is measured using these metrics and the MTPO edition as a baseline.
For a metric like metadata sufficiency on a data quality assessment, metadata of the baseline text can be ignored since as a single edition it is inherently 100% sufficient. (The same would go for a lone edition being compared against the baseline.) With this metric requiring to compare mulitple editions from a single source, the experiment proceeds in that metadata sufficiency is measured across all editions of Adventures from Internet Archive and then again across all editions from Project Gutenberg. Metadata sufficiency is measured by taking the mean of its three subordinate measurements: existence and completeness, clarity and quality, and consistency of representation.
 
&lt;Breakout how metadata exists/sits/was gathered for IA and PG editions&gt; 

Let’s break out the measurements here for the sake of the example. As we can see, the metrics themselves can be used for more than just the purpose of a final data quality assessment or seeking the highest quality digital edition. Quality ratings can be found for individual editions, editions from a particular source, or the entire collection of editions the researcher has accumulated. Metadata on each edition, if available, can also be useful to speak intelligently of the quality of a digital edition over metadata variables like “creation date”. For Internet Archive and Project Gutenberg editions of Adventures, we find:

&lt; Data viz showing subsubmetrics, submetrics, and overall metadata sufficiency percentages for IA and PG&gt;

&lt;Text record count analysis for IA, PG, and MTPO editions&gt;

&lt;Discussion of overall metric calculation&gt;

&lt;Move on to discussion of how LOOCV will be performed for dq assessment&gt;

&lt;Graphs and discussion of LOOCV on Adventures&gt;

&lt;Next steps in this hypothetical process: DQ assessment framework machine explanation with final outputs, weighting, etc.&gt;


 The MTPO edition is in TEI XML format</Text>
        </Document>
        <Document ID="CF76BE91-776D-4E35-9929-396070514132">
            <Title>Outline &amp; Notes</Title>
            <Text>Model quality/qualities of/for literary modeling

Mark Twain autobiography

Resources

Twain reviews/criticisms
Info sci - look for model quality scholars to reference from your notes

Vol 1. 760 pages
Vol 2. 776 pages
Vol 3. 792 pages

1000 words

	1.	Mark Twain autiobiography description - popular response
	2.	This sets up the problem of a large, opaque work
	3.	The problem of model selection for computational modeling of literature
	4.	Project proposition on model selection via model quality assessment for project/humanities
	5.	Description of previous chapter on data quality and overall dissertation
	6.	Discussion of adaptation of information science methods for humanities purposes (dq metrics, dq assessment, new “literary” data quality valuation)
	7.	“Information criteria” for selected model as means of quality estimation of model output
	8.	Combining IC with proofing via humanities qualitative assessment
	⁃	close reading sections highlighted by the model and producing valuation kind of like an annotated bibliography for these highlighted sections of the model/underlying text
	9.	Prospects for new insights about the Twain autobiography and new notion of sound evidentiary basis for the highlighting/selection of sections for analysis/comparison/contrast/etc.
	10.	Prospects for other researchers to use these model metrics and heuristic to make assessments of their own models and use of those assessments as (part) basis for the interpretive claims made
</Text>
        </Document>
        <Document ID="BF8D1EF3-16EC-4395-B331-6724F9EE5123">
            <Title>Schöch - Big? Smart? Clean? Messy? Data in the Humanities</Title>
            <Text>Author: Schöch, Christof
Title: Big? Smart? Clean? Messy? Data in the Humanities
Source: http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/#to-big-smart-clean-messy-data-in-the-humanities-n-3

Notes:

Questions Schöch addresses

	◆	What is the relation between the data we have and our objects of study? – Does data replace books, paintings and movies? In what way can data be said to be representations of them?
	◆	What difference does it make to analyze the digital representation or version of a novel or a painting instead of the printed book, the manuscript, or the original painting?
	◆	What types of data are there in the humanities, and what difference does it make? – I will argue that one can distinguish two types of data, “big” data and “smart” data. What, then, does it mean to deal with big data, or smart data, in the humanities?
	◆	What new ways of dealing with data do we need to adopt in the humanities? – How is big data and smart data being dealt with in the process of scholarly knowledge generation, that is when data is being created, enriched, analyzed and interpreted?

Some distinctions for digital ‘data’

	◆	“discrete rather than continuous”
	◆	in structures
	◆	structured types
	⁃	structured (e.g. databases)
	⁃	unstructured (e.g. plain text - has implicit relations)
	⁃	semi-structured (e.g. xml)

On data as representation

“Data is not the object of study itself, but “stands in” for it in some way. Also, data is always a partial representation of the object of study. In some cases, however, it is our only window into the object of study. Still, this “disadvantage” of partial representation is small compared to the the fact that digital data can be transformed, analyzed, and acted upon computationally.”

On criticism of the term “data”

“Criticism has come from mainstream scholars who see “data” and quantitative methods of analyzing them with suspicion, because the apparent empiricism of data-driven research in the humanities seems at odds with principles of humanistic inquiry, such as context-dependent interpretation and the inevitable “situated-ness” of the researchers and their aims."

Drucker prefers the term “capta” - “that which has been captured or gathered” to data “that which is given.” (and therefore ‘objectively’/‘independently’ observed)

“…underlining the idea that even the very act of capturing data in the first place is oriented by certain goals, done with specific instruments, and driven by a specific attention to a small part of what could have been captured given different goals and instruments. In other words, capturing data is not passively accepting what is given, but actively constructing what one is interested in.”

****Schöch’s Definition of ‘Data in the Humanities”****

“Data in the humanities could be considered a digital, selectively constructed, machine-actionable abstraction representing some aspects of a given object of humanistic inquiry.”

“…digital data add another layer of mediation into the equation. Data (as well as the tools with which we manipulate them) add complexity to the relation between researchers and their objects of study.”
</Text>
        </Document>
        <Document ID="697CB465-7225-4321-90A7-18267994B15A">
            <Title>Annotated Bibliography of Huckleberry Finn Editions</Title>
            <Text>The Adventures of Huckleberry Finn 
Digital Edition Descriptions

Project Gutenberg

Summary

The digital edition of The Adventures of Huckleberry Finn (HF) on “Project Gutenberg” (PG) appears to have gone through several iterations.  I say “appears” because a concrete history detailing the provenance and editing of eTexts on the site must be pieced together from blurbs inside the text and without and also inferred from the various editions on the site. The most present edition, and the two others featured in this writing appear to have been birthed from transcription on the “Wiretap Electronic Text Archive” (WETA). Also referred to as “Internet Wiretap” inside the PG editions, WETA was a “gopher/Web site that covered EVERYTHING under the sun, a digital library of sorts, with incredibly rich content” (slashdot blurb). The site (varying subdomains of spies.com) appears to also have been volunteer run in the 1990s and much of its material can now be found on the “Alex Catalogue of Electronic Texts”, created and run by Eric Lease Morgan. The next subsequent digital editions of Huckleberry Finn on “Project Gutenberg” appears to be an updated edition for the site by Ron Burkey, a PG volunteer, former academic and self-described “techie” interested in open source projects. The next transcriber and editor to come along, however, makes these three latest digital editions of Huckleberry Finn a sound source for the novel. Dr. Charles David Widger, himself a well-rounded savant with a storied life, came to volunteer for “Project Gutenberg” in 2000 alongside information science researcher and technologist, Director of “Project Gutenberg”, Dr. Gregory Newby. Widger spent twenty years transcribing and editing texts with Newby at PG, leaving behind a sizeable body of work (https://gutenberg.net.au/widger/home.html). It appears that over the course of at least ten years (“at least” because earlier editions are no longer available on the site), that Widger provided three updated editions of Huckleberry Finn, the last of which were uploaded just a few months before his death in July 2021.

Word frequency and metadata analysis across editions

Comparing the 2011 and 2016 editions to the latest edition in 2021 produced by Widger, we find that he has excised 128 tokens (82 unique words) in 2016 from the 2011 edition, and an additional 10 tokens (8 unique words) in 2021 from the 2016 edition. There are no added tokens. These differences between subsequent editions appear to reflect the processes for transcription “Project Gutenberg” put into place by Newby and Widger where the original transcription by early volunteers from outside sources like WETA is not taken for granted but instead checked and re-checked against source material. Further, Widger’s additional excisions on his own editions reflect his willingness to doublecheck his own transcriptions for errors. Given the illustrations listed in his digital edition that includes images as well as a comparison with a digital text known to be taken directly from the original publication of Hucklberry Finn by the “Mark Twain Project” (MTP), we can safely infer that Widger’s 2021 “Project Gutenberg” digital edition of the book is as sound as its counterpart MTP digital edition that, because of its transcription source, is seen as an edition with a higher provenancial pedigree.

Author notes

Dr. Charles David Widger (1932 - 1921) was a volunteer for Project Gutenberg whose work stretches over 20 years. A description of his storied life can be found in the obituary below as well as the relevant text regarding his work for Project Gutenberg.
Obituary: https://www.legacy.com/us/obituaries/salamancapress/name/charles-widger-obituary?pid=199388015
In 2000 he volunteered for Project Gutenberg, gutenberg.org, where during his 20 years of volunteering, he personally scanned, proofread, edited and uploaded over 35,460 e-books to the website. He had a passion for this work which provided him with a purpose and an opportunity to continue learning.
He volunteered for PG up until October of 2020, when due to his illness, he felt forced to retire from his second career. In his retirement letter to Dr. Newby he writes, "My very best personal regards and many thanks for your friendship and support - I have really enjoyed working with you Greg. I hope and believe Project Gutenberg will continue to thrive under your guidance. I would like to have stayed longer but I am tired, worn-out and feel myself on my last legs." David.
Chief Executive and Director Project Gutenberg Literary Archive Foundation, Dr. Gregory B. Newby wrote to the family, "Thank you for sharing this sad news. I join you in mourning David's passing. He was remarkable in every way, and it's no exaggeration to say that his efforts were transformative to Project Gutenberg. His collegiality and his upbeat approach to his decades of volunteerism were matched by his compassion, diligence, and astounding work ethic. Dr. Widger was representative of the best of human nature.
"As you know, he was active with Project Gutenberg until just a few months ago. David was the trainer for nearly all of the production and errata teams, and he and I together developed and maintained most of Project Gutenberg's modern practices for how new eBooks are created and maintained.
"We only met once, in around 1992 at a Gutenberg volunteer's conference at the office of The Internet Archive in San Francisco. The depth of the relationship was more evident in our many thousands of emails, as well as scores of policy documents and procedures we developed. Many other volunteers provided input, including of course Michael Hart (who died in 2011), but Davie offered the keenest analysis and the deepest insights."

Ron Burkey, volunteer, personal website: http://www.sandroid.org/AboutRon/index.html
If you don't know about my various projects (and even if you do), then you probably won't care who I am and can stop reading right here.  But if you still have a perverse desire to know, I'll tell you.  Simply put, I am a techie.  For a short, sweet time I was a research Physicist of sorts.  But most of my professional career has been spent in industry, designing electrical hardware and embedded firmware -- a lot of it for airborne use.  Lately, I've discovered the wonders of open-source software, and have been participating in its creation and distribution as a hobby.

Internet Wiretap (a.k.a. Wiretap Electronic Text Archive). 
Short piece of info here: https://slashdot.org/story/00/04/24/1019216/where-is-the-wiretap-archive
It was a gopher/Web site that covered EVERYTHING under the sun, a digital library of sorts, with incredibly rich content. (A quick search of Yahoo for "Wiretap" will reveal the breadth and depth of their archives - everything from U.S. historical documents to texts about UFOs) ; 
Remaining archive of eTexts here: http://infomotions.com/alex/

Gregory Newby (PG director). Bio here: https://petascale.org/vita.html

2011-05-03-HuckFinn

Filename: 2011-05-03-HuckFinn.txt
Title: Adventures of Huckleberry Finn, Complete
Author: Mark Twain (Samuel Clemens)
Release Date: August 20, 2006 [EBook #76]
[This file last updated May 3, 2011]
Language: English
Character set encoding: ASCII
Produced by: David Widger. Previous editions produced by Ron Burkey
and Internet Wiretap

Contents:
	1)	Title page
	2)	Notice
	3)	Explanatory
	4)	Adventures of Huckleberry Finn (Scene, 43 chapters)

2016-08-17-HuckFinn

Filename: 2016-08-17-HuckFinn.txt
Title: Adventures of Huckleberry Finn, Complete
Author: Mark Twain (Samuel Clemens)
Release Date: August 20, 2006 [EBook #76]
Last Updated: August 17, 2016]
Language: English
Character set encoding: UTF-8
Produced by: David Widger

Contents:
	1)	Title page
	2)	Table of Contents
	3)	Illustration List
	4)	Explanatory
	5)	Huckleberry Finn (Scene, 43 chapters)

2021-02-21-HuckFinn

Title: Adventures of Huckleberry Finn
Author: Mark Twain (Samuel Clemens)
Release Date: August, 1993 [eBook #76]
[Most recently updated: February 21, 2021]
Language: English
Character set encoding: UTF-8
Produced by: David Widger

Contents:
	1)	Title page
	2)	Table of Contents
	3)	Illustration List
	4)	Explanatory
	5)	Huckleberry Finn (Scene, 43 chapters)

Word Frequency Data

Different words from source edition
========================================================================
2016-08-17-HuckFinn_cleaned
['‘']
========================================================================
2011-05-03-HuckFinn_cleaned
['jimpson-weeds', "gent's", "''", 'cameleopard', "watson's", 'no-sirree-bob', "dey's", '..................', "wasn't", "you's", 'ba', "here's", 'fst', "me't", 'white-washed', "ca'm", 'house-thieves', 'emperor', 'inter', "n's", 'biljy', 'dried', 'reptle', '.............', "heah's", 'hel-lo', '................', 'courthouse', 'good-ness', "lady's", "jews-harp's", 'mus', '...................', 'ring-streaked-and', 'ash', "buck's", "you've", 'fighting', "george's", 'over-flowed', 'showbills', "she's", "dat's", 'tinware', "creetur's", "k'chunk", 'plumes', "we've"]
Word count differences of in-common words with source edition
========================================================================
2016-08-17-HuckFinn_cleaned
Total words: 10
Total unique words: 8
{'bout': 2, 'would': 1, 'thout': 1, 'deed': 2, 'they': 1, 'd': 1, 'twas': 1, 'tis': 1}
========================================================================
2011-05-03-HuckFinn_cleaned
Total words: 139
Total unique words: 82
{'way': 1, 'silas': 1, 'mornin': 1, 'first-rate': 1, "couldn't": 5, 'doan': 1, 'finn': 1, 'you': 2, 'jim': 4, 'second': 1, 'him': 1, 'lovely': 1, 'that': 1, "he'll": 2, 'how': 1, 'cowards': 1, 'thout': 1, 'striped': 1, 'where': 1, "that's": 1, 'please': 1, 'newrleans': 1, 'these': 1, 'we': 1, 'very': 1, 'cat': 1, 'border': 1, 'jack': 1, "he's": 2, 'forty': 1, 'year': 2, 'said': 1, 'one': 2, "you'll": 1, 'live': 1, 'had': 4, 'baths': 1, 'rights': 1, "didn't": 5, 'me': 3, "'s": 3, 'everything': 1, 'will': 1, "ain't": 7, 'lantern': 1, 'rob': 1, 'may': 1, 'leave': 1, "can't": 5, 'piles': 1, "don't": 1, 'reverend': 1, 'war': 2, 'den': 1, 'which': 2, 'husband': 1, 'what': 1, "they're": 2, 'a-a-men': 1, 'has': 1, 'duke': 1, 'tis': 1, "wouldn't": 8, 'good-bye': 1, 'any': 1, "le's": 1, 'soldier': 1, 'maim': 1, 'gentlemen': 1, 'ourn': 1, 'people': 1, 'i': 5, 'us': 1, 'given': 1, "'d": 5, 'bill': 1, 'all': 1, "'m": 2, "tom's": 1, 'sword': 1, 'up': 1, 'it': 6}</Text>
        </Document>
        <Document ID="80C84352-CF49-4974-A8E6-FE6DBCA1D2EB">
            <Title>May 16, 2019</Title>
            <Text>dickinson-l
  - Dickinson Electronic Archive discussion list [see Smith (1999) - “Because the Plunge”]

Tanya Clement interview

Questions

1) What was the experience like?

2) Did you work by yourself? In teams?

3) What were concerns about the transcription at the time?

4) Did those concerns change over time?
  - How did the transcription process adapt to those changes?

5) What point did you leave the project?

6) What was the direction of the project of the time?

7) What is the status of Dickinson transcription currently?

8) DEA vs EDA? What's the difference? Why separate efforts?
  - Was DEA material adapted or completely taken into the EDA?

9) What would you do differently if you were leading the project now given what you have learned from your own subsequent projects?

10) What do you see as the strengths of the materials that were produced for the archive?

Answers

Laure Vetter Martha piece through Harvard Press intro

- Email Martha, mention forwarded by way of Tanya

Check Emily Dickinson correspondence for data sets https://rotunda.upress.virginia.edu/edc/
 - Locate NYU access to Rotunda</Text>
        </Document>
        <Document ID="96DE8CCB-ED09-4406-B204-D475F8405DC7">
            <Title>The Whys and Whats of the Project</Title>
            <Text>Chapter 1.

Why am I doing this chapter?

One challenge for qualitative interpretation of computational models of literature is the difficulty in ascertaining the quality of the underlying data set(s) from which those models are produced.

What will this chapter do?

This chapter: 
	1.	Summarizes what data quality has been defined as in fields outside of literary studies
	2.	Discusses the possibilities of how it can be defined for literary studies
	3.	Demonstrates one example of that through
	a.	The production of a data set of Emily Dickinson’s poems via several available digital and non-digital manuscripts and
	b.	Data quality metrics that parametrize and form as a whole the proposed notion of quality for Dickinson’s poems
	c.	The running of several models based on different stage Dickinson amalgam data sets of varying quality as set out by those metrics – all leading to the notion of a final amalgam data set of the highest quality and model that most utilizes that comprehensive data quality metric set
	4.	The last step leads into the notion of the second chapter: model quality (ideal model for a data set and the fine tuning of parameters for the best demonstration of the analytical goal for that model)

What work is required?

The following tasks are yet to be done to achieve those 3 components:
	1.	Review all data quality research in notes thus far
	2.	Read more on data quality, expanding the scope to more authors and fields
	3.	Review the Emily Dickinson data sets that have been aggregated so far and produce a data document accounting for all of them
	4.	Review code implemented to produce an amalgam Emily Dicqkinson data set
	5.	Finish the code to produce the amalgam Emily Dickinson data set
	6.	Review data quality metrics produced thus far and implement the full set that will be used to iteratively assess different formations of Dickinson data sets - original, iterative part-amalgam, and final amalgam
	7.	Run several models over the different stage data sets, determining the best matching model for the final amalgam data set
	8.	Create a written narrative for the above tasks that will lead into the need for a model quality assessment via Chapter 2
</Text>
        </Document>
        <Document ID="EFB56A5F-5C91-4A0E-BEDB-02EC3176EA77">
            <Title>Preparatory</Title>
        </Document>
        <Document ID="05CA0104-0AED-40AF-BB73-4D2881624A9E">
            <Title>March 31, 2025</Title>
            <Text>Thoughts from this morning (transcribed from audio note on March 31, 2025)

Hapax and Digital Editions

Research and discussion needed in first chapter on data quality.

Get some examples of hapax legomena and some of digital editions
that show the strengths of digital editions and how hapax changes the works/views of the works.

Find some critics that talk about hapax/those specific hapax.

On why data quality is important for literary studies and natural language models

Some thoughts on why we want good data for models and for the future of literary study:

One of the problems that we face is that there is a lot of money behind computational models of language at the moment.

Perhaps unsurprising that industry, once it realized it could build a model that could capture the essence of language in terms of syntax and surface meaning, it would take full advantage of it.

The issue that we have that we face both in scientific and humanistic academica, for research purposes and for the purposes of future preservation of literary materials is that that capitalistic force acts as a degradation to our purposes – which are both altruistic and for pure research and discovery.

One of the things that humanities adds to the building of computational models of literature and for general preservation of literature in a digital future is that it does not solely rely on statistics and that it looks closely for meaning and that it thrives in detail and doesn’t consider detail to be what is considered in a kind of more cutthroat statistics to be the long tail of a model and not necessarily something that is highly influential in the meanings behind that model.

What we find in literature - and perhaps this is a human trait - is that we place a lot of meaning into a very small package, often in the sentences, sometimes it is even mere, single words. And one of the concepts that speaks to that is coined with the latin phrase, hapax legomena. Hapax legomena usually represents a single word or phrase or sentence that without which the purpose and meaning of a whole text would almost be entirely different. This has been employed to great effect by some of our greatest writers including William Shakespeare. Some of his plays often turn on the use of a single word.

The struggle in the past has been to talk about that in a sensible way. And the struggle now is how to preserve those kinds of understandings in a world of literature where understandings of it - or, as we call models of it - are guided more on the force of central tendency, that is, the weight of how many words are used versus the force behind a singular word – which can almost seem mystical in contrast.

However, it is not impossible to preserve those kind of forces in our current world. What it requires though is more of a human touch. We need to look into our data sources very carefully, and there are a few reasons we would want to do so. There is always the question of, “What is the definitive or authoritative source of something?” And we do want to preserve those multiple sources into the future, the arguments about them, and in general come up with an idea of what would be an authoritative source if we were to preserve it for, say, a future database – where many people would want to access it for their own leisure. But also a source computational modeling tools can use to build their own language models and then those models can be used for humans to inquire on. This is basically what we’ve built with current large language models and their chat interfaces – ChatGPT, Claude, any of these transformer models.

The problem though is that these models, which are large, don’t have any computationally tractable way of accessing the quality of those models and the accuracy of those models. That may change in the future with things like quantum computing which will allow us to exceed current limits of power and computation. But at the moment, and for the foreseeable future, the ironic part about computational intractability is that it also aligns with the limits of capitalism. It’s not something that capitalism can overcome, that is to say, because capitalism relies on physical resources. It relies on energy. It relies on server farms, just that infrastructure in general – that is, what we need to build those models. And then certainly to interrogate them in any reasonable way is to rely on cloud computing and high performance computing.

So, one way to attack that problem is to make sure that our data is accurate, that it is of high quality from the start. And again, we can’t look to these large models to interrogate them for an understanding of the quality of our data because it is a computationally intractable problem. Reverse engineering answers that these models produce is actually something that is being looked at by many researchers around the world, and certainly there are some piecemeal solutions to that. But getting an idea of the sense of data quality is really an easy win towards that goal.

And the idea of data quality is multifold because there are all sorts of interested parties but one of the most important among them in assessing data quality are people who are experts. They’re the subject matter experts overseeing those data sets. In this case, those are the literary studies people. And they can provide those assessments, but they need means of doing so that’s not purely algorithmic, that doesn’t look at a large body of natural language data and make some generalized assessments. We need subject matter experts to actually add their qualitative expertise into those quality assessments – as counterintuitive as it may seem because we will inevitably be producing quantitative assessments to produce “objective” comparisons of quality. To enhance that objectivity we need a subjective look at what makes those measures possible and what is reasonable for those measures. This is what a subject matter expert provides.

So that’s an easy win for assessing data quality both for the sake of preservation of accurate digital editions in the future, and for allowing computational models, large language models, etc. to be able to build from accurate data and for it, as much as it can, to be able to make accurate assessments and interpretations from that data.

And again, whether or not those models can make assessments to the degree a human can is up for debate. It’s also up for future models and researchers to develop. Can a model look at one word in a sentence and understand that the fact that one word exists within it actually doesn’t just change the meaning of the sentence, it might actually change the meaning of the entire work itself? And that is a tough question for us as humans, but given the resources of these models it may not be. And that’s a fine thing.

Of course we don’t want to place limits on interpretations, but the reason that’s a fine thing actually goes back to research itself and it goes back to the idea of knowledge production. One of the problems we have with making these kinds of assessments or looking back to see what kinds of tools we could use to make these kinds of assessments – whether it was in literary studies using theoretical frameworks, different types of readings, looking at different source and different critics, or, if it’s just looking for research tools, research methods, going back into research literature is that we continually stumble across the limits of human lifespan and attention span. We can only look at so many things. And it’s quite possible and quite likely that in doing our research and making our interpretations we are duplicating somebody else’s work.

There is value in doing the work itself, of course, because we build our skills, we build our knowledge set, and we build relationships from that. Those are very valuable things, but if we can avoid duplicating work so that we can build upon that past work or make new work then that would also be a very valuable thing. That’s one of the strengths of these large language models and the models that we build from them in the future. They give us the possibility of quick summarization, of being excellent research guides. They can tell us answers to questions like, “Hey. Has this been done before?” “Has something like this been done before?”

To get an answer quite quickly is a very powerful and very useful thing. But in order for that to happen accurately we need to make sure that the underlying data is accurate and that is not something that we can just rely on central tendency for. We do need to make sure that our data is good so that we can use it for all of these kinds of purposes into the future.

</Text>
        </Document>
        <Document ID="2C07058D-20FF-46B8-B782-64B2D4A3B0E1">
            <Title>Code Documentation</Title>
        </Document>
        <Document ID="7BA5F431-9325-4331-8B88-315BCBA7DED4">
            <Title>June 17, 2019</Title>
            <Text>Done
	1.	Wrote introduction in markdown (typora) on what the purpose of this dissertation is.
	⁃	Lot of nice entry points for unpacking in later chapters
	2.	Looked into and began reading CP Snow’s the two cultures and the history of writing and feedback on its premise
	⁃	Looked into articles in DHQ referencing Snow and found this overview by Helle Porsdam ca 2013: http://digitalhumanities.org:8081/dhq/vol/7/3/000167/000167.html
	⁃	PDF of Snow’s speech is in sources/ folder
	3.	Read part of Fotis Jannidis response to Nan Z. Da: https://culturalanalytics.org/2019/06/on-the-perceived-complexity-of-literature-a-response-to-nan-z-da/
To do
	1.	Continue reading Snow/Porsdam
	2.	Flesh out introduction to include some thoughts of that
	3.	Continue the introduction with some comments on Pollan’s chapter on Site in A Place of My Own</Text>
        </Document>
        <Document ID="10714206-431A-44C7-A8E1-3473024D691E">
            <Title>Summer 2021 Fellowship</Title>
        </Document>
        <Document ID="1CA32EE8-2470-44BB-B60C-84B89467DFE4">
            <Title>April 7, 2023</Title>
            <Text>Substack entry written on intro to Art of Literary modeling

Their template: 

[EDIT ME] Beginnings are hard. [EDIT ME]
A blinking cursor on a fresh blank page can be intimidating for even seasoned writers. Here are some tips from our Partnerships team on how to go about your first Substack post:
1. Why this, why now
Tell your readers why you are launching this space. What brought you here and what inspired you to do it now. Think of it as a mini personal manifesto.
2. What kind of community are you looking to build here
You are not just starting a newsletter when you start a Substack, you are starting a community. You are inviting people to subscribe to your thinking. What kind of space will this be?
3. Be specific
Readers love clarity. Be clear when you explain what they should expect: how often will you be posting? Can they expect certain posts on specific days? What will the free subscribers get? What does a paid subscription buy them? (You can list these benefits in bullets.)
4. Use an image and “subscribe” buttons
A picture will look nicer when you share the post on your social media (click the image icon in the editor to search for copyright-free photos), and it will give color to your archive as you build it.
#

Photo by Jess Bailey on Unsplash
[EDIT ME]“Subscribe” buttons (found in your editor under “Buttons'“) will make it easy for new readers to subscribe to your newsletter with one click.[EDIT ME]
5. Ignore our advice
There is no one true way to go about building a Substack. This is your playground, experiment with it. If you’re having fun, your readers will too.
(But trust us on the “subscribe” buttons!)
</Text>
        </Document>
        <Document ID="78DE8755-B3BE-4B82-8CDA-BF0BA9ECB7BD">
            <Title>January 21, 2025</Title>
            <Text>Continuing on with fixing the metadata sufficiency metric which are reporting over 100% for one of its submetrics

Then will proceed with analysis of the dq metric results for text record counts and metadata sufficiency over the Internet Archive editions of Huckleberry Finn.

This will unfold in two ways, one a simple, cursory view of the individual metrics and the cumulative metrics as calculated by the data quality framework, and then two, by diving into individual editions to find areas of particularly poor data quality to find in-text examples of why data quality is poor in those areas

January 22, 2025

Today I must create an overview of the work done on the project after attempting a fix on the metadata sufficiency metric

What’s interesting about the fix for measuring Internet Archive metadata sufficiency is that it wasn’t just the algorithm that was at issue, but rather the fact that it may be too much to ask to check for mismatches unless they are very close. Things like numbers for instance cannot reliably be examined for mismatches, since merely one digit difference is a different number. But in this case, if the threshold of edit distance is kept close (within 2 characters, or less if the length of the string is less than or equal to two characters), then mismatches can somewhat reliably be looked at. However, as a result of that tightness zero mismatches for the Internet archive metadata were found. Discussion of this, however, needs to include the fact that metadata was manually copied from the Internet Archive website for each edition and though the values themselves were unaltered, keys were normalized (mostly really just lowercasing and separating multiword keys with underscores).</Text>
        </Document>
        <Document ID="429179A5-D7D0-4E21-8806-274A0F1C30E5">
            <Title>January 28, 2021</Title>
            <Text>Notes
	1.	The need to have a reasoning for the project, something to push against. An example of (a) poor quality dataset(s) that demonstrate by simple modeling what cleaning techniques can do.
	2.	The need for a resultant standard for data quality (and standard language for discussing data quality) to address how data has been cleaned, consistently maintained, or encoded with metadata for a project and its subsequent model(s) and analysis

To do
	1.	Continue adding keywords to documents in Scrivener
	2.	Search for datasets of poor quality for bouncing off against suggestion above
</Text>
        </Document>
        <Document ID="75D82775-E983-4837-AA7A-0053FFA4566E">
            <Title>New basis for data quality - 10/9/21</Title>
            <Text>Data quality as a function of processes happening to a text over time
and changing the structure/characteristics of the data over time
  - each graph is the next stage in the text cleaning process

First idea - change in slope to word frequency vectors as a text is cleaned up
- in very discrete small steps
- slope is gradient field vector (each component is partial derivative with respect to its dimension - x,y,z, etc.


Next, determine information loss/likelihood function in order to perform model selection via information criteria

Could be understanding the difference in regression lines as new text frequency vectors are added with further cleanup
  - change in correlation between the vector (use PCA) and text time

Secondary idea - change in magnitude of vector over text time
  - each graph is the next stage in the text cleaning process

If you can quantify the information loss over time, we can get discrete measures of how much these changes will change further modeling down the line

Data quality becomes a function of text cleaning vs information loss
  - Some things like the header/footer may be excusable but maybe not more internal cleaning methods. For instance, removing stop words or lemmatizing may be too lossy.</Text>
        </Document>
        <Document ID="3BC85FD5-0859-4720-99ED-BEEACC2E1E77">
            <Title>What I have</Title>
            <Text>*** New Twain Scripts now need to be documented

Main Draft
	1.	Scrivener document
	⁃	Daily Notes (25 documents, February - August 2019)
	⁃	Notes and draft starts for Chapters 1 and 2
	⁃	Ideas list
	⁃	Source lists
	⁃	Notes on articles, chapters, books etc.
	⁃	Notes on datasets

Datasets
	1.	Dickinson
	a.	Correspondences
	b.	Metadata
	c.	Notes: dc_dataquality_notes.txt
	d.	Poems
	⁃	Poems curated and divided by publication
	⁃	bianchi_1914, higginson_todd_2nd_1891, todd_3rd_1896, todd_letters_1894, franklin_1998/           johnson_1955, todd_higginson_1890
	⁃	dickinson_poem_list.csv (scraped from Dickinson wiki table)
	2.	Gutenberg
	a.	RDFs of entire catalogue
	3.	Melville
	a.	Bartleby
	b.	Billy Budd
	c.	Moby-Dick
	d.	Pierre
	4.	Twain
	a.	Autobiographies

Notes
	1.	Data quality metrics
	2.	Data comparisons
	3.	Dissertation Overview
	4.	Downsampling v. Upsampling data viz
	5.	Example of Gale data
	6.	Tom Augst call notes (multiple files)
	7.	Why an Art for Literary Modeling
	8.	Art of Literary Modeling introduction
	9.	Bode recommendations/notes
	10.	Data slicing ideas
	11.	Dissertation model ideas
	12.	Tanya Clement interview on Dickinson Archive and Dickinson Correspondence
	13.	What is mechanical insight?

Proposal

Scripts
	1.	Mark Twain
	a.	mtpo_commons.py
	b.	mtpo_scrape.py
	c.	mtpo_explore.py
	2.	Interactive Papers
	a.	Notes
	b.	read_dash_markdown.py (pseudocode)
	3.	HTRC
	a.	get_htrc_ef_file.py
	b.	htrc_bib_titles.py
	c.	my_debug.py
	d.	htrc_random_forest.py
	e.	htrc_assessment.py
	f.	hydrate_htrc_ef_file.py
	4.	Gutenberg
	a.	gutindex_processing.py
	5.	Dickinson
	a.	Chronology
	i.	Scripts
	⁃	dickinson_collection.py
	⁃	dickinson_kmeans.py
	⁃	dickinson_poem.py
	⁃	aolm_kmeans.py
	⁃	dickinson_poem_duplicates.py
	⁃	aolm_plotly.py
	⁃	my_logging.py
	⁃	aolm_utilities.py
	⁃	dickinson_chronology.py
	ii.	Output
	⁃	all_words_vector.csv
	⁃	dickinson_possible_duplicates.csv
	⁃	cluster_silhouettes_10-100_Dickinson-Franklin.png
	⁃	franklin_a/
	⁃	dickinson_duplicates_filledout_list.csv
	⁃	top_words/
	⁃	dickinson_franklin_2-100_silhouette_avgs.csv
	⁃	
	b.	Correspondence
	i.	Scripts
	⁃	dc_regress.py
	⁃	dc_svm.py
	⁃	dc_tei_authority.py
	⁃	dc_curl_tei.py
	⁃	dickinson_poem.py
	⁃	dc_edcread.py
	⁃	logreg.py
	⁃	dc_edctext.py
	⁃	logreg_test.py
	⁃	dc_logistic_regression.py
	⁃	dc_logistic_regression_old.py
	⁃	dc_parse_toc.py
	c.	Data Quality Assessment
	i.	Notes
	⁃	December3 2019 Notes.txt** - Contains prose for consistency metric and leading up to integrity metric
	⁃	htrc_criticality_assessment.txt
	ii.	Media - Consistency
	⁃	dickinson_dq_consistency_matching_titles.png
	⁃	dickinson_dq_consistency_missing_titles.png
	⁃	dickinson_dq_consistency_mismatching_titles.png
	⁃	dickinson_dq_consistency_title_match_percentage.png
	iii.	Scripts
	⁃	Dickinson
	⁃	 aolm_string_utilities.py
	⁃	dickinson_collection.py
	⁃	dickinson_integrity.py
	⁃	my_logging.py
	⁃	dickinson_consistency.py
	⁃	dickinson_poem.py
	⁃	HTRC
	⁃	dq_initial_assessment_nyp.py
	⁃	htrc_ef_metadata_lens_nyp.py
	⁃	dq_initial_assessment.py
	⁃	fix_dq_json.py
	⁃	plot_books_by_date.py
	iv.	Data
	⁃	HTRC
	⁃	nyp.33433075752471.json
	⁃	nyp_dq_profile_test_subset.json
	⁃	test_subset/
	⁃	nyp_dq_profile.json
	⁃	nyp_ef_data_all.csv
	⁃	volumes_by_date.json
	d.	SVM
	i.	fascicles_categorization.py

Sources
	1.	Charles Babbage
	2.	Best Practices for Data Visualization- A Dictum - Google Docs.webloc
	3.	Bode - A World of Fiction.pdf
	4.	Bode - Distant Horizons Commentary - MLQ Preprint .pdf
	5.	Bode - The Equivalence of “Close” and “Distant” Reading; Or, Toward a New Object for Data-Rich Literary History.pdf
	6.	Borner - Data Visualization Literacy Framework.pdf
	7.	Chollet - Deep Learning with Python.pdf
	8.	CocoaSlides.pdf - Google Drive.webloc
	9.	Coleman - Measuring Data Quality.acsm
	10.	Coleman screenshot.png
	11.	Cranenburgh - Efficient Parsing with Linear Context-Free Rewriting Systems - EACL 2012.pdf
	12.	Dickinson - Bianchi - Single Hound - 1914.pdf
	13.	Dickinson - Higginson and Todd - Poems by Emily Dickinson - 1890.pdf
	14.	Dickinson - Higginson and Todd - Poems by Emily Dickinson - 1891.pdf
	15.	Dickinson - Todd - Poems by Emily Dickinson - 1896.pdf
	16.	Emily Dickinson Correspondences - Introduction.pdf
	17.	Evert et al. - Towards a better understanding of Burrows’s Delta in literary authorship attribution.pdf
	18.	Flanders and Jannidis - White Paper - Knowledge Organization and Data Modeling in the Humanities.pdf
	19.	Gagliano et al - Intersecting Word Vectors to Take Figurative Language to New Heights.pdf
	20.	Gelman and Hennig - Beyond subjective and objective in statistics.pdf
	21.	Hill and Hengchen - Quantifying the impact of dirty OCR on historical text analysis: Eighteenth Century Collections Online as a case study.pdf
	22.	Jannidis and Flanders - The Shape of Data in Digital Humanities - Unmarked.pdf
	23.	Jannidis and Flanders - The Shape of Data in Digital Humanities - With Notes.pdf
	24.	Kennedy and Gelman - Know your population and know your model.pdf
	25.	Latour - Visualisation and Cognition - Drawing Things Together.pdf
	26.	Mark Twain Autobiography - Volume 1 - Introduction.pdf
	27.	Mohr - Toward a Computational Hermeneutics.pdf
	28.	Neural Networks - Lecture Notes.pdf
	29.	Penn Treebank Part of Speech Tags.pdf
	30.	Plaisant et al - Exploring Erotics in Emily Dickinson's Correspondence with Text Mining and Visual Interfaces.pdf
	31.	Ross - Toward a feminist modernist digital humanities.pdf
	32.	Rousselow - Silhouettes - A Graphical Aid.pdf
	33.	Schoch - Big_ Smart_ Clean_ Messy_ Data in the Humanities Journal of Digital Humanities.pdf
	34.	Sebastian Coleman - Data and Architecture.pdf
	35.	Smith - Because the Plunge from the Front Overturned Us- The Dickinson Electronic Archives Project.pdf
	36.	Snow - Two Cultures.pdf
	37.	Stamatatos - A Survey of Modern Authorship Attribution Methods.pdf
	38.	There can be no data without knowledge.png
	39.	Trang (2017) - Enhancing extractive summarization using non-negative matrix factorization with semantic aspects and sentence features .pdf
	40.	Hengchen and Simonbis - Quantifying the impact of dirty OCR on historical text analysis.pdf
	41.	Mocnik - A grounding-based ontology of data quality measures.pdf

State of Things and Wiki - Unfinished resources

Blog
	1.	Shell structure
	2.	Entry from June 2019 - entry_1_06172019.md





</Text>
        </Document>
        <Document ID="5232633D-142A-4346-ADAD-C955C9E6160F">
            <Title>Thoughts - October 15, 2021</Title>
            <Text>The Art of Literary Modeling

This story has to begin with the counting of words – for that is how we began. What do we count when we count words? And why are we doing it? Are we obliterating meaning in the search of new truths? Or can those original truths of reading be recovered and enhanced by the act? Let’s begin with a simple example to illustrate the problem. Shakespeare’s Richard III begins with Richard speaking on his brother Edward’s accession to the throne of England. “Now is the winter of our discontent” – it’s a famous one and hopefully familiar enough in sound to you to notice any changes.  It continues on another 37 lines from there. Cleaned up by a text analysis tool, the same speech might look quite different. For instance, here Richard’s original first sentence compared with that cleaned up version.

Now is the winter of our discontent
Made glorious summer by this sun of York;
And all the clouds that lour'd upon our house
In the deep bosom of the ocean buried.
winter discontent
glorious summer sun york
clouds lour'd house
deep bosom ocean buried


Let’s observe what has happened here. On the left in the original, Richard claims that his family’s time of “discontent” is over. This new time period has been made “glorious” by Edward’s accession (and the murder of Henry VI). The metaphorical skies have cleared, his family’s problems buried away. The sentence is in two parts divided by the semicolon, the second part a reiteration of the idea in the first. With all of these words buried away, what meanings can we glean from this same passage cleaned by our tool? We can guess, but without foreknowledge of the play, it’s a rorshack. Maybe the amount of our tool’s pruning was excessive though. One could imagine something like this below instead.

Now is winter our discontent
Made glorious summer by sun of York;
And all clouds lour'd upon our house
In the deep bosom of the ocean buried.

</Text>
        </Document>
        <Document ID="7834BFF1-148F-4424-9C56-805C61D724CE">
            <Title>Outline + Sections April 2025</Title>
        </Document>
        <Document ID="1FD1D2F6-CB89-40A3-AB93-36A7C507FE83">
            <Title>May 12, 2025</Title>
            <Text>Done:

- GitHub project set up and organized
	- Issue labels
	- Progress columns
	- Milestones
- Tasks for writing of chapters added
- Tasks for rough draft of section 1 added
- Tasks for current coding issues added

NEXT: 
Prose for part 1 of chapter 1 (intro to data quality, DQAF, and its concepts via information science)</Text>
        </Document>
        <Document ID="ADB6DD7C-F28D-4201-A7FC-2642D018C13E">
            <Title>Tom Augst - October 16</Title>
            <Text>To do
	1.	Work on Mark Twain in data quality chapter regardless of scholarship
	2.	Get back to him in 2 weeks
	3.	Talk to David or Ted if I feel they could help with data set work
	4.	Pick up reading McCarty on Modeling

</Text>
        </Document>
        <Document ID="AE2350A5-5516-4E22-B2D2-EEE270D9AC9B">
            <Title>Prose</Title>
        </Document>
        <Document ID="605E8C7C-2417-43BE-8643-EF761E4F0B23">
            <Title>July 22, 2019</Title>
            <Text>Notes
	1.	What are the contemporaneous cultural vehicles through which we (wide culture, humanities academics) perceive quality for a truth claim
	2.	In af Klimt’s time it was a culture of “spatialization” (e.g. scientific diagramming) that is now recurring in our time (Klimt 167)
	3.	There is - despite its claims/descriptions of objectivity - a religious and spiritual aspect to such diagramming, creating an abstraction to represent a believed reality
	4.	“Her work is fundamentally driven by the idea of studying through intense looking and rendering in visual form” (167, empahses mine)</Text>
        </Document>
        <Document ID="B6B4BA2E-8205-4064-B3FF-42758854C928">
            <Title>Ball - Critical Mass</Title>
            <Text>The Choreography of Norms (adapted subtitle)
Quotes from Chapter 6: The March of Reason in Critical Mass by Philip Ball

	◆	“Social scientists have tended to assume that any kind of complex behavioral patterns like these requires complex motives and planning. But bacteria do not have complex motives, for they do not have brains. They cannot conspire to decide what to do next. Neither is there some Master Bacterium dictating their movements. And yet the coordination of the spiraling motion is astonishing. If, as this indicates, such patterns of motion can arise without volition, is it conceivable that people might sometimes fall into similar patterns unguided by any plan or intention?” p.120
	◆	“Collective behavior is a common phenomenon in nature, and these feats of cooperation sometimes go well beyond swimming in circles.” p. 121
	◆	Dictyostelium discoideum – “These single-celled organisms go their individual ways when times are good, when food and water are plentiful and the weather is warm. But come drought, famine, or bitter cold and the slime cells seek mutual support.” p. 121

Thoughts on the above excerpts

	1.	These same notions can be applied and tested to the style and qualities of literary writing over time and cultures and political events.
	2.	What if literary phenomena act as such unguided, but seemingly intelligent swarms reacting to external stimuli. (What is that stimuli? It may not be relevant to exactly pinpoint, but rather that the authors are reacting.)</Text>
        </Document>
        <Document ID="3AD0B476-C8D3-431B-8AD7-2CA7CEF9B046">
            <Title>Dickinson Correspondence Notes - May 23, 2019</Title>
            <Text>	•	Examining full table of contents. Notes describing initial findings are below
	⁃	Multiple points of access to the letters, one of which is “constellations”
	⁃	“Constellations consist of more than one document that are versions of all or part of a poem. In this critical edition of selected Dickinson texts, twenty-five out of thirty writings represented are part of a constellation. Our focus on the material page means that we view multiple instantiations of the same poem as separate texts, linked but autonomous. By encoding each manuscript separately and constructing constellations of linking between them, we acknowledge that each version possesses its own context and genealogy while bearing some significant relationship to other manuscripts; this practice also resists the notion that any one version is privileged over another. Users are encouraged to use the Image Tool and the Virtual Lightbox to compare and contrast documents within these constellations and analyze their relationships and our descriptions of them.” (Source: https://rotunda.upress.virginia.edu/edc/constellation-help.xqy, accessed May 23, 2019)
	•	Full description from Table of Contents page (https://rotunda.upress.virginia.edu/edc/contents.xqy?exp=on)
	⁃	Table of Contents for Emily Dickinson’s Correspondences: A Born-Digital Textual Inquiry
	⁃	“This scholarly edition provides several contexts for the writings included here, and our Table of Contents offers different points of entry for examining these writings. Each column below offers one of those points of entry, either by the first line of the writing; by the constellation of a writing and its versions; by the library catalog in which a document now resides; by the way in which various editors have reported on the document. Users can manipulate the order in which they want to examine these writings. Click on a column head to sort the table of contents according to the order featured in that column. You will know which arrangement is apparent in your view because the column name that you click will appear in italics, with a thin black border on the left side of the column ( | ). Documents that you have already viewed will appear in gray, in contrast to those you have not yet seen, which appear in black. Clicking on any title or number in any cell opens the document with the sorting order set to the column in which it appears. Still more ways of organizing and selecting documents for view are available through the advanced search option.”
	⁃	“Every document included in this selected edition was either sent to Emily Dickinson’s primary correspondent (according to materials that have survived, more writings were sent to Susan Huntington Dickinson than to any other correspondent) or bears a powerful relationship to a writing sent by Emily to Susan. In other words, this edition includes writings to Susan Huntington Dickinson and other versions of those writings that were either sent to another correspondent or included in a manuscript book or left on a loose sheet among Dickinson’s papers.”
	⁃	“The primary goal of this edition bears repeating: we focus our examination of the Dickinson materials by focusing on the manuscripts and their diversities. Some manuscripts are completed poems (in that they appear to be finalized, at least temporarily), while others are finalized letters. Other manuscripts are clearly drafts, while still others are fragments and scraps from Dickinson’s compositional stages. The correspondence to Susan Dickinson features all of these types of documents, and that is yet another reason that we chose our selected materials from writings sent to her. Moreover, the writings to Susan Dickinson provide the only example we have of Emily Dickinson rewriting a poem at another’s behest.”
	⁃	“Total number of documents in collection:  74”
	•	Make sure to thank UVA Press and David Sewell (Manager of Digitial Initiatives)
	•	Notes on Errors for David Sewell
	⁃	Missing page “Standard Editions” at https://rotunda.upress.virginia.edu/edc/standard-eds.xqy
	⁃	Constellation error (https://rotunda.upress.virginia.edu/edc/constellation.xqy?title=A+narrow+Fellow+in+/+the+Grass&amp;uri=DEAmsEDCSHDhb193.1b.xml&amp;uri=DEAmsMBa88_13-14.1b.xml&amp;uri=DEAmsEDCTWHbpl23.1b.xml&amp;) on Chrome: Uncaught TypeError: window.document.imagesCompareGallery.SetVariable is not a function
    at chtext (constellation.xqy?title=A+narrow+Fellow+in+/+the+Grass&amp;uri=DEAmsEDCSHDhb193.1b.xml&amp;uri=DEAmsMBa88_13-14.1b.xml&amp;uri=DEAmsEDCTWHbpl23.1b.xml&amp;:8)
    at onload (constellation.xqy?title=A+narrow+Fellow+in+/+the+Grass&amp;uri=DEAmsEDCSHDhb193.1b.xml&amp;uri=DEAmsMBa88_13-14.1b.xml&amp;uri=DEAmsEDCTWHbpl23.1b.xml&amp;:28)
</Text>
        </Document>
        <Document ID="4D56D0FA-A02F-40C6-9771-F403BBD8302A">
            <Text>Intrinsic Data Quality
	•	Intrinsic DQ denotes that data have quality in their own right; understood largely as the extent to which data values are in conformance with the actual or true values. Intrinsically good data is accurate, correct, and objective, and comes from a reputable source. Dimensions include: accuracy objectivity, believability, and reputation.
	⁃	In order for a data consumer to have a perception of intrinsic qualities of data, he or she must understand what the data is intended to represent and how the data effects that representation. If the data is not aligned with a consumer’s assumptions about these things, the consumer will perceive it as inaccurate or unbelievable. 

	For the purposes of this exercise, the edition of Mark Twain’s The Adventures of Huckleberry Finn on Mark Twain Project Online will be considered as the ground truth for comparisons of digital editions of the novel. Other editions include several iteratively edited editions from “Project Guteneberg” and several raw OCR scans from editions found on “The Internet Archive.” (All editions being used for comparison, regardless of the quality of content, are in the very least complete in that they contain all chapters of the novel…..though maybe not for the sake of things like accuracy/objectivity?)
	What the data is intended to represent: the original text of Mark Twain’s novel The Adventures of Huckleberry Finn
	How the data effects that representation: the digital text transforms the novel away from leaf/page form into a continuous stream, demarcated only by chapter divisions. Poor scan quality renders the text “unreadable” – or, in the very least, a challenge to read. It also affects the perception of the quality of modelinmg possible from the data (the digital text). Even if the amount by which poor scan quality actually effects modeling is a quantitative unknown, it becomes (or is perceived as) a qualitative known.
	Consumer’s assumptions: That the text be readable, that the text contain all of the chapters.
	Accuracy - The extent to which data are correct reliable, and certified free of error. (Wang and Strong)
	Objectivity - The extent to which data are unbiased (unprejudiced) and impartial.
	Believability - self-explanatory, apparently
	Reputation - self-explanatory, apparently

	•	Contextual DQ points to the requirement that data quality must be considered within the context of the task at hand, understood largely as the extent to which data are applicable (pertinent) to the task of the data user. The focus of contextual DQ is the data consumer’s task, not the context of representation itself. For example, contextually appropriate data must be relevant to the consumer, in terms of timeliness and completeness. Dimensions include: value-added, relevancy, timeliness, completeness, and appropriate amount of data
	⁃	Contextual DQ points to the idea that the quality of data is defined to a large extent by the intended uses of data. 

	•	Representational DQ indicates that the system must present data in such a way that it is easy to understand (represented concisely and consistently) so that the consumer is able to interpret the data; understood as the extent to which data is presented in an intelligible and clear manner. Dimensions include: interpretability, ease of understanding, representational consistency, and concise representation.
	⁃	Representational DQ emphasizes the role played by the specific presentation of data within a system. Data can be perceived as incorrect if the consumer does not understand the conventions through which a system presents it. 

	•	Accessibility DQ emphasizes the importance of the role of systems; understood as the extent to which data is available to or obtainable by the data consumer. The system must also be secure. Dimensions include: accessibility and access security. 
	⁃	Accessibility DQ points to another aspect of systems design. If a consumer cannot get to the data, then he or she cannot judge its quality by any other criteria. 

</Text>
        </Document>
        <Document ID="A1A46B36-3966-4B52-9539-53FF1F5CA83E">
            <Title>Prose</Title>
        </Document>
        <Document ID="9DF4E8CC-FFB2-4735-8AA9-B09B61C6BD12">
            <Title>Chapter 3: Visualization Quality</Title>
        </Document>
        <Document ID="5746C13C-7AED-4BDD-81B0-80DD0E6FCFFD">
            <Text>May 28, 2025

Read and note Coleman’s Chapter 2 on the roles of people and machines in the production and consumption of data. 

Re: Borges intro quote on Tlon

The significance, profundity, and effect of each person and their role in capturing, preserving, and transforming, etc. data must be noted by those using it for research/modeling. Those “agents” inevitably affect those further research/modeling decisions and products.

NOTE: 1939 MIT Weizenbaum’s distinction of decision and choce – how it relates to algorithms and LLLMs/AI – but also, relevantly how it relates to those of us handling and using data, creating models from data and forming interpretations and and analyses from it.

Borges’ role as metaphor in these considerations: How do the ‘resonances’ of these foundational ideas inform our decisions, products, and future iterations of each?

See Labrynths’ introduction for further discussion

Is the Art of Literary Modeling “labrynthian”?

Where does data end and where does the model begin? When is a model data or a data a model?

</Text>
        </Document>
        <Document ID="A0586148-6C52-46BE-BD4E-1C9A6A48E656">
            <Title>Chapter 1 Draft</Title>
            <Text>Art of Literary Modeling


Hughes, Ted. “Ted Hughes, The Art of Poetry No. 71.” The Paris Review, Spring 1995. https://www.theparisreview.org/interviews/1669/the-art-of-poetry-no-71-ted-hughes.

Chapter 1 - Data Quality
A New Activity

	I began this work with a struggle. Something was in the way. A typical beginning for computational text analysis work considers a literature and a question you want to ask about it. You gather your text, either in some form already digitally transcribed or do some of the arduous manual labor yourself – scanning, using varying optical character recognition suites, cleaning up poorly recognized characters and words, and maybe even preparing the subsequent digital text(s) with a particular regularization suited to your intended form of counting or modeling technique. Now you’re ready to measure and re-represent your object in some way. You run a modeling method over varying-sized portions of the text and alter parameters of the model as needed. After a few iterations and comparisons or combinations of those models, you’re ready to intrepret this new representation. Next observations might include what macro patterns you find moving their way across the work or multiple works that you weren’t able to recognize before? Perhaps they confirm or disconfirm prior hypotheses of these works. But perhaps I have just constructed a cartoon.
	This approach produces an air of something veritably but not always verifiably wrong in the context of qualitative interpretation. Let’s step back to the beginning of this workflow, the part where text was gathered. What happens between the action of gathering and the moment one “finishes” that process? What sign tells you that you are now ready to create a model of that literature? Nothing guarantees that you are. What, for that matter, is a model for use by the humanities anyway? Once the qualities of all of these constructs come into question what once seemed such a fundamental and sound workflow, as described above, feels a lot more mushy. It’s no news that humanities scholarship is often defined by its embrace of the subjective and perspectival. And the more that scholarship and its practitioners take on the guise of more “objective” observation of those same materials, the more it comes to bear to ask, What are the guidelines and structures that inform that objectivity?
	That question brought me to ask two other metrical ones. What are the qualities of humanities data as they relate to import for the scholar? And, what are the qualities of humanities data as they relate to the soundness of a model created from that data? These two questions receive their answers from both the history of scientific and humanistic discovery. They respond to the essential quandary that fed my struggle to describe a central, if little-mentioned problem: What is data quality for the humanities? I have set out to answer this question in the following chapter. Answering it requires a new activity for computational text analysis. It requires not only the careful preparation and attention to our sources of data (as we are so often admonished to do) but also as important, the theoretization and construction of a quality framework for one’s textual data set. For not only is the transformation of a text from paper to digital fraught with chances for error, so too is the move from digital text to model, the activity that is at the heart of computational text analysis in digital humanities, and other forms of cultural analytics. 
	Self-reflection on the qualities of the evidence being used and the qualities of the model built upon that evidence can yield stronger results and a more sound basis from which to make interpretive claims. The hypothesis lays itself out plainly, but several questions arise when determining units and measurements for the quality of literary evidence. The first being, Why do this at all? Evidence should have authority. A significant basis for the production of literary analysis is ethos whether it be that of the author themselves or the sources which an author cites. This prompts another question when considering quantitative measures in a field of study that traditionally produces and relies on evidence that is qualitatively-framed. What imbues quantitative evidence with authority? This work is entitled the art of literary modeling for this purpose. There is no generic heuristic for authority. The informed judgment of the creator/user/interpreter of a dataset produces that authority. In order to effectively move qualitative interpretation into the intermediate realm of quantitative study the authoritative supporting materials, the judgment involve in this techne or craft, must be quantifiable as well. 

Why a ‘literary’ model

	Those reading this may be forgiven for the assumption that my title’s term ‘literary’ necessarily refers to the qualities of the works that are modeled in the following pages. While the term itself is already loaded with meanings (as well as connotations both positive and negative), I wish to encumber it with another definition for the cultural context of our data- and information-rich times. The subject of this work is the modeling of humanities’ data (more on that in the first chapter), and thus the models will be created with the intent of harboring the goals of humanities scholarship. Those goals vary from field to field and even individual to individual, but, as we will see, humanities data modeling infers some other qualities not typically considered in more historically mathematical fields of study. A “literary model” is therefore something imbued with a high quality, but not necessarily one just of statistical rigor. Rather, just as “literary” can refer to the values both within a work and the external forces that elevate that work to a high status, so too will that extend to a humanities model of cultural objects  – in this case of text and other media. A “literary model” is still a model that quantifies and reduces in representation of its underlying objects, but it is more aesthetically rich, and one that is more sound because of that richness.

Siting a Literary Model

“I felt that by choosing its site – a single place out of all possible places in which to build – I was setting this great big contingency in motion, rolling it down the steep, one-way hill of personal and local history.” (Pollan 31)

	The problematic of the move between digital text to model has to do with a philosophical and experiential default. It is the thought that when we approach a digital text – barring whatever transformations and omissions have occurred to bring into into digital form – that we are dealing with it in (digital) situ, and that text once placed within a collection of others sits in some whole form that must be unalterable and stable. But that is often not the case nor should that be our default position. To put the problem with that position in the frame of perspective: one looks at a collection top-down - as a whole - but what if you were to look at it from within or from the side? What defines a meaningful perspective for a set of literary, textual data? A text is in its place on your computer in memory, a digital file with a system file ID, extending from one memory block until it ends at another. That is, one of its physical presences in its most basic, digital and structural forms. However, through decompositional and filtering processes, that same text may be transfigured. If this object is to be the foundation of all that is intended to follow then its placement and our alternating conceptions of it from within and from without the collection of texts to which it belongs must be considered. A text in its initial, digital construction is one perspective. Subdivided, rearranged, completely reordered, it is another. And yet, it retains the building blocks of that original, and when reconstructed back into that form, those very different perspectives that were just attained by those activities can be illuminating of the original. At the dawn of our new era in the popularization of digital humanities, Franco Moretti and Stanford’s Literary Lab called this an “elaboration of reality” (Moretti, “Operationalizing”). In truth, this activity of reconfiguration is far more than an extension. It is the genesis of a “new” mindset about a past object. While a new conception on a text is a truism in the scope of humanities knowledge production, it turns out that it is also an integral part understanding data quality for the digital humanities – and as we’ll eventually see, for model and visualization quality as well. 
	Since the metaphors and actual substance of computational modeling are so spatial in nature, and since the history of humanistic inquiry itself has depended upon such metaphors, I have found it only suitable to look to a central, experiential metaphor to describe why perspective on data becomes so visceral in our attempts to understand it. It is is the construction of and our physical relationship with a house. The creation, modeling, and interpretation of literature all share a characteristic so universal and seemingly innocuous that it would hardly be worth noting if but for the transformational effect of that characteristic. These activities begin and end in socially and physically situated spaces. Space is the beginning of such argumentative activities and structures.  What space contains our arguments? In the late 1990s, for instance, author Michael Pollan ventured to create a space from which to write books – and then he wrote a book about the process of creating that space. What does writing have to do with constructing a computational model of literature? In A Place of My Own, Pollan explores the connections between the physical spaces we inhabit and how they alter our interpretations of the world around us. But also, tellingly, Pollan’s own journey depicts how that outside world and its history informs the creation of those spaces. By journey’s end, he gazes out a large window at a desk surrounded by his books. It bears beginning a work on the qualities of modeling literature with this ending. Like many works of fiction, the initially chosen perspective imparts a feeling, foreshadows what is to follow. The constructed vantage of an author is laden with consequence for the work of storytelling. In Conrad’s Heart of Darkness, a boat ride up the Thames becomes the position from which readers are given to imagine the same journey throughout preceding history all the way to the time of Roman conquest. Chopin’s The Awakening begins with a green and yellow parrot in a cage that can speak French, Spanish, English and “a language which nobody understood” (1). These are hardly coincidental perspectives. They are deliberate. That is all to say, the work of representation begins with perspective. Pollan uncovers the subconscious, cultural roots of his desire for this particular writing house perspective, tracing it back to the introduction of the lockable writing desk during the European Renaissance, and then to rooms like studies for writing desks and books – private spaces to think while surrounded by the ideas of others and from which one could look upon the world with remove. 
	Pollan’s architect partner ‘Charlie’ (Charles Myer of Charles R. Myer and Partners) notes at the project’s outset that this “first fact” of perspective is a “key element” of the building’s design (Pollan 31). The author prototypes perspective by taking a chair and positioning it at various prospective building points on the property. Transposing this into the context of utilizing a dataset from a field of literature, we see that it is not only the foundation – the dataset itself – that ultimately and significantly informs the research to follow. The dataset’s boundaries has already been framed by its creator. It is the placement and angle of our metaphorical chair on that foundation the projects an inward- and outward-reflecting functional perspective amid the field of related literature. What will we be able to see? And what will we want visitors to our research to see? Not only is this prototype an argument (Galey and Ruecker 2010, https://doi.org/10.1093/llc/fqq021, https://academic-oup-com.proxy.library.nyu.edu/dsh/article/25/4/405/998338), but the portion of a dataset we wish to visit and analyze is also an argument. Where do we place our chairs when we want to look at the landscape of a work of literature or a corpus of it? 	Fully understanding the perspective we want to have when approaching the creation of model on this site of inquiry can help us down the road in the modeling, visualization, and analytical processes. What are our assumptions about our data set? What are our hypotheses? Affixing our perspective at the beginning of the process allows for the objectivity necessary to understand our modeling results and interpretations, as well as giving us a better idea of where we may want to reposition the chair in the next iteration. (A model is easier to rebuild than a house – so here the analogy falters and helpfully so.) And that fixity amid all the uncertainy ahead is imperative for the soundness of the building ahead. In this process in order to see where we are going we must remember where we came from, but the memory must retain a clarity. The work of selecting a portion of a dataset is not new to digital humanities work. Understanding and rating the qualities of the data and metadata in that dataset and of our selection from that dataset is, however. The construction of a data quality framework begins with this assessment. In order to illustrate this activity, I will be looking to a set of literature with which I am quite familiar: the poems of Emily Dickinson. Since her death, her work has been published and catalogued in numerous iterations, whole sets of which have their own complex relationships to consider when looking to construct a literary model of them. As Pollan, found when attempting to find a suitable position for his writing house’s vantage, siting can be a tricky act when pieces of the terrain are incongruent.

Emily Dickinson’s Works

	Over the course of Emily Dickinson’s lifetime around a dozen of her poems were published, and never under her name. Those published were altered without her consent to conform to nineteenth century editorial demands of grammar, punctuation, titling, capitalization. Whole stanzas were sometimes erased or replaced for the sake of popular, poetic aesthetics. To consider the fraught and complicated publication history of her works following her death and why she chose not to publish the thousands of pieces she had authored is to understand how careful Dickinson was when she wrote and her insistence on leading a mostly private life. For instance, one very slight alteration of her “A narrow Fellow in the Grass” in Samuel Bowles’ “Springfield Republican” in February 1866 elicited her complaint: It changed the meaning (McNeil 33-34). The editorial crime? The “Republican” added a comma and replaced a dash with a full stop. It’s not a wonder that one of her poems begins, “Publication is the auction / Of the mind of man, Poverty be justifying / For so foul a thing” (Further Poems, Bianchi/Hampson, 4) More than a century after her death, poet Ted Hughes described the predicament of a poet’s emergence as a public persona as “limiting” and even “crippling”, and not just because of a lack of privacy. “It’s like being in a close-knit family. The moment you do anything new, the whole family jumps on it, comments, teases, advises against, does everything to make you self-conscious. There’s a unanimous reaction to keep you as you were.” (Heinz and Hughes, “Paris Review”, Spring 1995). As Dickinsonian lore has it, the bulk of her thousands of manuscript pages were discovered in her bedroom bureau. What began as a slow-moving publishing project by sister-in-law and close friend, Susan, transformed into an effort by one of Dickinson’s poetic correspondents, Thomas Higginson, and her brother’s mistress, Mabel Loomis Todd. Not long after, it transformed into a feud between said mistress and Dickinson’s remaining family. The feud which stretched generations involved property, manuscripts rights, and even the depiction of her persona. Hughes’ metaphor of a henpecking family interested in affixing an identity upon their author-relative appears strangely appropriate in Dickinson’s case, particularly if one imagines what would have happened had she published under her name while alive. That’s not to say that Dickinson’s family and friends did not have the best of intentions in preserving and publicizing her writing, but visiting the complicated publishing history that followed her death at times depicts more about the people involved in it than the poet herself. And the editorial changes to the poems, particularly in their nascent forms in print, make the replacement of merely one of her famous dashes in the “Republican” look quaint.
	This kind of information, the detailed and rich provenance depicting a publication history is not always available for a literary scholar attempting to make sense of the various forms of a writer’s works in print. And even when that information is available, disentangling the thorniness of that history can prove difficult. One of the advertised promises of computational analysis of literature is to uncover latent information that can aid in difficult research questions, to help detect patterns, trends, and influences especially when there are gaps in the historical record. The production of mathematical models of literature – that is quantified representations of the pieces of language in sequence that compose literary works – are made to pull off a paradoxical magic trick: to reduce our knowledge temporarily in an effort to expand it. Polly Longsworth, writing on attempts by Dickinson’s publishers and subsequent biographers to reverse engineer a persona of the poet through her image and the traces of her life – letters, notes, furnishings, etc. – imparts a telling if disturbing description of the nature of that task and Dickinson’s own part in making it a difficult one. “By straining to keep all traces of what she called "personality'" out of her poems and letters, she made herself vulnerable to the
imposition of multiple personalities” (Longsworth 39). The multiple roles that Dickinson has taken on post-mortem reflects that computational magic trick. For the notion of a model of Dickinson’s poetry contains the idea that we are reassembling an Emily Dickinson and her ouevre by considering the multiple versions of her poems put to print, altered by her editors and publishers. We are reconstructing a Dickinson that is based on quantifiable evidence, but also, of course, the perspective of the modeler themselves. This is a feature for literary scholarship and not a bug: to produce a fact-based and experientially-inflected interpretation of reality. Moretti’s “elaboration” is not so new after all, merely an enhanced component of a fact-finding toolkit that extends our ability to gain new perspectives into a partially more objective realm.



Siting for the Dickinson data set
Description of the Dickinson problem
Filters as perspective



	Where should scholars performing this craft look for inspiration or influence? As it turns out, the answer is both within the field and from outside of it. As is routinely pointed in defenses of the subfield, humanists carry important ethical values of their own (not the only ethics/values) that inform the general culture of their field(s). I am not making an argument for or against a set of ethics/values here, but rather that specific sets of research values exist for the humanities and that they need be included in computational work. And this is for two reasons. The first is translational: so that the work can be communicated to fields outside of the humanities and reproduced by them. The second is so other scholars in the field can appreciate or even adopt the interpretations and given evidence to ensure the continuation of knowledge production begun by computational work lest it be shunted aside as parallel if disparate subculture itself. As for the former case of translation, we can borrow means of ascertaining these qualities from outside of the field, if but because fields who are based and versed in quantitative methods and evidence have produced such efforts. The most direct borrowing is the creation of a data quality framework. Laura Sebastian Coleman, an information scientist, has written on the creation of such frameworks. Her academic research and position overseeing the data architectures of private insurance and healthcare companies has given her the abstraction and experience to pen a significant work on the topic.  This is where the process of creating a data quality framework for a digital humanities project will begin. As we will see, such a framework does not stop with metrics of data quality. The effect is cumulative, where evidence of quality produces an ethos for subsequent research processes and subsequent research projects. Thinking of this framework as a physical foundation for the work to follow is a useful and (as it turns out) definitional metaphor for the construction of computational models of literature. Even the most complex of things must begin by necessity with a simple form.

	
Throughout this work I call attention to this analogy not to torture a metaphor of construction (and it will be revisited in different stages of constructing models for literature) but because modeling is metaphorical. The language and motivation by which it occurs is intensely physical and spatial. They operationalize the process of creation. Yet, because of the abstract nature of the mathematics and materials involved (“data”), this motivation is often lost in the work. (possible as footnote?)	
	

“Charlie had said that key elements of my building’s design…would be determined by this first fact.” [italics mine] (Pollan 31)
“[T]here were views to consider (from the building, of the building), the fall of light across its floor, the movement of air around it, the ambient sounds, the angle at which it met the late-day sun.” (31)
“Dwell too long on so many soon-to-be-set-in-stone characteristics and the decisions is liable to paralyze you.” (31)
“I felt that by choosing its site – a single place out of all possible places in which to build – I was setting this great big contingency in motion, rolling it down the steep, one-way hill of personal and local history.” (31)

Data Quality and Laura Sebastian Coleman

	But what is data quality? Coleman defines it as “the degree to which data meets the expectations of data consumers, based on their intended use of the data” (Coleman, talk slide 8). She renders in perhaps surprisingly subjective terms what will ultimately become a series of quantitative measures. Since there is no escaping bias in measurement, why pretend one can eliminate all of it? Instead, we explicitly define the quality rating of our data in terms of the degree to which it meets our expectation and intended use. Assessment of quality comes through understanding of “processes that created” our data, the “systems through which” our data was created, “the concepts” our data represents, and “the known and potential uses of [our] data” (Coleman, talk slide 9). This evaluation is ultimately a means of identifying and understanding the implications of errors in our data. In terms of literary data sets, errors may be both structural and functional. For instance, outliers in categorical fields may be determined to be errors in measurement. One can imagine multiple scenarios for such an error, the predominant source of which for text analysis might be OCR errors. Another way of thinking about errors though are misclassified works in a corpus. For example, if a novel falls far from the lexical norms of writings from a period and place it may be the result of a highly unique author, or it could be something else like a misdating or an authorial misattribution. When the scale of inquiry goes into the hundreds and thousands of works, errors like these may not be far behind depending on our understanding of the processes of the creation of that data. As Coleman notes, who transcribed and produced the metadata for a work, and what prescribed methods by which that transcription and metadata production happened become key means to determining the expected correctness of those editorial processes. Further down the line, those errors may produce unintended consequences for modeling and interpreting models. Up until this point, the preceding history of prominent computational text analysis in literary study has either written this off as negligible, statistical noise or produced flat values of “error” (as is common in statistics), neither of which offer a reasonable or sound explanatory foundation from which to base further study of subjected corpora.
	Furthermore, assessment of data quality is a cyclical process and imbues an unorthodox ethics for working with data. In moving from measurement goals to data collection, data quality calculation, and comparison with an expectation (e.g. a norm), conclusions from assessments inherently require the creation of dataset variants that contain corrected errors. While the manipulation of data can inspire terror in the objectively-minded, the reality is that real world data is messy and working with it requires the mature understanding that our datasets contain creation bias and selections of that data are also inherently biased. Consequently, the assumptions upon which our selected measurements stand contain those biases. If a datum confutes our expectation and intended use, and one can determine that this confutation is produced via error, then – almost counterintuitively – it is incumbent upon us to emend it to proceed ethically (and if possible to report on that error to the data creator). All of the above requires a somewhat intimate knowledge of the items in our data set; if not a knowledge rooted at the closely-read level, then a loose (but flexible) expectation of what a work may contain. There will always be unidentified errors due to lack of expertise (a finite resource), but the closer a researcher can get to understanding a data set, the more trust can be built between them and their intended audience(s) that the modeling and their interpretation of it is sound. Thus re-framed in quantitative measures is also the primary justification as to why people who are trained in literary study are those best positioned to create large-scale, computed models of literature.

</Text>
        </Document>
        <Document ID="21BBDFDB-6886-4E7C-B6B9-34F0FD0585A3">
            <Text>December 17, 2024

Work done today

Worked on metrics for Clarity and Quality of Representation for Project Gutenberg Huck Finn metadata

What’s left is to figure out to best measure how many mismatches among keys and values for the metadata there are

So far I have a flat, but dumb mismatch count</Text>
        </Document>
        <Document ID="58C7D8F0-6D67-4E12-99D6-5B8E47E940EA">
            <Title>June 6, 2019</Title>
            <Text>Done
	1.	Continued work on plotting Dickinson data,
	2.	continued reading McGann
	3.	read more on from DQ texts (Sebastian coleman, jannidis/flanders)</Text>
        </Document>
        <Document ID="17432E00-FCC7-4EC3-8839-90FE38CC5658">
            <Title>April 15, 2019</Title>
            <Text>Introduction prose
What does (or can) the humanities add to the project of cultural data modeling? To some, that question may be a minor afterthought in seeking patterns or even insight amidst the mass of textual objects humanity has thus far produced and continues to produce. But that is the subject of this writing, and, as it turns out, the central premise that underpins the project of digital humanities studies. What qualities affirm our answers? And how do we know they do so.
Done
	1.	Read through part of Part I, Section 2 “A gentle introduction to data modeling” (Jannidis and Flanders 2019) and part of Part II, Section 4 “How subjective is your model?” (Pierazzo 2019) in The Shape of Data in the Digital Humanities: Modeling Texts and Text-Based Resources
	2.	Read through almost all of “The Limits of Modelling: Data Culture and the Humanities” (Quamen 2013)
	3.	Highlighted and took notes in the above on Paperpile
	4.	Wrote note above about how this reading ties to my initial inquiry/premise on the contribution of humanities to data modeling</Text>
        </Document>
        <Document ID="5C2B55AE-9DE2-4432-B10B-8BCA2D5AEBF4">
            <Text>What are the problems with data in literary studies?

Inaccurate transcription
Overrepresented portions of a dataset (and thus underrepresented portions of it)
Overrepresented datasets
Hapax legomena ignored by CDF (statistics)
Inconsistent transcription across editions/versions of the same/similar text
Different editions - which to consider
Access to datasets - public/private
The size of a individual set/the amount of datasets available - is this enough to be modeled on its own
What is the best version of an individual text/the most suitable version of a text for modeling?
The challenge of difference in kind when it comes to evaluation: quantitative versus qualitative (and the mindsets that make this distinction as translated into modeling/anlaysis/interpretation/criticism)

</Text>
        </Document>
        <Document ID="69D12EF2-7A3B-4EC5-A003-73B11A649DF6">
            <Title>Source Notes</Title>
        </Document>
        <Document ID="2D1728BC-A4DB-4E69-8FA6-38D9827EB143">
            <Title>Draft - Begun August 2019</Title>
            <Text>Art of Literary Modeling

“[T]here were views to consider (from the building, of the building), the fall of light across its floor, the movement of air around it, the ambient sounds, the angle at which it met the late-day sun.” 
– Michael Pollan, A Place of My Own: The Education of an Amateur Builder


	The creation, modeling, and interpretation of literature all share a seemingly innocuous characteristic: they all begin and end in socially- and physically-situated spaces. What would that have to do those processes? In the late 1990s for instance, before he became a celebrated food author, Michael Pollan ventured to create a space from which to write – and then he wrote a book about the process from that space. Pollan explores the connections between the physical spaces we inhabit and how they alter our interpretations of the world around us. But also, tellingly, how that outside world informs the creation of those spaces. By journey’s end, he gazes out a large window at a desk surrounded by his books. It bears beginning a work on the qualities of modeling literature with this ending. The constructed vantage of an author is laden with consequence for the work of storytelling. That is, the work of representation begins with perspective. Pollan uncovers the roots of his desire for such a perspective, tracing them back to the introduction of the lockable writing desk in 16th century Europe, and then to private rooms like studies for writing desks and books – private spaces to think while surrounded by the ideas of others and from which one could look upon the world with remove.

It ends with a house with a room
“capturing data is not passively accepting what is given, but actively constructing what one is interested in.”

	Toward the end of Distant Horizons (2019), Ted Underwood qualifies the risks of distant reading. He states that subjective evidence is central to the argument he has been making about/in favor of distant reading throughout the book, and that that argument also “resist[s] the notion that a firm boundary can be drawn between measurement and interpretation” (Underwood 147). Toward the beginning and end of Deep Learning with Python (2018), François Chollet alludes to this challenge from the other side of the qualitative-quantitative divide: “Deep learning is more of an art than a science” (Chollet 223). What does developing the correct neural network architecture for a research problem require? It requires “practice” and “intuition” (60). Sure, experts like Chollet and Underwood can give you “best practices” and “principles” (60) and “guidelines” (223). But in the end, experience and the particular question and problem set at hand will prevail. “There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate” (223). This is the horizon line of the blurred problem space between two spheres of culture modeling that have seen themselves largely as qualitative or quantitative in their most recent histories. The irony of it all is the similar apprehensiveness with which both authors/disciplines approach that space. The humanists are unsettled by the fixity, reductiveness, and supposed discipline-disrupting allure of quantification. The scientists are unsettled by the ambiguity, complexity, and supposed theoretically-devoid character of qualitative decision making. The unpaved path along that line offers the solution, a place where both interpretation and measurement are used conscientiously. Exploring this improvisational and exploratory art (or techne, for the classical-minded) of modeling literature is the subject of this work.</Text>
        </Document>
        <Document ID="DB75272F-9CD5-4488-B450-15F11F395389">
            <Title>Sum2021Fell/Application/ProjDesc/Body (w/ outline)</Title>
            <Text>1.	 Mark Twain autobiography description - popular response

In fits and starts over the course of his life, Mark Twain attempted to write an autobiography of his adventure-filled life. Toward the end, he dictated much of it to a stenographer. He also came to a peculiar agreement with his publisher and estate in which the work would not be published until 100 years after his death. He reasoned that this would grant him freedom to talk openly about the people in his life as well as public figures and subjects of the time. By the time that anniversary came, the Mark Twain Papers &amp; Project at University of California, Berekely had been preparing the autobiography for years for its eventual publication. As a celebrated and legendary American author, it was met with great commercial success. However, much to the initial confusion of readers, the entire autobiography would not be fully published until five years later, meted out every two years. The final product was three volumes in length, each of which, including editorial apparatuses, contained almost 800 pages. In effect, it was a book popularly bought but not as widely read.

2. This sets up the problem of a large, opaque work

The presentation of an unwieldy and thus opaque work for reading and interpretation is not a new kind of problem for humanities scholars, and certainly not for digital humanities scholars. On either end of the analytical spectrum, it is approached with similar divide and conquer heuristics. One might look to previous scholarship, lightly read the whole work while concentrating on a few sections, or, via computation, attempt to model the work(s) with an algorithm that can describe the entirety of the collection/object while giving insight on mathematically notable sections. These are but a few possible, general approaches. 

3. The problem of model selection for computational modeling of literature

For the computational approach there are a few significant decision points. There is the choice of models, sometimes driven by state of the field and sometimes driven by state of the art. And then there is the type of model output analysis. How do we comprehend the model and how do we account for how well it represents the underlying textual data? These questions of model selection and estimation of quality are not under-theorized in the field of statistics. The question of which model might be most appropriate to understand or discover new insights about Mark Twain’s autobiography and works like it, is though. 

4.	Project proposition on model selection via model quality assessment for project/humanities
5. Description of previous chapter on data quality and overall dissertation

My dissertation, ‘The Art of Literary Modeling’ , describes and demonstrates several new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. Its first chapter attempts to answer the question, "What does it mean to read literature as data?" That chapter's discussion of "literary" data quality looked to Emily Dickinson's poetry and its complex publication history. My proposed project for the summer fellowship is the followup chapter on new means of qualitative and quantitative assessment of model quality specifically for humanities researchers and their application towards uncovering new insights about Twain’s sizable autobiography. The digital version of this work, in TEI form, has been supplied to me via the Mark Twain Project Online at University of California, Berkeley. 

6. Discussion of adaptation of information science methods for humanities purposes (dq metrics, dq assessment, new “literary” data quality valuation)

That first chapter of my dissertation adapts a series of data quality metrics from information science scholar and career practitioner, Laura Sebastian Coleman, that are more typically employed in environments where data sets are dynamic and have external concerns – legal, technical, financial, and otherwise. Newly derived metrics from Coleman’s examples concerning the textual consistency and lexical and thematic qualities across Dickinson's multiple, posthumous publications are brought about by analyzing and modeling those publications, ultimately filling out an overall quality assessment of those (digital) works.  When taken into account alongside historical and bibliographic knowledge of the books, this produces a new, amalgam Dickinson poetry collection from which to conduct future readings, modeling, and analysis.

7. “Information criteria” for selected model as means of quality estimation of model output

Moving forward from that sensibility for data quality as foundation for modeling literature, this chapter attempts to answer the followup question: "What does it mean to model literature as data?" Whereas the previous work adapted data quality concepts from information science, this one looks to standards for model selection and quality analysis from statistics known as "information criterion." For instance, a set of information criteria measurements based on and related to one initially developed by Japanese statistician Hirotugu Akaike in the early 1970s, can provide a sense of the relative quality of models by reflecting the amount of information loss between a model and the original data set. While a mathematical description of data generation – in the case of literature, authorship – remains out of reach, Akaike’s information criterion measure (and other criteria produced by successive statisticians) allows us to at least understand the difference between the statistical distributions of a work’s words (e.g. their likelihood of being used together) according to various kinds of modeling techniques such as frequency analysis, topic models, regression analyses, matrix factorization methods, etc. In turn, the comparison made by information criteria presents a new qualitative decision point for the use of literary study.

	8.	Combining IC with proofing via humanities qualitative assessment
	⁃	close reading sections highlighted by the model and producing valuation kind of like an annotated bibliography for these highlighted sections of the model/underlying text
	9.	Prospects for new insights about the Twain autobiography and new notion of sound evidentiary basis for the highlighting/selection of sections for analysis/comparison/contrast/etc.
	10.	Prospects for other researchers to use these model metrics and heuristic to make assessments of their own models and use of those assessments as (part) basis for the interpretive claims made

Of course, in the case of literary scholars the quantitative outputs of such measurement can provide a befuddling endpoint. What to do with these numbers? The process of adaptation of such a measurement requires the incorporation of qualitative assessment of sections of a work as deemed notable (or perhaps very unnotable, e.g. hapax legomena) according to the various models being compared. A direct comparison must be made between the highest quality model’s highlights of Twain’s autobiography and scholarly knowledge of Twain’s life and writings. Such comparisons are essentially close readings supported by qualified computation modeling as well by the epistemological and subject matter concerns of more traditional humanities approaches to textual scholarship. 

These readings/measures of model quality can be also thought of as a new form of annotated scholarship for readers and scholars of Twain’s autobiography. As such, they are also generalizable examples of how to engage in a thoughtful discourse between reading and the modeling of literature as data.



</Text>
        </Document>
        <Document ID="15A91200-B671-4AAA-87C8-8D2C62E4599A">
            <Title>July 29, 2019</Title>
            <Text>Introduction prose

Introductory Statement for Art of Literary Modeling

Toward the end of Distant Horizons (2019), Ted Underwood qualifies the risks of distant reading. He states that subjective evidence is central to the argument he has been making about/in favor of distant reading throughout the book, and that that argument also “resist[s] the notion that a firm boundary can be drawn between measurement and interpretation” (Underwood 147). Toward the beginning and end of Deep Learning with Python (2018), François Chollet alludes to this challenge from the other side of the qualitative-quantitative divide: “Deep learning is more of an art than a science” (Chollet 223). What does developing the correct neural network architecture for a research problem require? It requires “practice” and “intution” (60). Sure, experts like Chollet and Underwood can give you “best practices” and “principles” (60) and “guidelines” (223). But in the end, experience and the particular question and problem set at hand will prevail. “There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate” (223). This is the horizon line of the blurred problem space between two spheres of culture modeling that have seen themselves largely as qualitative or quantitative in their most recent histories. The irony of it all is the similar apprehensiveness with which both authors/disciplines approach that space. The humanists are unsettled by the fixity, reductiveness, and supposed discipline-disrupting allure of quantification. The scientists are unsettled by the ambiguity, complexity, and supposed theoretically-devoid character of qualitative decision making. The unpaved path along that line offers the solution, a place where both interpretation and measurement are used conscientiously. Exploring this improvisational and exploratory art (or techne, for the classical-minded) of modeling literature is the subject of this work.</Text>
        </Document>
        <Document ID="95335B2E-3B6E-4D0F-9554-97237F9E7524">
            <Title>Visualization Quality</Title>
        </Document>
        <Document ID="DFCF95EB-61D1-4796-A4B7-929A1E0E5F86">
            <Title>May 21, 2019</Title>
            <Text>To do
	1.	Plan for first chapter writing
	2.	Check in with Tom and Committee early September
	3.	Update Ted and David this month or next
	4.	Summer by week schedule
	5.	Goal for this week
	⁃	Produce a piece of Emily Dickinson research based on the current data available
	⁃	Look for Dickinson data other than the Emily Dickinson Archive’s
	⁃	Possible sources include Dickinson Electronic Archive, Dickinson Letters at UVA (suggested by Tanya), the latter requires some kind of access, check with NYU libraries
	6.	Look into costs of Utrecht and Asilomar conference
</Text>
        </Document>
        <Document ID="ED567BA7-C873-490D-8F3F-62CA2F3A6C33">
            <Title>Art of Literary Modeling - Beginning - October 21 2019 (Conflicts)</Title>
        </Document>
        <Document ID="0DC571FA-1003-4A7B-8665-7C7AC66AFA61">
            <Title>June 3, 2019</Title>
            <Text>Notes

Some thoughts on digital editions and text mining

What is the point of a digital edition? What is the point of text mining and analysis? The latter work is not here to provide putative qualititative interpretation, is instead to provide informative qualitative interpretation of a quantitatively-derived perspective. New perspectives. Human perspectives. Informed by numbers but not defined by numbers. That is the difference and that is what digital humanities – or all this counting in literary studies – is all about. Rich, digital editions are not merely necessary for the purposes of preservation, but they are necessary to substantively inform quantitative models of human culture (in this case, texts) so that conclusions and decisions based on these new quantitatively-derived perspectives are sound conclusions and decisions.
Done
	1.	Read through gradient descent articles for the purpose of doing some machine learning on Dickinson correspondences corpus
	2.	Checked out Open Me Carefully from McGill library and read some of the introductory material regarding genre classification and letter dating
	3.	Started coding for a linear regression script of letters by date range/line count - see scripts/dc_regress.py

To do
Notes from code (for work tomorrow programming and analyzing)

Plan
1. Look at history tag in each of the TEI files
2. gather provenance - notBefore/notAfter attributes
3. Regress these poems date ranges by line counts
4. Learn the line (linear regression)
Thoughts
	1.	Create a BeautifulSoup meta class for easily working with the TEI files
	2.	Have to learn how to parse these XML files (specifically the content)
</Text>
        </Document>
        <Document ID="D0920011-C97C-4B9E-8CC8-A774E79077CC">
            <Title>Process for producing initial DQ assessment
Process for producing initial data quality assessment
</Title>
            <Text>Ideas
Process for producing initial data quality assessment

	1.	Creating a dq assessment metadata file to be processed by dq init assessment script
	⁃	Includes path to json file with metadata
	⁃	Includes custom map dict that describes each field
</Text>
        </Document>
        <Document ID="FB3BCCB9-5BAA-4F50-BFE0-D434B12F01B6">
            <Title>PhD docs</Title>
        </Document>
        <Document ID="2D9C0DC1-F59F-4137-873F-24554DC0AF93">
            <Title>December 9, 2024</Title>
            <Text>Measurement Types

#1 Dataset Completeness – Sufficiency of Metadata and Reference Data

“Over the life of a database [field of study], opportunity costs are associated with insufficient metadata. Redundant data may be stored because people are not aware of what data an organization already possesses and in what form. Data consumers may use data inappropriate to their purpose because they make incorrect assumptions about it. Or different consumers may use different data for the same kind of analysis, generating inconsistent results.” (Sebastian-Coleman 224)

“Metadata assessment tests first for existence and completeness (percentage of tables defined, percentage of columns defined; percentage of codefields supported by references data, etc.) and next for the clarity and quality of definitions (clear, comprehensible, unambiguous, grammatically correct, etc.) and consistency of representation (the same field content defined in the same way).” (224)

“One of the primary goals of metadatramanagement is to document organizational understanding of terms and usage.” (225)

Notes on Huck Finn Metadata processing for measurement of Dataset Completeness

Some processing/cleanup from the raw Project Gutenberg text files is necessary. The question becomes how much processing is acceptable before we move into the threshold where we are manipulating the metadata in such a way that it no longer represents the raw state of the “data quality” of the data (each edition text file). Essentially, we must at least make note that some processing was necessary in order to get the metadata into such a state that it may be adequately compared. Here this initial processing is a matter of judgment depending on the usefulness of leaving the data in as raw of a state as possible versus if it would be useful to bare with/do some pre-processing.


</Text>
        </Document>
        <Document ID="50988221-93F6-4973-905D-C111B77FD925">
            <Title>Abstract - Working Draft</Title>
            <Text>The publication of Mark Twain’s autobiography 100 years after his death presents a difficult proposition for the reading public with an unwieldy size of over 2000 pages. The work also presents an opportunity for computational text analysis to provide insights where more traditional close readings of it might prove difficult. My dissertation, “The Art of Literary Modeling”, demonstrates new scholarly activities that provide context-sensitive, evidentiary bases for making interpretive claims from computational models of literature. Its first chapter answers the question, "What does it mean to read literature as data?" That chapter's discussion of "literary" data quality looks to versions of Emily Dickinson's poetry amid her multiple, posthumous publications. It adapts data quality metrics from information science to provide an overall assessment of those digital versions, and produces a new collection of poems that reflect their data quality with respect to her oeuvre. This next chapter will answer the followup question: "What does it mean to model literature as data?" Applying standards for model selection and quality analysis from statistics known as "information criterion" reveals differences between various models of a work. That comparison made possible by information criteria presents a new, qualitative decision point for the study of literature as data. By looking to passages of Twain’s autobiography noted by high quality models of it and then further assessing them with scholarly understandings of those passages, it becomes possible to produce new findings on the autobiography and sound methodology for model quality generalizable to humanities research beyond this project.</Text>
        </Document>
        <Document ID="61C6E575-5D78-4D89-AFFC-A06DD6CB9328">
            <Title>Draft/Ch1DQ/Prose/On frequency analysis</Title>
            <Text>Next Steps/Prose

	1.	Prose explaining Bolts of Melody vs Emily Dickinson Archive 
	⁃	Token frequency comparison as component of data quality
	⁃	Unitary frequency analysis quality(ies) vs quality from comparative frequency analysis
	2.	Code up comparisons
	⁃	Explain the work and the thought process behind the work
	⁃	What’s are its goal(s)?
	3.	Prose and examples of more advanced frequency measures
	⁃	Mendenhall’s Characteristic Curves of Composition
	⁃	Kilgariff’s Chi-Squared Method
	⁃	John Burrows’ Delta Method
	⁃	See Laramee’s stylometry intro for details
	⁃	Why use them and how they are related to data quality in this context


Fuzzy string matching techniques

	1.	Character overlap
	⁃	Jaccard measure
	⁃	"the percentage of unique characters that two strings share when compared to the total number of unique characters in both strings” (Taming Text, FSM, 4.1.1.)
	⁃	Jaro-Winkler distance
</Text>
        </Document>
    </Documents>
</SearchIndexes>